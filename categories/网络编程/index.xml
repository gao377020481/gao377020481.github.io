<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>网络编程 on Gao&#39;s happy day</title>
    <link>https://gao377020481.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</link>
    <description>Recent content in 网络编程 on Gao&#39;s happy day</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gao377020481.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DNS协议解析</title>
      <link>https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/</guid>
      <description>DNS Domain Name System 域名系统，是一个分布式数据库，用于映射IP与域名
每级域名长度限制为63，总长度限制为253，使用TCP UDP端口53
DNS分层 顶级域：com org等 第二级域：baidu google等 第三级域：www edu等
域名解析 静态映射：在本机上配置域名和ip映射并直接使用
动态映射：使用DNS域名解析系统，在DNS服务器上配置ip到域名的映射
域名服务器 根域名服务器： 共a-m十三台（十三个ip）但拥有很多镜像服务器，镜像与本体使用同一个ip，存有顶级域名服务器的ip 顶级域名服务器：管理在该顶级域名服务器下注册的二级域名 权限域名服务器：一个区域的域名解析 本地域名服务器：处理本地的请求，保存本地的映射
域名解析方式 迭代查询：本机请求本地域名服务器，本地域名服务器开始迭代的查询各个层级服务器，先查询根获得顶级的ip然后根据获得的ip查询顶级在获得区域的ip依次迭代查到请求的映射
递归查询：递归查询时只发出一次请求然后等待接收到最终结果，在上面的步骤中本机使用的就是递归查询
协议报文格式 dns_dp dns_dp dns_dp dns_dp
具体查看文档
DNS client UDP编程 首先需要自己定义数据结构用于存储dns报文
struct dns_header{ unsigned short id; unsigned short flags; unsigned short questions; unsigned short answer; unsigned short authority; unsigned short additional; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; unsigned char *name; }; 这里只需要question和header是因为我们作为client只实现发送A请求也就是获取域名的ipv4地址，在实现中header的授权码和附加码都不需要使用只需要使用questions id和flags即可</description>
    </item>
    
    <item>
      <title>内存池</title>
      <link>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/</guid>
      <description>内存池实现（注释详细） #include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #define MP_ALIGNMENT 32 //对齐信息 #define MP_PAGE_SIZE	4096 //单次分配大块大小 #define MP_MAX_ALLOC_FROM_POOL	(MP_PAGE_SIZE-1)  #define mp_align(n, alignment) (((n)+(alignment-1)) &amp;amp; ~(alignment-1)) #define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) &amp;amp; ~(alignment-1))  struct mp_large_s { struct mp_large_s *next; void *alloc; }; // 当单次分配超过pagesize时就需要一次分配然后归入large的一个链表中保存  struct mp_node_s { unsigned char *last; unsigned char *end; struct mp_node_s *next; size_t failed; };// 页，用于小块的分配,last指向页内使用到的位置  struct mp_pool_s { size_t max; struct mp_node_s *current; struct mp_large_s *large; struct mp_node_s head[0]; }; //内存池  struct mp_pool_s *mp_create_pool(size_t size); void mp_destory_pool(struct mp_pool_s *pool); void *mp_alloc(struct mp_pool_s *pool, size_t size); void *mp_nalloc(struct mp_pool_s *pool, size_t size); void *mp_calloc(struct mp_pool_s *pool, size_t size); void mp_free(struct mp_pool_s *pool, void *p); //首先需要明确，在分配的时候需要将所有的数据结构都存在我们管理的内存池中 //比如struct mp_pool_s *pool这个内存池本身也需要受我们管理 struct mp_pool_s *mp_create_pool(size_t size) { struct mp_pool_s *p; int ret = posix_memalign((void **)&amp;amp;p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s)); //posix_memalign 分配足够的内存，size（page_size：4096） 加上内存池本身和小块结构本身 	if (ret) { return NULL; } p-&amp;gt;max = (size &amp;lt; MP_MAX_ALLOC_FROM_POOL) ?</description>
    </item>
    
    <item>
      <title>定时器</title>
      <link>https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
      <description>定时器 定时器作用很多，常见于心跳检测，冷却等等 实现时区别主要在于定时器队列的管控（排序）
基本方案： 红黑树： 使用红黑树对定时器排序，排序依据为定时器过期时间，每隔单位时间检查红黑树中最小时间是否小于等于当前时间，如果小于等于就删除节点并触发节点的callback。时间复杂度增删O(logn)，Nginx使用红黑树。删除添加操作自旋。
最小堆： 最小堆根节点最小，直接拿出根节点与当前时间比较即可，删除操作将根节点与末尾节点对换并删除末尾节点然后将新的根节点下沉，添加时加入末尾节点并上升。
时间轮：  时间轮可以分为单层级与多层级。简单的单层级时间轮使用初始化好的链表数组来存储对应的事件节点链表，时间数据结构中一般包含引用计数，该数据结构只有在引用计数置零后销毁，一般也代表着事件对应的资源可以释放。单层时间轮的大小至少需要大于最长定时时间/单位时间，举例：每5秒发送一个心跳包，连接收到心跳包时需要开启一个10秒的定时器并将事件引用计数加一（事件数据结构插入链表数组中10秒后的链表中），也就是最长定时10秒，10秒后检查该连接对应的事件并将引用计数减一，如果减一后为0就说明连接超时，释放所有资源，关闭事件。在该例子中，初始化的链表数组大小至少为11，因为假如在第0秒来一个心跳包，我们就需要在第10号位置将该连接对应的事件节点加入事件链表中，如果小于11，比如为8，那从0开始往后10个的位置就是在2号位置，那2秒后就得触发了，这与我们设置的10秒定时时间不一致。

代码实现： 红黑树： 红黑树数据结构直接使用nginx自带的rbtree头文件，就不自己写了
红黑树定时器头文件： #ifndef _MARK_RBT_ #define _MARK_RBT_  #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdint.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stddef.h&amp;gt; #if defined(__APPLE__) #include &amp;lt;AvailabilityMacros.h&amp;gt;#include &amp;lt;sys/time.h&amp;gt;#include &amp;lt;mach/task.h&amp;gt;#include &amp;lt;mach/mach.h&amp;gt;#else #include &amp;lt;time.h&amp;gt;#endif  #include &amp;#34;rbtree.h&amp;#34; ngx_rbtree_t timer; static ngx_rbtree_node_t sentinel; typedef struct timer_entry_s timer_entry_t; typedef void (*timer_handler_pt)(timer_entry_t *ev); struct timer_entry_s { ngx_rbtree_node_t timer; timer_handler_pt handler; }; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) 	struct timespec ti; clock_gettime(CLOCK_MONOTONIC, &amp;amp;ti); t = (uint32_t)ti.</description>
    </item>
    
    <item>
      <title>简易Tcp服务器</title>
      <link>https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</guid>
      <description>Tcp服务器 一请求一线程 首先说明问题： 已请求一线程能承载的请求数量极少，posix标准线程8M，请求数量多时极其占用内存
简单实现 实现一请求一线程很简单：
#define BUFFER_SIZE 1024 void *client_routine(void *arg) { int clientfd = *(int *) arg; while(1) { char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len &amp;lt;0 ) { close(clientfd); break; } else if(len ==0 ) { close(clientfd); break; } else{ printf(&amp;#34;Recv: %s, %d btye(s) from %d\n&amp;#34;, buffer, len, clientfd); } } } int main(int argc, char *argv[]) { if(argc &amp;lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(&amp;amp;addr, 0, sizeof(struct sockaddr_in)); addr.</description>
    </item>
    
    <item>
      <title>线程池</title>
      <link>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</guid>
      <description>线程池 基本功能模块：  线程池创建函数 线程池删除函数 线程池回调函数 线程池添加函数 线程池数据结构 线程任务数据结构 线程本身数据结构（由pid唯一确认）  首先实现数据结构： 线程任务数据结构：
struct nTask { void (*task_func)(struct nTask *task); void *user_data; struct nTask *prev; struct nTask *next; }; 这是任务中的一个个体，任务队列头存储在线程池数据结构中 void (*task_func)(struct nTask *task)函数指针表明函数为task_func且参数为struct nTask， 参数若为void是否更好
线程本身数据结构：
struct nWorker { pthread_t threadid; int terminate; struct nManager *manager; struct nWorker *prev; struct nWorker *next; }; pid唯一标识线程，terminate用于标识该线程应被删除，存储manager（也就是所属线程池）是为了通过manager找到task队列以获取task
线程池数据结构：
typedef struct nManager { struct nTask *tasks; struct nWorker *workers; pthread_mutex_t mutex; pthread_cond_t cond; } ThreadPool; 可以看到线程池其实只是一个管理者，使用mutex控制各个线程对进程内公共资源的访问，保证同时只有一个线程在访问公共资源，cond来控制各个线程的状态（处于等待队列（阻塞）或可以运行（运行、就绪态））细节在回调函数中</description>
    </item>
    
    <item>
      <title>线程池进阶版</title>
      <link>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/</guid>
      <description>#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;string.h&amp;gt; #include &amp;lt;pthread.h&amp;gt; #define LL_ADD(item, list) do { \ item-&amp;gt;prev = NULL;	\ item-&amp;gt;next = list;	\ list = item;	\ } while(0)  #define LL_REMOVE(item, list) do {	\ if (item-&amp;gt;prev != NULL) item-&amp;gt;prev-&amp;gt;next = item-&amp;gt;next;	\ if (item-&amp;gt;next != NULL) item-&amp;gt;next-&amp;gt;prev = item-&amp;gt;prev;	\ if (list == item) list = item-&amp;gt;next;	\ item-&amp;gt;prev = item-&amp;gt;next = NULL;	\ } while(0)  typedef struct NWORKER {//工作线程信息 	pthread_t thread; //线程id 	int terminate; //是否要终止 	struct NWORKQUEUE *workqueue; //线程池，用于找到工作队列 	struct NWORKER *prev; struct NWORKER *next; } nWorker; typedef struct NJOB { //工作个体 	void (*job_function)(struct NJOB *job); void *user_data; struct NJOB *prev; struct NJOB *next; } nJob; typedef struct NWORKQUEUE { struct NWORKER *workers; //所有工作线程的链表 	struct NJOB *waiting_jobs; //工作队列 	pthread_mutex_t jobs_mtx; pthread_cond_t jobs_cond; } nWorkQueue; typedef nWorkQueue nThreadPool; static void *ntyWorkerThread(void *ptr) { //工作线程取用工作 	nWorker *worker = (nWorker*)ptr; while (1) { pthread_mutex_lock(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //先获取工作队列的操作互斥锁  while (worker-&amp;gt;workqueue-&amp;gt;waiting_jobs == NULL) { if (worker-&amp;gt;terminate) break; pthread_cond_wait(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_cond, &amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //如果工作队列为空，这个线程就阻塞在条件变量上等待事件发生 	} if (worker-&amp;gt;terminate) { pthread_mutex_unlock(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //如果检测到工作线程被终止，那么这个线程就需要结束工作，但在结束工作前需要将对工作队列的取用权限放开，所以这里在break前需要解锁这个互斥锁 	break; } nJob *job = worker-&amp;gt;workqueue-&amp;gt;waiting_jobs; //从工作队列中获取一个工作 	if (job !</description>
    </item>
    
    <item>
      <title>请求池</title>
      <link>https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/</guid>
      <description>请求池实现 同步阻塞请求池 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;errno.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/socket.h&amp;gt;#include &amp;lt;netinet/in.h&amp;gt; #include &amp;lt;sys/epoll.h&amp;gt;#include &amp;lt;netdb.h&amp;gt;#include &amp;lt;arpa/inet.h&amp;gt; #include &amp;lt;pthread.h&amp;gt; #define DNS_SVR	&amp;#34;114.114.114.114&amp;#34;  #define DNS_HOST	0x01 #define DNS_CNAME	0x05  struct dns_header { unsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item { char *domain; char *ip; }; int dns_create_header(struct dns_header *header) { if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-&amp;gt;id = random(); header-&amp;gt;flags |= htons(0x0100); header-&amp;gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-&amp;gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-&amp;gt;qname == NULL) return -2; question-&amp;gt;length = strlen(hostname) + 2; question-&amp;gt;qtype = htons(1); question-&amp;gt;qclass = htons(1); const char delim[2] = &amp;#34;.</description>
    </item>
    
    <item>
      <title>进阶TCP服务器</title>
      <link>https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</guid>
      <description>进阶TCP服务器 该模块涉及TCP服务器常见点：并发量、IO模型
IO网络模型 阻塞IO （Blocking IO） 
阻塞IO的情况下，我们如果需要更多的并发，只能使用多线程，一个IO占用一个线程，资源浪费很大但是在并发量小的情况下性能很强。
非阻塞IO （Non-Blocking IO） 
在非阻塞状态下，recv() 接口在被调用后立即返回，返回值代表了不同的含义。如在本例中，
 recv() 返回值大于 0，表示接受数据完毕，返回值即是接受到的字节数； recv() 返回 0，表示连接已经正常断开； recv() 返回 -1，且 errno 等于 EAGAIN，表示 recv 操作还没执行完成； recv() 返回 -1，且 errno 不等于 EAGAIN，表示 recv 操作遇到系统错误 errno。  非阻塞的接口相比于阻塞型接口的显著差异在于，在被调用之后立即返回。使用如下的函数可以将某句柄 fd 设为非阻塞状态：
fcntl( fd, F_SETFL, O_NONBLOCK ); 多路复用IO （IO Multiplexing） 
多路复用IO，select/poll、epoll。
select/poll select和poll很相似，在检测IO时间的时候都需要遍历整个FD存储结构，只是select使用数组存储FD，其具有最大值限制，而poll使用链表无最大值限制（与内存大小相关）。
先来分析select的优缺点，这样就知道epoll相比select的优势等。
select 本质上是通过设置或检查存放fd标志位的数据结构进行下一步处理。 这带来缺点： 单个进程可监视的fd数量被限制，即能监听端口的数量有限 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试 一般该数和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认1024个，64位默认2048。
当socket较多时，每次select都要通过遍历FD_SETSIZE个socket，不管是否活跃，这会浪费很多CPU时间。如果能给 socket 注册某个回调函数，当他们活跃时，自动完成相关操作，即可避免轮询，这就是epoll与kqueue。
select 调用流程</description>
    </item>
    
    <item>
      <title>连接池</title>
      <link>https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/</guid>
      <description>连接池实现 mysql连接池 头文件： #ifndef DBPOOL_H_ #define DBPOOL_H_  #include &amp;lt;iostream&amp;gt;#include &amp;lt;list&amp;gt;#include &amp;lt;mutex&amp;gt;#include &amp;lt;condition_variable&amp;gt;#include &amp;lt;map&amp;gt;#include &amp;lt;stdint.h&amp;gt; #include &amp;lt;mysql.h&amp;gt; #define MAX_ESCAPE_STRING_LEN	10240  using namespace std; // 返回结果 select的时候用 class CResultSet { public: CResultSet(MYSQL_RES* res); virtual ~CResultSet(); bool Next(); int GetInt(const char* key); char* GetString(const char* key); private: int _GetIndex(const char* key); MYSQL_RES* m_res; MYSQL_ROW	m_row; map&amp;lt;string, int&amp;gt;	m_key_map; }; // 插入数据用 class CPrepareStatement { public: CPrepareStatement(); virtual ~CPrepareStatement(); bool Init(MYSQL* mysql, string&amp;amp; sql); void SetParam(uint32_t index, int&amp;amp; value); void SetParam(uint32_t index, uint32_t&amp;amp; value); void SetParam(uint32_t index, string&amp;amp; value); void SetParam(uint32_t index, const string&amp;amp; value); bool ExecuteUpdate(); uint32_t GetInsertId(); private: MYSQL_STMT*	m_stmt; MYSQL_BIND*	m_param_bind; uint32_t	m_param_cnt; }; class CDBPool; class CDBConn { public: CDBConn(CDBPool* pDBPool); virtual ~CDBConn(); int Init(); // 创建表 	bool ExecuteCreate(const char* sql_query); // 删除表 	bool ExecuteDrop(const char* sql_query); // 查询 	CResultSet* ExecuteQuery(const char* sql_query); /** * 执行DB更新，修改 * * @param sql_query sql * @param care_affected_rows 是否在意影响的行数，false:不在意；true:在意 * * @return 成功返回true 失败返回false */ bool ExecuteUpdate(const char* sql_query, bool care_affected_rows = true); uint32_t GetInsertId(); // 开启事务 	bool StartTransaction(); // 提交事务 	bool Commit(); // 回滚事务 	bool Rollback(); // 获取连接池名 	const char* GetPoolName(); MYSQL* GetMysql() { return m_mysql; } private: CDBPool* m_pDBPool;	// to get MySQL server information 	MYSQL* m_mysql;	// 对应一个连接 	char	m_escape_string[MAX_ESCAPE_STRING_LEN + 1]; }; class CDBPool {	// 只是负责管理连接CDBConn，真正干活的是CDBConn public: CDBPool() {} CDBPool(const char* pool_name, const char* db_server_ip, uint16_t db_server_port, const char* username, const char* password, const char* db_name, int max_conn_cnt); virtual ~CDBPool(); int Init();	// 连接数据库，创建连接 	CDBConn* GetDBConn(const int timeout_ms = -1);	// 获取连接资源 	void RelDBConn(CDBConn* pConn);	// 归还连接资源  const char* GetPoolName() { return m_pool_name.</description>
    </item>
    
    <item>
      <title>锁</title>
      <link>https://gao377020481.github.io/p/%E9%94%81/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E9%94%81/</guid>
      <description>锁 自旋锁 当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -&amp;gt; 等待的机制被称为自旋锁(spinlock)。
自旋锁的原理比较简单，如果持有锁的线程能在短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。
因为自旋锁避免了操作系统进程调度和线程切换，所以自旋锁通常适用在时间比较短的情况下。由于这个原因，操作系统的内核经常使用自旋锁。但是，如果长时间上锁的话，自旋锁会非常耗费性能，它阻止了其他线程的运行和调度。线程持有锁的时间越长，则持有该锁的线程将被 OS(Operating System) 调度程序中断的风险越大。如果发生中断情况，那么其他线程将保持旋转状态(反复尝试获取锁)，而持有该锁的线程并不打算释放锁，这样导致的是结果是无限期推迟，直到持有锁的线程可以完成并释放它为止。
解决上面这种情况一个很好的方式是给自旋锁设定一个自旋时间，等时间一到立即释放自旋锁。自旋锁的目的是占着CPU资源不进行释放，等到获取锁立即进行处理。但是如何去选择自旋时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用 CPU 资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！
在计算任务轻的情况下使用自旋锁可以显著提升速度，这是因为线程切换的开销大于等锁的开销，但是计算任务重的话自旋锁的等待时间就成为主要的开销了。
互斥锁 互斥锁实际是一个互斥量，为获得互斥锁的线程会挂起，这就涉及到线程切换的开销，计算任务重的情况下会比较适合使用。
读写锁 读写锁即只能由一人写但可以由多人读的锁，适用于读操作很多但写操作很少的情况下。
原子操作 多线程下使用原子操作确保我们的操作不会被其他线程参与，一般内联汇编 memory 内存屏障，只允许这一缓存写回内存，确保多线程安全。
多进程下对共享内存的操作使用内联汇编lock，锁住总线，同一时刻只允许一个进程通过总线操作内存。
粒度大小排序
互斥锁 &amp;gt; 自旋锁 &amp;gt; 读写锁 &amp;gt; 原子操作
代码实现 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;pthread.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;sys/mman.h&amp;gt; #define THREAD_SIZE 10  int count = 0; pthread_mutex_t mutex; pthread_spinlock_t spinlock; pthread_rwlock_t rwlock; // MOV dest, src; at&amp;amp;t // MOV src, dest; x86  int inc(int *value, int add) { int old; __asm__ volatile ( //lock 锁住总线 只允许一个cpu操作内存（通过总线）确保多进程安全  &amp;#34;lock; xaddl %2, %1;&amp;#34; // &amp;#34;lock; xchg %2, %1, %3;&amp;#34;  : &amp;#34;=a&amp;#34; (old) : &amp;#34;m&amp;#34; (*value), &amp;#34;a&amp;#34; (add) : &amp;#34;cc&amp;#34;, &amp;#34;memory&amp;#34; //memory 内存屏障，只允许这一缓存写回内存，确保多线程安全  ); return old; } // void *func(void *arg) { int *pcount = (int *)arg; int i = 0; while (i++ &amp;lt; 100000) { #if 0//无锁 (*pcount) ++; #elif 0// 互斥锁版本  pthread_mutex_lock(&amp;amp;mutex); (*pcount) ++; pthread_mutex_unlock(&amp;amp;mutex); #elif 0// 互斥锁非阻塞版本  if (0 !</description>
    </item>
    
    <item>
      <title>简易http客户端(C posix API)</title>
      <link>https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/</guid>
      <description>HTTP 实现http客户端程序
基础 HTTP使用TCP连接
HTTP报文：
 
实现 域名到ip地址转换(dns) 直接调用api进行转换比较简单：
char * host_to_ip(const char* hostname) { struct hostent *host_entry = gethostbyname(hostname); if(host_entry) { return inet_ntoa(*(struct in_addr*)*host_entry -&amp;gt; h_addr_list); } return NULL; } host_entry存储了dns请求的接收，从中取出第一个ip地址并将点分十进制转换为字符串返回
创建TCP套接字（建立连接） posix api创建
int http_create_socket(char *ip) { int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in sin = {0}; sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr.s_addr = inet_addr(ip); if(0 != connect(sockfd, (struct sockaddr*)&amp;amp;sin, sizeof(struct sockaddr_in))) { return -1; } fcntl(sockfd, F_SETFL, O_NONBLOCK); return sockfd; } fcntl(sockfd, F_SETFL, O_NONBLOCK);这个函数用于设置该套接字io为非阻塞</description>
    </item>
    
  </channel>
</rss>

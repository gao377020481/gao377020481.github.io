<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HPC on Gao&#39;s Happy Day</title>
    <link>https://gao377020481.github.io/categories/hpc/</link>
    <description>Recent content in HPC on Gao&#39;s Happy Day</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gao377020481.github.io/categories/hpc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对象优化</title>
      <link>https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/</link>
      <pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/</guid>
      <description>面向对象带来的性能损耗 关于对象的构成可以参考之前的一篇博客，C++对象
 缓存开销  数据被组织在对象内，然后对象又有一些额外的内存开销，比如虚表等。
方法开销  面向对象有很多的小函数，函数调用耗时，小的区块也限制了编译器优化的空间，虚函数机制多次跳转也有损耗。
构造开销  构造函数开销大，有可能要处理虚继承虚函数的情况。
JAVA中：
 在堆上分配空间，调用类的构造函数，初始化类的字段 对象状态被垃圾收集器跟踪，可能发生垃圾回收 在使用调试器创建对象时，你可以在一定程度上看到这一点。  **C++**中：
 拷贝构造经常发生（传参赋值返回），JAVA中都是引用就不会潜在发生。 构造与解构在继承体系中都是链状的，call的开销也很大  异常  这个可以参考之前写的异常处理，讲到try catch的实现，里面其实就是很多的跳转，还是非局部的，也就是很像function call。消耗当然大了。
优化 Method In-lining 编译器很难自己去in-lining，c++的in-line关键字也只是鼓励编译器尝试in-line函数。但还是多用inline关键字。
一般对于带循环的小函数编译器不会inline它，所以建议用重复的几行来替代小循环。
私有，final的方法很可能被inline但是虚函数从不会被inline因为需要在运行时间接调用
nested template：
template &amp;lt;template &amp;lt;typename, typename...&amp;gt; class V, typename... Args&amp;gt; void print_container(V&amp;lt;Args...&amp;gt; &amp;amp;con) 这需要C++11支持，template &amp;lt;typename, typename&amp;hellip;&amp;gt; class V是第一个模板参数，他是一个有typename, typename&amp;hellip;为模板参数的类，它的名字叫V，typename&amp;hellip; Args指后续的模板参数，数量任意多，Args就是V里的模板参数，需要在这里指示出来。这是C++11的写法，可以让编译器自动推导传入类型模板参数的数量和类型。
举个例子，
vector&amp;lt;vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt;的参数传进去，推导出来就是这个： void print_container&amp;lt;std::vector, std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;, std::allocator&amp;lt;std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt;&amp;gt;(std::vector&amp;lt;std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt; &amp;amp;con) Encapsulation 意思就是把性能差的部分拿出来单独放到一个区域中或类中，在辅助上硬件比如FPGA,GPGPU，可以达到不错的加速效果。</description>
    </item>
    
    <item>
      <title>Mercury初探</title>
      <link>https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/</link>
      <pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/</guid>
      <description>最近在看HPC领域分布式文件系统，发现代码里用到mercury来提供RPC功能，决定还是循序渐进先把这个框架学习一下。 &amp;lt;主要是报的一些关于mercury的错我看不懂*-*&amp;gt;
Architecture Network Abstraction Layer 这一层是中间层，向上给Bluk layer和RPC layer提供一致的interface，向下采用插件机制适配不同的网络协议。
可提供的功能：target address lookup, point-to-point messaging with both unexpected and expected messaging（就是阻塞和非阻塞）, remote memory access (RMA), progress and cancelation.
初始化 首先需要初始化接口并选择底层的插件，之前的ofi就是一种网络插件，大意是指与网卡交互所使用的协议方法的集合或者说一层。
初始化 初始化 初始化 初始化 Mercury RPC Layer Mercury Bulk Layer </description>
    </item>
    
    <item>
      <title>Gekkofs</title>
      <link>https://gao377020481.github.io/p/gekkofs/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/gekkofs/</guid>
      <description>最近在看GekkoFS，打算从理论到实现来试试能不能记下来这个分布式文件系统的实现。
他的亮点主要在于使用了系统调用拦截来直接在mount的目录上拦截相应的系统调用。
使用时还用到了一个比较少见的指令: LD_PRELOAD=&amp;lt;install_path&amp;gt;/lib64/libgkfs_intercept.so cp ~/some_input_data &amp;lt;pseudo_gkfs_mount_dir_path&amp;gt;/some_input_data 这个指令的意思是提前加载某个动态库（libgkfs_intercept.so），使用的时候将拦截的动态库提前加载进来，就可以在进入glibc的文件操作之前转向别的路径。
概述 node-local burst buffer file system.
 客户端GekkoFS client library  客户端是以库的形式提供的给HPC应用程序使用的。GekkoFS既没有实现基于fuse用户态文件系统的客户端，也没有实现VFS kernel的内核态的客户端。而是通过可以库的形式，截获所有文件系统的系统调用：如果是GekkoFS文件系统的文件，就通过网络转发给Gekkofs 服务端。如果不是GekkoFS文件系统，就调用原始的glibc库的系统调用。
Server端：GekkoFS daemon  Sever端通过本地的rocksdb来保存文件系统的元数据。 通过本地文件系统来保存数据。数据被切成同样大小的chunk（测试用512k）。
在客户端访问文件时，通过hash的方法映射到多个Server节点上。
总的来说这个架构还是很清晰的，附上论文里的图：

重点在于其工程实现，希望能从对它的代码分析中学到些有用的知识。</description>
    </item>
    
    <item>
      <title>向量化</title>
      <link>https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/</guid>
      <description>Requirements  loop得有可以（ determinable (at run time)）确定的循环次数 loop里不能包含（inline不算）过程调用 loop里不能包含分支 迭代次数也得多 迭代之间最好也没有依赖性 理论上就算有依赖性也可以向量化但是不实用  现代向量处理器  依赖于大量的硬件单元而非流水线 有SIMD指令 大量依赖缓存，所以对齐很重要  几个x86 vec指令  MMX instructions  适用于 32,16 or 8-bit 整数类型的计算指令，有8 x 64-bit 的寄存器： MM0 … MM7
SSE instructions  支持：64 &amp;amp; 32-bit floating point，64, 32,16 or 8-bit integer types， 有8 x 128-bit 寄存器： xmm0 … xmm7 (xmm8 .. xmm16 in 64-bit mode)
AVX instructions  ymm0 … ymm15 寄存器相比于SSE拓展到256bits
AVX2 AVX-512  AVX-512只在 Intel KNL and Skylake上有，更强力512-bits寄存器，zmm0 … zmm15，他分四部分：Foundation（扩展32，64指令），Conflict Detection Instructions（检测要vec的loop的冲突，尽可能让更多的loop能vec），Exponential and Reciprocal Instructions（KNL里的，能支持超越运算，咱也不懂。。）， Prefetch Instructions （KNL里的预取指支持）</description>
    </item>
    
    <item>
      <title>Spectre&amp;Meltdown</title>
      <link>https://gao377020481.github.io/p/spectremeltdown/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/spectremeltdown/</guid>
      <description>Spectre&amp;amp;Meltdown
这是一种利用现代处理器特性来“窃取”内存中重要信息的漏洞。
Meltdown  诱使CPU在乱序执行的基础上将高权限的data信息放置于cache中  首先有一个数组，直接访问它的第data*4096个index处的元素 这样这个index处的元素会被放进cache中   循环遍历这个数组，当遍历到第data*4096个index处元素时，载入速度明显变快，这就说明这里是当时载入cache的元素 取到data*4096这个index就取到了data  假设data是一个内核内存空间内的数据，我们就get到了机密的内核数据，这都依赖于cpu的乱序执行：
exception(dont have priority to access))access(array[data*4096])乱序执行使得在指令生效前，access运行在了exception之前，虽然指令未生效（寄存器状态等未变），但cache内却有了array[data*4096]这个元素
Spectre Spectre基本原理与Meltdown类似，但他更强
Meltdown一旦被从根本上避免就无法使用，比如内核和用户使用不同的页目录寄存器等
Spectre并非依赖于乱序执行，而是依赖于分支预测。
分支预测也会使cpu提前跑一下cpu认为正确的分支，尽管他不一定真的是接下来要执行的，同样会在cache里留下痕迹。
但他要求代码有如下形式：
if(index1&amp;lt;array_a_size) {index2=array_a[index1];if(index2 &amp;lt; array_b_size);value = array_b[index2];}通过控制index1的长度，让array_b的特定下标的数据Cacheline被点亮，如果有办法访问一次array_b的全部内容，我们就可以窃取到index1这个值。</description>
    </item>
    
    <item>
      <title>内存优化</title>
      <link>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/</guid>
      <description>从内存结构的角度来优化我们的程序。
Vector temporaries REAL V(1024,3), S(1024), U(3)DO I=1,1024S(I) = U(1)*V(I,1)END DODO I=1,1024S(I) = S(I) + U(2)*V(I,2)END DODO I=1,1024S(I) = S(I) + U(3)*V(I,3)END DODO J=1,3DO I=1,1024V(I,J) = S(I) * U(J)END DOEND DO----------------------------------------&amp;gt;REAL V(1024,3), S, U(3)DO I=1,1024S = U(1)*V(I,1) + U(2)*V(I,2) + U(3)*V(I,3)DO J=1,3V(I,J) = S * U(J)END DOEND DO将运算组织成近似向量的形式，编译器就很容易借助这种优势来将代码优化为向量的运算。</description>
    </item>
    
    <item>
      <title>编译优化</title>
      <link>https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/</guid>
      <description>讨论一些编译器会做的优化，这些优化一般不需要我们自己来做。
但是我们讨论时从源码角度来模拟，实际上编译器是在中间代码（IR）层进行优化的。
分四部分：
IR optimisations Basic optimisations Constant folding 将一些变量转换为常量，这有助于减少内存的访问。 且常量相关的求值是可以在编译阶段就确定，不必等待运行阶段。
转换前：
integer, parameter :: n=100do i=1,n....end do转换后：
do i=1,100....end doAlgebraic simplifications 简化算术运算，包括了运用结合，交换和分配律等：
(i-j)+(i-j)+(i-j) &amp;mdash;&amp;mdash;&amp;gt; 3*i - 3*i
但是要注意浮点数操作，它不遵循结合律：
1.0 + (MF - MF) = 1.0
(1.0 + MF) - MF = 0.0
这个例子里MF比1.0大，1.0在和MF结合后会被舍去，所以第二个结果出来就是0.0了，这是因为浮点运算会先算出精确值然后舍去多余位，所以结合律失效。
Copy propagation 拷贝出来的变量在后续的使用中可以被原来的变量替代，那就少用一个寄存器。
x = yc = x + 3d = x + y--------&amp;gt;x = yc = y + 3d = y + y这里x这个变量就不需要了。</description>
    </item>
    
    <item>
      <title>OFI/libfabric</title>
      <link>https://gao377020481.github.io/p/ofi-for-hpc/</link>
      <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/ofi-for-hpc/</guid>
      <description>之前一直很好奇，在超算里（或大规模的集群里），是怎么减少network传输的损耗的，最近项目中接触到了mecury这个rpc框架，他依赖于OFI、libfabric这个网络框架来做集群内的高性能网络传输。
OFI相比于普通的内核TCP/IP协议栈的优化思路与用户态协议栈有异曲同工之妙，准备好好写一下这篇笔记，先从内核里的TCP/IP协议栈的缺点开始，到优化思路与实现，再看看能不能学习一下ofi实现的trick。开个坑先，慢慢写。。
从缺点找灵感 TCP/IP 缺点
 用于提供可靠性的header占用很多带宽 同步的话耗时，异步的话耗空间（内核里的缓冲区）拷贝也耗时  高性能的网络API应该是什么样的？ 尽量少的内存拷贝 两点：
 用户提供buffer，与协议栈用一个buffer 建立一个流量控制机制  异步操作 两种策略：
 中断和信号，这种机制会打断正在运行的程序，evict CPU cache，而且一个随时能接受信号还不影响自己工作的程序就比较难开发 事件queue， 来了就进queue  Direct Hardware Access 主要两种思路：
 越过kernel，直接与网卡的buffer交互（代表DPDK） 硬件软件配合来在用户空间共享同一块地址空间作为buffer（RDMA）  那应该怎么设计呢？ 先抄来个需要的interface：
/* Notable socket function prototypes *//* &amp;quot;control&amp;quot; functions */int socket(int domain, int type, int protocol);int bind(int socket, const struct sockaddr *addr, socklen_t addrlen);int listen(int socket, int backlog);int accept(int socket, struct sockaddr *addr, socklen_t *addrlen);int connect(int socket, const struct sockaddr *addr, socklen_t addrlen);int shutdown(int socket, int how);int close(int socket); /* &amp;quot;fast path&amp;quot; data operations - send only (receive calls not shown) */ssize_t send(int socket, const void *buf, size_t len, int flags);ssize_t sendto(int socket, const void *buf, size_t len, int flags,const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int socket, const struct msghdr *msg, int flags);ssize_t write(int socket, const void *buf, size_t count);ssize_t writev(int socket, const struct iovec *iov, int iovcnt);/* &amp;quot;indirect&amp;quot; data operations */int poll(struct pollfd *fds, nfds_t nfds, int timeout);int select(int nfds, fd_set *readfds, fd_set *writefds,fd_set *exceptfds, struct timeval *timeout); 首先来看看这几类都有什么目标？</description>
    </item>
    
    <item>
      <title>Openmpi</title>
      <link>https://gao377020481.github.io/p/cmake/</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/cmake/</guid>
      <description>Openmpi 初步使用 安装与测试 直接官网下载release包
wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.1.tar.gz linux下解压:
tar -zxf openmpi-4.1.1.tar.gz 进入开始configure： prefix 为指定安装路径
cd openmpi-4.1.1/ ./configure --prefix=/usr/local/openmpi 安装：
make sudo make install 设置环境变量
sudo vim /etc/profile 加入：
export PATH=/usr/local/openmpi/bin:$PATH export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH 生效：
source /etc/profile 测试
mpicc --version 写代码测试：hello.c
#include &amp;lt;stdio.h&amp;gt;#include &amp;#34;mpi.h&amp;#34; int main(int argc, char* argv[]) { int rank, size, len; char version[MPI_MAX_LIBRARY_VERSION_STRING]; MPI_Init(&amp;amp;argc, &amp;amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank); MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size); MPI_Get_library_version(version, &amp;amp;len); printf(&amp;#34;Hello, world, I am %d of %d, (%s, %d)\n&amp;#34;, rank, size, version, len); MPI_Finalize(); return 0; } 编译并运行,我这里是四核虚拟机</description>
    </item>
    
  </channel>
</rss>

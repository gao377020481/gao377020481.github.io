<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Gao&#39;s Happy Day</title>
    <link>https://gao377020481.github.io/post/</link>
    <description>Recent content in Posts on Gao&#39;s Happy Day</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://gao377020481.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对象优化</title>
      <link>https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/</link>
      <pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/</guid>
      <description>面向对象带来的性能损耗 关于对象的构成可以参考之前的一篇博客，C++对象
 缓存开销  数据被组织在对象内，然后对象又有一些额外的内存开销，比如虚表等。
方法开销  面向对象有很多的小函数，函数调用耗时，小的区块也限制了编译器优化的空间，虚函数机制多次跳转也有损耗。
构造开销  构造函数开销大，有可能要处理虚继承虚函数的情况。
JAVA中：
 在堆上分配空间，调用类的构造函数，初始化类的字段 对象状态被垃圾收集器跟踪，可能发生垃圾回收 在使用调试器创建对象时，你可以在一定程度上看到这一点。  **C++**中：
 拷贝构造经常发生（传参赋值返回），JAVA中都是引用就不会潜在发生。 构造与解构在继承体系中都是链状的，call的开销也很大  异常  这个可以参考之前写的异常处理，讲到try catch的实现，里面其实就是很多的跳转，还是非局部的，也就是很像function call。消耗当然大了。
优化 Method In-lining 编译器很难自己去in-lining，c++的in-line关键字也只是鼓励编译器尝试in-line函数。但还是多用inline关键字。
一般对于带循环的小函数编译器不会inline它，所以建议用重复的几行来替代小循环。
私有，final的方法很可能被inline但是虚函数从不会被inline因为需要在运行时间接调用
nested template：
template &amp;lt;template &amp;lt;typename, typename...&amp;gt; class V, typename... Args&amp;gt; void print_container(V&amp;lt;Args...&amp;gt; &amp;amp;con) 这需要C++11支持，template &amp;lt;typename, typename&amp;hellip;&amp;gt; class V是第一个模板参数，他是一个有typename, typename&amp;hellip;为模板参数的类，它的名字叫V，typename&amp;hellip; Args指后续的模板参数，数量任意多，Args就是V里的模板参数，需要在这里指示出来。这是C++11的写法，可以让编译器自动推导传入类型模板参数的数量和类型。
举个例子，
vector&amp;lt;vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt;的参数传进去，推导出来就是这个： void print_container&amp;lt;std::vector, std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;, std::allocator&amp;lt;std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt;&amp;gt;(std::vector&amp;lt;std::vector&amp;lt;std::vector&amp;lt;int&amp;gt;&amp;gt;&amp;gt; &amp;amp;con) Encapsulation 意思就是把性能差的部分拿出来单独放到一个区域中或类中，在辅助上硬件比如FPGA,GPGPU，可以达到不错的加速效果。</description>
    </item>
    
    <item>
      <title>Mercury初探</title>
      <link>https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/</link>
      <pubDate>Sat, 12 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/</guid>
      <description>最近在看HPC领域分布式文件系统，发现代码里用到mercury来提供RPC功能，决定还是循序渐进先把这个框架学习一下。 &amp;lt;主要是报的一些关于mercury的错我看不懂*-*&amp;gt;
Architecture Network Abstraction Layer 这一层是中间层，向上给Bluk layer和RPC layer提供一致的interface，向下采用插件机制适配不同的网络协议。
可提供的功能：target address lookup, point-to-point messaging with both unexpected and expected messaging（就是阻塞和非阻塞）, remote memory access (RMA), progress and cancelation.
初始化 首先需要初始化接口并选择底层的插件，之前的ofi就是一种网络插件，大意是指与网卡交互所使用的协议方法的集合或者说一层。
初始化 初始化 初始化 初始化 Mercury RPC Layer Mercury Bulk Layer </description>
    </item>
    
    <item>
      <title>Gekkofs</title>
      <link>https://gao377020481.github.io/p/gekkofs/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/gekkofs/</guid>
      <description>最近在看GekkoFS，打算从理论到实现来试试能不能记下来这个分布式文件系统的实现。
他的亮点主要在于使用了系统调用拦截来直接在mount的目录上拦截相应的系统调用。
使用时还用到了一个比较少见的指令: LD_PRELOAD=&amp;lt;install_path&amp;gt;/lib64/libgkfs_intercept.so cp ~/some_input_data &amp;lt;pseudo_gkfs_mount_dir_path&amp;gt;/some_input_data 这个指令的意思是提前加载某个动态库（libgkfs_intercept.so），使用的时候将拦截的动态库提前加载进来，就可以在进入glibc的文件操作之前转向别的路径。
概述 node-local burst buffer file system.
 客户端GekkoFS client library  客户端是以库的形式提供的给HPC应用程序使用的。GekkoFS既没有实现基于fuse用户态文件系统的客户端，也没有实现VFS kernel的内核态的客户端。而是通过可以库的形式，截获所有文件系统的系统调用：如果是GekkoFS文件系统的文件，就通过网络转发给Gekkofs 服务端。如果不是GekkoFS文件系统，就调用原始的glibc库的系统调用。
Server端：GekkoFS daemon  Sever端通过本地的rocksdb来保存文件系统的元数据。 通过本地文件系统来保存数据。数据被切成同样大小的chunk（测试用512k）。
在客户端访问文件时，通过hash的方法映射到多个Server节点上。
总的来说这个架构还是很清晰的，附上论文里的图：

重点在于其工程实现，希望能从对它的代码分析中学到些有用的知识。</description>
    </item>
    
    <item>
      <title>向量化</title>
      <link>https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/</link>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/</guid>
      <description>Requirements  loop得有可以（ determinable (at run time)）确定的循环次数 loop里不能包含（inline不算）过程调用 loop里不能包含分支 迭代次数也得多 迭代之间最好也没有依赖性 理论上就算有依赖性也可以向量化但是不实用  现代向量处理器  依赖于大量的硬件单元而非流水线 有SIMD指令 大量依赖缓存，所以对齐很重要  几个x86 vec指令  MMX instructions  适用于 32,16 or 8-bit 整数类型的计算指令，有8 x 64-bit 的寄存器： MM0 … MM7
SSE instructions  支持：64 &amp;amp; 32-bit floating point，64, 32,16 or 8-bit integer types， 有8 x 128-bit 寄存器： xmm0 … xmm7 (xmm8 .. xmm16 in 64-bit mode)
AVX instructions  ymm0 … ymm15 寄存器相比于SSE拓展到256bits
AVX2 AVX-512  AVX-512只在 Intel KNL and Skylake上有，更强力512-bits寄存器，zmm0 … zmm15，他分四部分：Foundation（扩展32，64指令），Conflict Detection Instructions（检测要vec的loop的冲突，尽可能让更多的loop能vec），Exponential and Reciprocal Instructions（KNL里的，能支持超越运算，咱也不懂。。）， Prefetch Instructions （KNL里的预取指支持）</description>
    </item>
    
    <item>
      <title>Spectre&amp;Meltdown</title>
      <link>https://gao377020481.github.io/p/spectremeltdown/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/spectremeltdown/</guid>
      <description>Spectre&amp;amp;Meltdown
这是一种利用现代处理器特性来“窃取”内存中重要信息的漏洞。
Meltdown  诱使CPU在乱序执行的基础上将高权限的data信息放置于cache中  首先有一个数组，直接访问它的第data*4096个index处的元素 这样这个index处的元素会被放进cache中   循环遍历这个数组，当遍历到第data*4096个index处元素时，载入速度明显变快，这就说明这里是当时载入cache的元素 取到data*4096这个index就取到了data  假设data是一个内核内存空间内的数据，我们就get到了机密的内核数据，这都依赖于cpu的乱序执行：
exception(dont have priority to access))access(array[data*4096])乱序执行使得在指令生效前，access运行在了exception之前，虽然指令未生效（寄存器状态等未变），但cache内却有了array[data*4096]这个元素
Spectre Spectre基本原理与Meltdown类似，但他更强
Meltdown一旦被从根本上避免就无法使用，比如内核和用户使用不同的页目录寄存器等
Spectre并非依赖于乱序执行，而是依赖于分支预测。
分支预测也会使cpu提前跑一下cpu认为正确的分支，尽管他不一定真的是接下来要执行的，同样会在cache里留下痕迹。
但他要求代码有如下形式：
if(index1&amp;lt;array_a_size) {index2=array_a[index1];if(index2 &amp;lt; array_b_size);value = array_b[index2];}通过控制index1的长度，让array_b的特定下标的数据Cacheline被点亮，如果有办法访问一次array_b的全部内容，我们就可以窃取到index1这个值。</description>
    </item>
    
    <item>
      <title>内存优化</title>
      <link>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/</guid>
      <description>从内存结构的角度来优化我们的程序。
Vector temporaries REAL V(1024,3), S(1024), U(3)DO I=1,1024S(I) = U(1)*V(I,1)END DODO I=1,1024S(I) = S(I) + U(2)*V(I,2)END DODO I=1,1024S(I) = S(I) + U(3)*V(I,3)END DODO J=1,3DO I=1,1024V(I,J) = S(I) * U(J)END DOEND DO----------------------------------------&amp;gt;REAL V(1024,3), S, U(3)DO I=1,1024S = U(1)*V(I,1) + U(2)*V(I,2) + U(3)*V(I,3)DO J=1,3V(I,J) = S * U(J)END DOEND DO将运算组织成近似向量的形式，编译器就很容易借助这种优势来将代码优化为向量的运算。</description>
    </item>
    
    <item>
      <title>编译优化</title>
      <link>https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/</guid>
      <description>讨论一些编译器会做的优化，这些优化一般不需要我们自己来做。
但是我们讨论时从源码角度来模拟，实际上编译器是在中间代码（IR）层进行优化的。
分四部分：
IR optimisations Basic optimisations Constant folding 将一些变量转换为常量，这有助于减少内存的访问。 且常量相关的求值是可以在编译阶段就确定，不必等待运行阶段。
转换前：
integer, parameter :: n=100do i=1,n....end do转换后：
do i=1,100....end doAlgebraic simplifications 简化算术运算，包括了运用结合，交换和分配律等：
(i-j)+(i-j)+(i-j) &amp;mdash;&amp;mdash;&amp;gt; 3*i - 3*i
但是要注意浮点数操作，它不遵循结合律：
1.0 + (MF - MF) = 1.0
(1.0 + MF) - MF = 0.0
这个例子里MF比1.0大，1.0在和MF结合后会被舍去，所以第二个结果出来就是0.0了，这是因为浮点运算会先算出精确值然后舍去多余位，所以结合律失效。
Copy propagation 拷贝出来的变量在后续的使用中可以被原来的变量替代，那就少用一个寄存器。
x = yc = x + 3d = x + y--------&amp;gt;x = yc = y + 3d = y + y这里x这个变量就不需要了。</description>
    </item>
    
    <item>
      <title>struct收紧结构编译器指令</title>
      <link>https://gao377020481.github.io/p/struct%E6%94%B6%E7%B4%A7%E7%BB%93%E6%9E%84%E7%BC%96%E8%AF%91%E5%99%A8%E6%8C%87%E4%BB%A4/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/struct%E6%94%B6%E7%B4%A7%E7%BB%93%E6%9E%84%E7%BC%96%E8%AF%91%E5%99%A8%E6%8C%87%E4%BB%A4/</guid>
      <description>C/C++编译器指令 attribute((packed)) 会告诉编译器不要对齐，收紧结构，这样struct的大小就是成员大小的和。
struct xfs_agfl {__be32 agfl_magicnum;__be32 agfl_seqno;uuid_t agfl_uuid;__be64 agfl_lsn;__be32 agfl_crc;} __attribute__((packed));xfs里这个AGFL（allocation group free internal list） 就用到了这个东西。</description>
    </item>
    
    <item>
      <title>NVM下software优化（FS角度应用NVM，NVM &#43; DRAM）</title>
      <link>https://gao377020481.github.io/p/nvm%E4%B8%8Bsoftware%E4%BC%98%E5%8C%96/</link>
      <pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/nvm%E4%B8%8Bsoftware%E4%BC%98%E5%8C%96/</guid>
      <description>原文：System Software for Persistent Memory
这篇文章分析了NVM出现的情况下，软件层面（主要是内存和文件系统）的一些变革和适配。14年的文章了，intel的人参与，显然在傲腾研发的时候发出来的。
后面的nova是借鉴了这里的思路的，堆砖砌瓦，总觉得这种知识的传承和发掘很有趣。。。
这篇文章搞了个PMFS，发掘了fs在NVM上的优化思路。（细粒度logging和大规模的页IO）
背景 先来回顾一下PM发展，从一开始将NAND flash与DRAM混合使用（NVDIMM），掉电时放入NAND flash，上电再从NAND flash中加载出来到DRAM中。（电池+NAND Flash+DRAM）这样的做法，但是因为受限于NAND，这玩意还是只能block addressable。想要byte-addressable还得看NVM（PCM等做法）。对于PM主要两种思路：1. 扩展虚拟内存管理器来将PM当内存用2. 把PM当块设备用（FS里，ext4的做法）3.直接做个文件系统，跳出块设备的层来管理PM。
于是就有了PMFS：这是个POSIX semantic的fs，有一些针对NVM的特殊优化。
PMFS:
 API完善（POSIX-compliant file system interface），所以向后兼容老的应用程序。 轻量级文件系统，绕过VFS的块设备层，避免了繁杂耗时的内核发生的拷贝（page cache），过程调用等 优化的mmap，绕过page cache（需要先从disk-&amp;gt;SSD拷贝到memory-&amp;gt;DRAM），这是为了加速，但在NVM里就不需要，直接从NVM映射用户空间页表就可以，应用需要的时候直接通过虚拟内存就能寻址到  . PMFS also implements other features, such as transparent large page support [18], to further optimize memory-mapped I/O. &amp;mdash;有待研究
挑战：
 现在的内存管理器把PM作为一个memory用，因为cache（WB）的存在，写回是异步的，这个memory并不会及时更新，比如cache还未写回PM，就掉电，那数据还是要丢失，durable和reliability就无法保证。所以需要一个原语来完成这种强制写回。PM write barrier or pm_wbarrier这是一个硬件实现. 为了性能，PM被PMFS直接整个映射到内核地址空间，这就带来了stray writes的问题（就是kernel里出bug的话&amp;ndash;一般是内核驱动程序出错，指针写到我PM的这一块内核地址空间去了，发生了脏写）。一个解决办法是平时在cpu的页表里把PM设置为只读，需要写的时候先改成写，写完再改成只读。但你这样搞，来回变页表，TLB要一直更新 global TLB shootdowns，cost很大。所以intel的人实现了个utilize processor write protection control to implement uninterruptible, temporal, write windows（回头看看是啥）。 测试可靠性（主要是consistency），Intel的人开发了个东西：For PMFS validation, we use a hypervisorbased validation tool that uses record-replay to simulate and test for ordering and durability failures in PM software (§3.</description>
    </item>
    
    <item>
      <title>NVM下的新DBMS架构（VMM角度应用NVM，only NVM）</title>
      <link>https://gao377020481.github.io/p/nvm%E4%B8%8B%E7%9A%84%E6%96%B0dbms%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/nvm%E4%B8%8B%E7%9A%84%E6%96%B0dbms%E6%9E%B6%E6%9E%84/</guid>
      <description>原文：Let’s Talk About Storage &amp;amp; Recovery Methods for Non-Volatile Memory Database Systems
cmu几个大佬在16年的文章，主要讨论的是完全的NVM下的优化，偏向于应用allocator而非FS，当然他也模糊了两者之间的概念，得益于NVM的nonvolatile，allocator也能实现naming，使用固定指针存到固定位置就行（FS里当然是地址开始处由SB提供信息了）。
background OLTP的增多，简单就是数据增删改增多。所以需要优化DBMS。但是DBMS一直以来都在权衡VM(volatile memory)和NVM(non-volatile memory)，因为他们自身的特性。DRAM作为VM可以按字节寻址，具有很高的读写性能，SSD/HDD等NVM只能按block寻址，随机写的性能也很差，一般都需要利用其顺序写速度快的特性来做优化(LSM in leveldb/rocksdb)。这里有一点：server里40%的能源都被flash DRAM用掉，DRAM需要一直刷新，即使这片数据不用也要刷新，不刷新就消失了。而SSD/HDD这些的缺点就不用说了，很慢，且写放大读放大都存在。
NVM是个概念，有一系列的具体实现技术，他就是兼具DRAM的性能和寻址又掉电保留数据。目前的DBMS都没有好好利用这个东西：
磁盘型DBMS如mysql orcal在内存中有一个cache，积累一下然后去顺序的以block写入磁盘，内存型DBMS如redis voltdb都有自己的持久化策略来缓和DRAM的易失性。这些多余的组件在NVM中都没必要。
这篇文章是讲一种只使用NVM的情况下的DBMS，不是DRAM和NVM混合架构也不是只用NVM做log(nova就有点这个意思)。
说一下模拟：
使用的是intel的一个工具，有一个microcode，他会用dram模拟nvm，当nvm需要停顿的时候就让cpustall。（ The microcode estimates the additional cycles that the CPU would have to wait if DRAM is replaced by slower NVM and then stalls the CPU for those cycles. ）
直接用libnuma来提供malloc，然后用fence来确保数据不会在cache中停留而是直接进入NVM。 文件系统也有对应的api，一个基于byte-addressable的FS。当然IO还是要过VFS tip：基于mmap可以不过VFS，特殊的mmap甚至可以掠过kernel里的page cache直接写数据到disk（NVMM）进一步加快速度。
这里用memory allocator，但是restart的时候不好找到数据，文件系统有superblock可以找到inode数据结构获取文件目录结构信息，memory没办法。所以改进一下：NVM-aware Memory Allocator
 虚拟内存到NVM的映射是固定的，基于此保存指向这个地址的pointer，每次重启拿到这个pointer就可以获取到之前的数据信息。 避免数据在cache中停留引发的不一致，依次使用clflush将cache中的data写回数据，再用SFENCE确保clflush在这里完成（对所有processor可见，因为你现在把cache刷回去了那你得让cache刷回到所有proccsor，cache分级的，每个processor都有自己的呢，我理解实现上把cacheline置为steal就可以了），之后提交事务，这就是sync原语。  参照 三个存储引擎类型： (1) inplace updates engine, (2) copy-on-write updates engine, and (3） log-structured updates engine</description>
    </item>
    
    <item>
      <title>OFI/libfabric</title>
      <link>https://gao377020481.github.io/p/ofi-for-hpc/</link>
      <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/ofi-for-hpc/</guid>
      <description>之前一直很好奇，在超算里（或大规模的集群里），是怎么减少network传输的损耗的，最近项目中接触到了mecury这个rpc框架，他依赖于OFI、libfabric这个网络框架来做集群内的高性能网络传输。
OFI相比于普通的内核TCP/IP协议栈的优化思路与用户态协议栈有异曲同工之妙，准备好好写一下这篇笔记，先从内核里的TCP/IP协议栈的缺点开始，到优化思路与实现，再看看能不能学习一下ofi实现的trick。开个坑先，慢慢写。。
从缺点找灵感 TCP/IP 缺点
 用于提供可靠性的header占用很多带宽 同步的话耗时，异步的话耗空间（内核里的缓冲区）拷贝也耗时  高性能的网络API应该是什么样的？ 尽量少的内存拷贝 两点：
 用户提供buffer，与协议栈用一个buffer 建立一个流量控制机制  异步操作 两种策略：
 中断和信号，这种机制会打断正在运行的程序，evict CPU cache，而且一个随时能接受信号还不影响自己工作的程序就比较难开发 事件queue， 来了就进queue  Direct Hardware Access 主要两种思路：
 越过kernel，直接与网卡的buffer交互（代表DPDK） 硬件软件配合来在用户空间共享同一块地址空间作为buffer（RDMA）  那应该怎么设计呢？ 先抄来个需要的interface：
/* Notable socket function prototypes *//* &amp;quot;control&amp;quot; functions */int socket(int domain, int type, int protocol);int bind(int socket, const struct sockaddr *addr, socklen_t addrlen);int listen(int socket, int backlog);int accept(int socket, struct sockaddr *addr, socklen_t *addrlen);int connect(int socket, const struct sockaddr *addr, socklen_t addrlen);int shutdown(int socket, int how);int close(int socket); /* &amp;quot;fast path&amp;quot; data operations - send only (receive calls not shown) */ssize_t send(int socket, const void *buf, size_t len, int flags);ssize_t sendto(int socket, const void *buf, size_t len, int flags,const struct sockaddr *dest_addr, socklen_t addrlen);ssize_t sendmsg(int socket, const struct msghdr *msg, int flags);ssize_t write(int socket, const void *buf, size_t count);ssize_t writev(int socket, const struct iovec *iov, int iovcnt);/* &amp;quot;indirect&amp;quot; data operations */int poll(struct pollfd *fds, nfds_t nfds, int timeout);int select(int nfds, fd_set *readfds, fd_set *writefds,fd_set *exceptfds, struct timeval *timeout); 首先来看看这几类都有什么目标？</description>
    </item>
    
    <item>
      <title>存储领域概念整理</title>
      <link>https://gao377020481.github.io/p/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/</guid>
      <description>Non-Volatile Memory (NVM) 是相对DRAM（掉电后数据丢失）而言的，指可以持久化保存数据的存储介质
Non Volatile Main Memory (NVMM) NVMM是对多种新型非易失存储介质的统称，目前的NVMM包括：相变存储器 Phase-Change Memory (PCM)、忆阻器 Memristor（ReRAM）、自旋扭矩转换随机存储器 Spin Transfer Torque - Magnetic Random Access Memory (STT-MRAM)等等。
DIMM全称Dual-Inline-Memory-Modules，中文名叫双列直插式存储模块，是指奔腾CPU推出后出现的新型内存条，它提供了64位的数据通道。
NAND Flash可以做成不同接口和不同形式的闪存设备：SATA接口一般用于普通固态硬盘（SSD）产品、PCIe接口则常用于高端服务器闪存产品。NVMM由于可以媲美DRAM的性能，可以做成类似内存条形式直接插在主板的DIMM插槽上。这类产品可以称为持久化内存，Persistent Memory (PM)，或存储级内存，Storage Class Memory (SCM)。
NVDIMM NVDIMM又是什么呢？实际上早已有之，是基于NAND Flash的非易失型内存条——通常被做成“电池+NAND Flash+DRAM”的形式：在掉电时用电池电量将DRAM数据刷回NAND Flash实现持久化。
3D Xpoint 英特尔同时也推出了基于3D Xpoint技术的Optane SSD，采用PCIe接口。相比基于NAND Flash的企业级SSD在顺序读写上似乎并没有太大提升，顺序写大约在2000MB/s的水平。但得益于稳定的低时延——读写均为10us，其4KB随机读写性能非常逆天,随机写达到500000 IOPS。</description>
    </item>
    
    <item>
      <title>一次关于proc的分析</title>
      <link>https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/</guid>
      <description>一次关于proc的分析 运行：
strace file /proc/version来一小段
execve(&amp;quot;/usr/bin/file&amp;quot;, [&amp;quot;file&amp;quot;, &amp;quot;/proc/version&amp;quot;], 0x7ffc3c1ee838 /* 25 vars */) = 0brk(NULL) = 0x5591347ae000access(&amp;quot;/etc/ld.so.nohwcap&amp;quot;, F_OK) = -1 ENOENT (No such file or directory)access(&amp;quot;/etc/ld.so.preload&amp;quot;, R_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, &amp;quot;/etc/ld.so.cache&amp;quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, {st_mode=S_IFREG|0644, st_size=36168, ...}) = 0mmap(NULL, 36168, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f48f3973000close(3) = 0/etc/ld. so.nohwcap 这个文件存在，链接器就只会去load完全无优化的库版本（哪怕CPU支持优化）
/etc/ld.so.preload 这个文件就跟LD_PRELOAD 干一个事情，这玩意在GekkoFS的系统调用截取时用过，preload一下自己的IO库，当发生IO操作，链接器会讲call的func链接到自己的库上去而不是走正常的posix fileIOapi。</description>
    </item>
    
    <item>
      <title>Openmpi</title>
      <link>https://gao377020481.github.io/p/cmake/</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/cmake/</guid>
      <description>Openmpi 初步使用 安装与测试 直接官网下载release包
wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.1.tar.gz linux下解压:
tar -zxf openmpi-4.1.1.tar.gz 进入开始configure： prefix 为指定安装路径
cd openmpi-4.1.1/ ./configure --prefix=/usr/local/openmpi 安装：
make sudo make install 设置环境变量
sudo vim /etc/profile 加入：
export PATH=/usr/local/openmpi/bin:$PATH export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH 生效：
source /etc/profile 测试
mpicc --version 写代码测试：hello.c
#include &amp;lt;stdio.h&amp;gt;#include &amp;#34;mpi.h&amp;#34; int main(int argc, char* argv[]) { int rank, size, len; char version[MPI_MAX_LIBRARY_VERSION_STRING]; MPI_Init(&amp;amp;argc, &amp;amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;amp;rank); MPI_Comm_size(MPI_COMM_WORLD, &amp;amp;size); MPI_Get_library_version(version, &amp;amp;len); printf(&amp;#34;Hello, world, I am %d of %d, (%s, %d)\n&amp;#34;, rank, size, version, len); MPI_Finalize(); return 0; } 编译并运行,我这里是四核虚拟机</description>
    </item>
    
    <item>
      <title>QUIC</title>
      <link>https://gao377020481.github.io/p/quic/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/quic/</guid>
      <description>QUIC
quick udp internet connection
是由 google 提出的使用 udp 进行多路并发传输的协议
优势 Quic 相比现在广泛应用的 http2+tcp+tls 协议有如下优势
 减少了 TCP 三次握手及 TLS 握手时间。 改进的拥塞控制。 避免队头阻塞的多路复用。 连接迁移。 前向冗余纠错。  0RTT建连 传输层和加密层总共只需要0RTT就可以建立连接。
因为其握手数据和HTTP数据一同发送，建连过程可以认为是0RTT的。
灵活的拥塞控制 QUIC默认使用了 TCP 协议的 Cubic 拥塞控制算法，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。
但其：
可插拔  该拥塞控制算法实现于应用层，不需要修改内核就可以对其快速迭代 同一程序的不同连接使用不同拥塞控制算法，针对不同用户做适配 变更算法只需要修改配置再重新加载，不需要停机  递增的packet number和维持顺序的stream offset 使用严格递增的packet number，即使是相同包的重发也递增。接收端就可以区分开这个包是重发的还是之前的。
避免了在计算RTT的时候引发的歧义问题，因为发送方RTT计算需要计算的边界是包发出和包收到两处，如果使用重发包的时刻作为左边界，收到ack的时刻作为右边界，万一这个ack是初始发出的包的而不是重发的那就统计小了。
SACK选项空间更大 QUIC的SACK选项空间256Bytes 对比TCP的30Bytes很大，能够提供更多已经收到segment的信息，方便发送端进行精度更高的选择重传
Ack Delay Ack Delay是在接收端进行处理的时间，该时间也需要记录并发送，TCP的timestamp区域并不记录这个，计算的RTT理论上就更大不够准确。
QUIC在计算RTT时会减去Ack Delay
基于 stream 和 connecton 级别的流量控制 connection: TCP连接 ，复用：一个connection上可能有多个stream</description>
    </item>
    
    <item>
      <title>micro 和 marco benchmarks</title>
      <link>https://gao377020481.github.io/p/benchmark/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/benchmark/</guid>
      <description>Micro-benchmarks (repeatable sections of code) can be useful but may not represent real-world behavior. Factors that can skew micro-benchmark performance include Java virtual machine warm-up time, and global code interactions.
Macro-benchmarks (repeatable test sequences from the user point of view) test your system as actual end users will see it.</description>
    </item>
    
    <item>
      <title>HTTP 1 to 2</title>
      <link>https://gao377020481.github.io/p/http-1-to-2/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/http-1-to-2/</guid>
      <description>HTTP/1.1 建连 输入网址
 有网址，无IP。所以先生成DNS查询报文，置于目的端口53的UDP报文中。目的地址就是建网时通过DHCP获得的DNS服务器IP。但是还无网关的MAC，所以使用ARP查询报文，其目的IP为网关IP，MAC地址为全1（广播）。网关收到后就通过ARP回答返回给客户机他自己的MAC地址。客户机拿到网关MAC就继续组装好DNS查询报文，发送给网关路由器。 DNS查询被网关转发到了最终DNS服务器，服务器查询到网址对应ip并返回给客户机。 客户机拿到IP就生成TCP套接字然后向IP所处机器发起连接请求，三次握手建立连接后向其发送HTTP GET报文。 网址服务器返回一个HTTP相应报文。 客户机拿到后浏览器渲染一下显示出来。  缺点   队头阻塞
  低效的 TCP 利用
  臃肿的消息首部
  受限的优先级设置
  HTTP2 </description>
    </item>
    
    <item>
      <title>STL</title>
      <link>https://gao377020481.github.io/p/stl/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/stl/</guid>
      <description>Alloc c++内存空间管理模块， 包含于:
&amp;lt;memory&amp;gt;:
 &amp;lt;stl_construct.h&amp;gt;: 定义全局函数construct(),destory()，负责对象析构与构造 &amp;lt;stl_alloc.h&amp;gt;: 定义一二级配置器，名为alloc &amp;lt;stl_uninitialized.h&amp;gt;: 一些全局函数用来操作大块内存数据  construct&amp;amp;destory construct类似C++的placement new。
destory存在两版本：
 版本一接收一指针参数，析构掉指针处的对象 版本二接收两指针参数，析构掉两指针之间的所有对象  alloc alloc空间配置器分一级配置器和二级配置器：
 一级配置器就是直接封装malloc和free 二级配置器：  维护16个自由链表组成内存池，分别负责十六种小型区块配置，填充内存池调用malloc 出现内存不足，或要求区块大于128Bytes转一级配置器    二级配置器：
 对于所需求区块无空闲块的情况直接去更大区块处查找，找到后出现的碎片会挂到与它大小匹配的区块链表上 都没有就分配40块需求区块，20块挂在链表上，20块作为后备  free free是如何知道对象的大小呢，这是因为malloc在分配内存时，会在对象的内存块的上下区域添加内存大小的cookie，VC6中malloc添加的是对象大小+1.不管怎么样，这都足够free去get到需要归还给操作系统的内存空间大小了，当然所谓归还也只是还给CRTL的内存池，只有空闲内存到达某种条件（例如整整几页的空间都空闲，他们可能还连续），那就使用系统调用来让内核释放掉这一块虚拟内存（其实也就是进task_struct里找到mm_sturct然后操作里面的bitmap，将要释放的内存对应的块设置为再次可用）。
Iterator  迭代器的根本还是一个指针，当然它是一种智能指针 通过指针萃取到指针所指对象的类型或一些其他信息很重要，这就是traits技法  分类上个侯捷老师的图：

traits 利用template自动推导和偏特化实现普遍可用的萃取特性方法，直接记录代码：
template &amp;lt;class T&amp;gt; struct MyIter { typedef T value_type; // 内嵌型别声明  T* ptr; MyIter(T* p = 0) : ptr(p) {} T&amp;amp; operator*() const { return *ptr; } }; // class type泛化版本 template &amp;lt;class T&amp;gt; struct iterator_traits { typedef typename T::value_type value_type; }; // 原生指针偏特化版本 template &amp;lt;class T&amp;gt; struct iterator_traits&amp;lt;T*&amp;gt; { typedef T value_type; }; // const对象指针偏特化版本 template &amp;lt;class T&amp;gt; struct iterator_traits&amp;lt;const T*&amp;gt; { typedef T value_type; }; template &amp;lt;class I&amp;gt; typename iterator_traits&amp;lt;I&amp;gt;::value_type //typename 告诉编译器这是一个type func(I ite) { std::cout &amp;lt;&amp;lt; &amp;#34;normal version&amp;#34; &amp;lt;&amp;lt; std::endl; return *ite; } 当然value_type只是常用的一种，STL还有其他几种，就不写了。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;对象</title>
      <link>https://gao377020481.github.io/p/c-%E5%AF%B9%E8%B1%A1/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/c-%E5%AF%B9%E8%B1%A1/</guid>
      <description>读书笔记：深度探索c++对象模型
C++ 对象模型 可用的对象模型有很多种，C++对象模型派生于简单对象模型，这里直接说C++对象模型：
 Non-static data member 存在于每一个object内 Static data member 存在于每一个object外 Static 与 Non-static function members 也被存放在所有object外 virtual function members：object内存在指向虚表vtb的指针（一般用于支持RTTI的type_info object的指针置于vtb第一个slot）  继承 虚继承 base class不管被继承多少次，永远只存在一个实体，虚继承产生的派生类只是有路径指向对应的base class实体。
这里的“路径”可以是类似虚表的指针（bptr）也可以是一系列指针，视存取效率而定。
然而，这样的话object的布局和大小会随着继承体系的改变而改变，所以这一部分的内容应被存放于对象模型的特殊部分。这个特殊部分就是“共享局部”，其他的固定不变的部分就是“不变局部”。所以为了实现虚继承，引入了两个局部的概念，虽然实现方式各编译器有所不同，但两个局部的概念一致。
构造 默认构造函数 如果没有user-declared constructor, 一个trivial 的constructor可能被生成， 但他啥都不干。
  只有编译器需要时，non-trivial 的constructor才会被编译器合成，但他也只干编译器需要的事情，比如A内有一个B类的成员（组合），B有一个构造函数，那么A也就需要一个non-trivial 的constructor，这个构造函数只会构造B，而不会管A内可能存在的其他成员。
  如果A内有B,C,D三个类的成员，这三个类都有自己的构造函数，显然编译器会为A生成一个构造函数，他会依次调用B,C,D的构造函数，顺序取决于三成员在A内的排列顺序。
  回到1，如果A内其他成员比如一个int值在user-declared constructor里初始化但是user-declared constructor内未初始化B，那编译器怎么办呢，编译器会扩写user-declared constructor，给里面加上个B的构造函数的调用。
  类内有虚函数存在，编译器需要扩张已有构造函数或生成一个构造函数来完成虚表的初始化操作
  虚继承的使用也会给构造函数增加工作量，编译器需要让构造函数给类添加执行期判断的能力，比如在派生类中添加一个bptr提供指向唯一基类实体的路径。
  默认拷贝构造函数 三种调用情况：
 X x1 = x2 变量赋值 f(x1) 函数传参 return x1 返回值  不需要默认拷贝构造函数：</description>
    </item>
    
    <item>
      <title>File System</title>
      <link>https://gao377020481.github.io/p/file-system/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/file-system/</guid>
      <description>JOS JOS不像其他操作系统一样在内核添加磁盘驱动，然后提供系统调用。我们实现一个文件系统进程来作为磁盘驱动。
 引入一个文件系统进程（FS进程）的特殊进程，该进程提供文件操作的接口,并被赋予io权限。（x86处理器使用EFLAGS寄存器的IOPL为来控制保护模式下代码是否能执行设备IO指令，比如in和out。） 建立RPC机制，客户端进程向FS进程发送请求，FS进程真正执行文件操作，并将数据返回给客户端进程。 更高级的抽象，引入文件描述符。通过文件描述符这一层抽象就可以将控制台，pipe，普通文件，统统按照文件来对待。（文件描述符和pipe实现原理） 支持从磁盘加载程序并运行。  结构 superblock
依然使用超级块来记录文件系统的元数据
file
File struct用来描述文件：文件名，大小，类型，保存文件内容的block号
tips: Directories与file结构相同，只是内容是一些file
Block Cache 在FS进程的虚拟内存空间中映射一段磁盘区域。
FS进程在创建时，set特殊的缺页处理函数。
当发生缺页中断时call那个缺页处理函数，从磁盘上把数据读入物理内存。
根据FS进程内存地址空间的映射关系，FS可以很方便的通过虚拟内存找到刚读入的数据在物理内存中的位置。
The Block Bitmap 磁盘块的是否使用的bitmap
The file system interface 文件系统建立好后，还需要通过ipc来构建供用户进程操作文件的API栈，课程的图拿来用一下：
 Regular env FS env+---------------+ +---------------+| read | | file_read || (lib/fd.c) | | (fs/fs.c) |...|.......|.......|...|.......^.......|...............| v | | | | RPC mechanism| devfile_read | | serve_read || (lib/file.c) | | (fs/serv.c) || | | | ^ || v | | | || fsipc | | serve || (lib/file.</description>
    </item>
    
    <item>
      <title>Preemptive Multitasking</title>
      <link>https://gao377020481.github.io/p/preemptive-multitasking/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/preemptive-multitasking/</guid>
      <description>JOS 多CPU支持 支持&amp;quot;symmetric multiprocessing&amp;quot; (SMP)， 启动阶段CPU分为BSP和AP，BSP负责初始化系统和启动操作系统，AP由BSP激活。哪一个CPU是BSP由硬件和BISO决定，到目前位置所有JOS代码都运行在BSP上。在SMP系统中，每个CPU都有一个对应的local APIC（LAPIC），负责传递中断。CPU通过内存映射IO(MMIO)访问它对应的APIC，这样就能通过访问内存达到访问设备寄存器的目的。BSP读取mp configuration table中保存的CPU信息，初始化cpus数组，ncpu（总共多少可用CPU），bootcpu指针（指向BSP对应的CpuInfo结构）。然后BSP通过在内存上写值向AP传递中断来启动其他cpu（为他们设置寄存器值等操作）。
CPU私有数据：1.内核栈 2.TSS 3.env 4.寄存器
显然以上私有数据都需要创建新的一份。
多CPU执行内核代码，需要锁来避免竞争，使用CAS机制实现一个内核锁就可以。当然这个粒度太大，在linux内核中有各种粒度的实现。
协作调度 实现yield函数由进程调用主动让出cpu，显然需要将yield注册为一项系统调用，由内核来真正的做切换进程的工作。这里的调度也就是最简单的FIFO。
fork 提供系统调用fork给用户创建进程的能力，fork()拷贝父进程的地址空间和寄存器状态到子进程。父进程从fork()返回的是子进程的进程ID，而子进程从fork()返回的是0。父进程和子进程有独立的地址空间，任何一方修改了内存，不会影响到另一方。
基于写时复制的原理，子进程只需要在一开始拷贝父进程的页目录就可以，当真正触发写操作的时候再在缺页处理函数里做真正的拷贝。因为用户进程中已经有拷贝需要的所有信息（物理页位置等），所以只需要在用户进程中调用用户进程自己的缺页处理函数就可以。所以：
 需要在进程fork的时候就设置新进程的缺页处理函数 同时fork的时候也要对页复制做对应处理（共享，写时复制，只读三种情况不同） 因为要在用户进程中处理异常，所以需要新建一个用户异常栈保存用于异常处理的函数需要的参数。 缺页中断发生时：trap()-&amp;gt;trap_dispatch()-&amp;gt;page_fault_handler() 这个page_fault_handler会进入汇编代码然后给用户异常栈赋好值再切换到用户栈，基于刚赋好的值，用户进程会直接执行真正的用户缺页处理函数，在这个却也处理函数里会判断是否因为写时复制导致的触发，是的话就拷贝这个物理页到新的地方然后建立虚拟地址到物理页的映射关系。 还有一点需要注意的是内核代码组织十分严格，所以默认内核态不会出现缺页异常，一旦出现可能内核被攻击了，所以在一开始page_fault_handler里需要判断由内核进程触发的话就要panic整个系统。  定时 外部时钟中断强制进入内核，内核判断当前周期到了没，可以将中断号+偏移量来控制时钟周期，到了就触发对应的处理函数。拿时间片轮转调度进程举例：在SMP上首先通过LAPIC来通知各个cpu然后让出进程。
IPC Inter-Process communication
进程间通信，这里进程间通信使用使两个进程的虚拟地址指向同一块物理页的机制来完成。调用recv的进程阻塞（让出cpu），调用send的进程陷入内核查找对应的recv进程，和其要接受到的虚拟地址，首先将要发送的物理地址找到，然后修改recv进程的要接受到的虚拟地址对应的页表项，将其映射到那个要发送的物理地址处。然后设置接收进程为就绪态等待内核调度。
Linux Kernel 涉及进程调度、锁、进程通信
进程调度 调度器 核心调度器
调度器的实现基于两个函数：周期性调度器函数和主调度器函数。这些函数根据现有进程的优先级分配CPU时间。这也是为什么整个方法称之为优先调度的原因。
a.主调度器函数 在内核中的许多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数（schedule）。 主调度器负责将CPU的使用权从一个进程切换到另一个进程。周期性调度器只是定时更新调度相关的统计信息。cfs队列实际上是用红黑树组织的，rt队列是用链表组织的。
b.周期性调度器函数
周期性调度器在scheduler_tick中实现，如果系统正在活动中，内核会按照频率HZ自动调用该函数。该函数主要有两个任务如下：
 更新相关统计量：管理内核中与整个系统和各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1。比如运行时间。 激活负责当前进程的调度类的周期性调度方法。  调度类
为方便添加新的调度策略，Linux内核抽象一个调度类sched_class，允许不同进程有针对性的选择调度算法。
运行队列
每个处理器有一个运行队列，结构体是rq。rq是描述就绪队列，其设计是为每一个CPU就绪队列，本地进程在本地队列上排序。cfs和rt。
调度进程
主动调度进程的函数是schedule() ，它会把主要工作委托给__schedule()去处理。
函数__shcedule的主要处理过程如下：
  调用pick_next_task()以选择下一个进程。
  调用context_switch()以切换进程。 调用context_switch：
  切换用户虚拟地址空间
  切换寄存器</description>
    </item>
    
    <item>
      <title>User Environments</title>
      <link>https://gao377020481.github.io/p/user-environments/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/user-environments/</guid>
      <description>JOS 这一部分做三件事：
 建立进程概念 异常处理，陷入内核 系统调用  进程和用户环境 JOS里进程是一个env结构，里面保存了简单的信息：页目录、寄存器缓存项（用于切换），连接指针（用于调度），状态等
首先提供基础设施：内核中保存进程的链表，这个数组需要初始化在物理内存上
显然创建一个进程，需要创建其对应的页目录，那就要在物理内存上分配对应的页目录和页表结构然后把他映射到虚拟内存里的页目录上。
要在进程上运行程序，就需要加载和解析ELF文件，同样分配物理页，载入elf文件各个segment，建立虚拟地址到物理地址的映射关系，然后修改寄存器的值为elf的entry。就可以运行起来elf上的程序了。
运行一个进程就只需要将进程结构内保存的寄存器值弹出到寄存器里。
trap与异常处理 一般来讲当异常发生，cpu会触发电信号，触发硬中断然后由中断控制器找到中断处理函数，但也有软中断的时候，通过指令提供的中断号结合IDT来查找到对应的中断处理函数。
中断发生，陷入内核，处理器根据TSS寄存器找到TSS结构，将栈寄存器SS和ESP分别设置为其中的SS0和ESP0两个字段的值，这样就切换到内核栈了
缺页异常与系统调用 缺页异常是一个中断，中断号是14，将这个号压入内核栈，然后call trap函数，在trap里dispatch到对应的处理函数就行。缺页的话就要分配物理页然后加载磁盘数据再建立映射关系，这一套操作目前由内核完成。
系统调用是一个中断，我们设置中断处理函数sys_call，并在trap内部根据传入的系统调用号做对应的dispatch到对应的系统调用处理。上面内核栈已经切换成功，其实cpu就在内核态了，接下来只需要压入触发系统调用对应的int指令需要的中断号和其它参数就可以call trap，然后dispatch到sys_call并传递参数（参数保存在一个trapframe结构中）。
Linux Kernel 进程 Linux内核把进程称为任务(task)，进程的虚拟地址空间分为用户虚拟地址空间和内核虚拟地址空间，所有进程共享内核虚拟地址空间，每个进程有独立的用户空间虚拟地址空间。
进程有两种特殊形式：没有用户虚拟地址空间的进程称为内核线程，共享用户虚拟地址空间的进程称为用户线程。通用在不会引起混淆的情况下把用户线程简称为线程。共享同一个用户虚拟地址空间的所有用户线程组成一个线程组。
四要素：
a.有一段程序供其执行。
b.有进程专用的系统堆栈空间。
c.在内核有task_struct数据结构。
d.有独立的存储空间，拥有专有的用户空间。
如果只具备前三条而缺少第四条，则称为“线程”。如果完全没有用户空间，就称为“内核线程”。而如果共享用户空间映射就称为“用户线程”。内核为每个进程分配一个task_struct结构时。实际分配两个连续物理页面(8192字节)，数据结构task_struct的大小约占1kb字节左右，进程的系统空间堆栈的大小约为7kb字节（不能扩展，是静态确定的）。</description>
    </item>
    
    <item>
      <title>Memory Management</title>
      <link>https://gao377020481.github.io/p/memory-management/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/memory-management/</guid>
      <description>内存管理
JOS JOS中主要是在做：
 线性地址到物理地址的映射（通过操作修改页表项实现） 在物理内存上划分出内核的页目录和页表存储区域并完成映射 保护模式的映射关系与段寄存器相关，32位的JOS通过段选择子找到段描述符，再解析段描述符的基地址，检查权限位后加上虚拟地址来实现 物理内存的分配在JOS内目前还无相应算法，只是单纯的连续分配  Linux kernel 最上层拿C++来说，其STL用于容器的alloc分配器的实现，又是基于malloc的，使用开链数组来存放多个线性增长大小的空闲内存块，当有分配需要时一次申请多块（应该是20块）该大小内存。且找不到对应块时还具有相应的回收，拼接，查找功能。对于大块内存的分配使用单独的路径来分配和存储。分层分级分区的思路是各级内存分配器的惯用思路。
然后低一层的malloc根据选择动态链接库的不同（就使用不同的分配器lib），有不同实现，以下以glibc为例。
至于内核中内存分配，由于其确定性，多用slab来做分配，也存在buddy allocation机制来管理。
虚拟地址一般都是大于物理地址的，所以只需要搞好虚拟地址上的内存管理就行了，至于物理页如果dirty，自然有page fault interrupt 来操心。当然以下提到物理页连续等，是默认了虚拟页到物理页的映射关系，实际上mmap,brk这样的系统调用都不会真的操作物理页，很多时候都是系统告诉我们：“你要的物理页已经准备好了”，实际上他也就是改了一下task_struct里mm_struct里的一些页表项罢了（准备好的是映射，真正的物理页可还是dirty的）。
所以内存分配器到底在哪一层呢，实际上内存分配器能看到的还就是虚拟内存，他一直在做修改页映射的操作，他并不会做将某个文件放到内存里的工作，这个工作显然是由缺页中断完成的紧跟其后的就是磁盘IO（以一个read操作为例&amp;ndash;，read -&amp;gt; vfs_read(首先要找到inode: 顺序是：check fd 在不在进程的打开文件列表里， 在的话找到file结构然后顺着找到dentry接着找到inode，check一下在不在page cache里这里就要通过inode里的address_space找到那颗组织page cache的基数树在上面查询了，查询用的是文件偏移量，不在就通过VFS里的inode做对应文件系统的operation，以xfs为例子，那就是解析inode数据结构，首先肯定要去它的data fork去找，xfs的data fork组织成两种形式，extent list or b+ tree, 无论是哪一种都能找到所有相关的block号了，一个一个把他们读出来就行了，这个工作显然是交给通用块设备层去做了，这层有点像VFS，下面马上就是各种不同的硬件驱动，上面是统一的请求，所以这一层用于整合，再下去就是到IO调度层，内核根据一些策略来做IO调度，这是因为磁盘读一次比较慢，尽量把连续扇区放一起读更快，再下去就真的从磁盘上读数据了。），只是它感性的认为自己在操作物理内存，比如：“太好了，这次我分配了0X10到0X2000这么多连续的内存空间”，实际上他就是把0X10到0X2000这一块虚拟内存的页表项改了一下，让虚拟页映射到了某一段连续物理页，实际的物理的IO并未发生呢
glibc (example for user-space) 以ptmalloc为例：
malloc时，如果内存分配区（主，非主）真的无空间，调用mmap或brk/sbrk来在虚拟内存上取一块作为新的空间，mmap取得内存在mmap区域，brk/sbrk在heap上（在进程的描述符task_struct中的mm_struct里可以找到）。主分配区可以使用sbrk和mmap向os申请内存，而非分配区只能通过mmap向os申请内存。接下来os可能（因为mmap只是给一块虚拟地址，真实数据还未拷贝到物理内存上）要去物理内存上找可用区块然后将真实数据拷贝上去（用户触发缺页syscall），这里再涉及物理页的分配。
ptmalloc的内存管理方法就可以很好的解决每次要内存都要去向OS要的问题。它回收管理分配出的内存而不是每次都还给操作系统而是放入bin中留待下次使用，bin的整理等一些其他操作会在特定时候触发。
首先每个进程都有一个主分配区和几个非主分配区，分配区在多线程间共享，对分配区的操作需要加线程互斥锁。主分配区由一号或主线程拥有。
最小的内存管理单位为chunk，一段连续的内存被分成多个chunk，chunk内记录当前和前一个chunk的大小，用于合并。
下一层级是分配区中的bins，bins分为:
 fast bin: 保存小块的chunk bins: 2.1 unsorted bin： 是一个chunk缓冲区，这里的chunk都是在等待整理的chunk（释放或合并出来的），同时也有可能是即将被用得到的chunk，因为很多程序会频繁的分配释放相同大小的内存，它在fastbin里找不到就会直接来这里找，速度快。chunk的size 没有限制。 2.2 small bin： 类似alloc分配器的开链数组实现，大小小于512字节的chunk被称为small chunk，分级每个相差8KB放入small bin对应槽位。共62个不同大小的bin 2.3 large bin： 与small bin类似，只是其中存的是大chunk，且不单纯线性增长，共63个不同大小的bin  chunk的分配与释放是一个很复杂的管理流程，这里只说管理层级，不谈细致流程。
kernel space buddy 连续的物理页称为页块（page block），阶（order）是页的数量单位，2的n次方个连续页称为n阶页块。 如下条件的两个n阶页块称为伙伴（buddy）：</description>
    </item>
    
    <item>
      <title>Boot</title>
      <link>https://gao377020481.github.io/p/boot/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/boot/</guid>
      <description>以MIT6.828的分节方式来记录操作系统的笔记。
所以第一章： Boot 启动
JOS 在JOS里，Boot作为Lab1存在。
实模式下运行BIOS，BIOS设置eip以启动bootloader，boot loader检查系统状态，开启保护模式，将磁盘上的kernel加载入物理内存，设置eip启动kernel。Kernel开启分页。
BIOS BIOS的启动依赖于硬件的电信号，在qemu虚拟机里模拟了这样一个信号来启动BIOS。
Bootloader Bootloader：
 从实模式进入保护模式，加载全局描述符表（虚拟地址到线性（物理地址）的映射开启） 从磁盘加载kernel到内存（通过读取ELF文件的方式）  Kernel 进入Kernel后：
 开启分页（就是在物理内存的特定位置创建内核页目录和页表数组，实现线性地址到物理地址的映射关系） 这里还基于内存映射的关系，实现了向IO HOLE区域（VGA显存）写值的功能，得以在终端上输出了字符  Linux kernel 在linux kernel中，这一环节的基本流程很相似，参考深入理解Linux内核附录1记录一个很简要的流程：
BIOS 硬件电信号拉起ROM上的BIOS程序，BIOS启动后简单检查和初始化一下硬件，随后在磁盘扇区上搜索操作系统来启动，找到磁盘第一个扇区（引导扇区）后将其拷贝到物理内存的0X00007C00处。
Bootloader 物理内存上有bootloader的第一部分了，第一部分可能会移动他的位置并将第二部分再装入物理内存的特定位置，第二部分会从磁盘中读取OS的映射表，提供给用户选项，选择启动哪一个操作系统，选中后bootloader就会调用BIOS过程来不停的装载内核映像，其中setup()函数在0X00090200处，下一步会跳转到这里
setup() 初始化硬件、和一些寄存器等，并跳转到startup_32()
startup_32() 初始化临时堆栈，段寄存器并解压内核映像放置到物理内存上，然后跳转到内核映像上启动
解压的内核映像启动点仍是一个叫做startup_32()的函数，它会再检查一下硬件软件信息然后的跳转到start_kernel()函数
start_kernel() 完成linux内核初始化工作，具体工作过多，这里不说</description>
    </item>
    
    <item>
      <title>Mysql</title>
      <link>https://gao377020481.github.io/p/mysql/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/mysql/</guid>
      <description>视图 视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。其内容由查询定义。 基表：用来创建视图的表叫做基表。 通过视图，可以展现基表的部分数据。 视图数据来自定义视图的查询中使用的表，使用视图动态生成。
优点  简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。  CREATEVIEW&amp;lt;视图名&amp;gt;AS&amp;lt;SELECT语句&amp;gt;触发器 触发器（trigger）是MySQL提供给程序员和数据分析员来保证数据完整性的一种方法，它是与表事件相关的特殊的存储过程，它的执行不是由程序调用，也不是手工启动，而是由事件来触发，比如当对一个表进行DML操作（ insert ， delete ， update ）时就会激活它执行。
监视对象： table
监视事件： insert 、 update 、 delete
触发时间： before ， after
触发事件： insert 、 update 、 delete
CREATETABLE`work`(`id`INTPRIMARYKEYauto_increment,`address`VARCHAR(32))DEFAULTcharset=utf8ENGINE=INNODB;CREATETABLE`time`(`id`INTPRIMARYKEYauto_increment,`time`DATETIME)DEFAULTcharset=utf8ENGINE=INNODB;CREATETRIGGERtrig_test1AFTERINSERTON`work`FOREACHROWINSERTINTO`time`VALUES(NULL,NOW());存储过程 SQL语句需要先编译然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。
存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。
优点  能完成较复杂的判断和运算 有限的编程 可编程行强，灵活 SQL编程的代码可重复使用 执行的速度相对快一些 减少网络之间的数据传输，节省开销  CREATEPROCEDURE过程名([[IN|OUT|INOUT]参数名数据类型[,[IN|OUT|INOUT]参数名数据类型…]])[特性...]过程体存储过程根据需要可能会有输入、输出、输入输出参数，如果有多个参数用&amp;quot;,&amp;ldquo;分割开。
MySQL 存储过程的参数用在存储过程的定义，共有三种参数类型 IN , OUT , INOUT 。
IN ：参数的值必须在调用存储过程时指定，0在存储过程中修改该参数的值不能被返回，可以设置默认值
OUT ：该值可在存储过程内部被改变，并可返回
INOUT ：调用时指定，并且可被改变和返回
过程体的开始与结束使用 BEGIN 与 END 进行标识。</description>
    </item>
    
    <item>
      <title>Redis</title>
      <link>https://gao377020481.github.io/p/redis/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/redis/</guid>
      <description>Redis
Remote Dictionary Service， 远程字典服务，内存式数据库，非关系型，KV结构
三张网图描述redis基本数据结构：   
数据与编码 String 字符数组，该字符串是动态字符串，字符串长度小于1M时，加倍扩容；超过1M每次只多扩1M；字符串最大长度为512M；
Tips：redis字符串是二进制安全字符串；可以存储图片，二进制协议等二进制数据；
 字符串长度小于等于 20 且能转成整数，则使用 int 存储； 字符串长度小于等于 44，则使用 embstr 存储； 字符串长度大于 44，则使用 raw 存储；  44为界
首先说明redis以64字节作为大小结构分界点，但其sdshdr和redisobject结构会占用一些空间，所以真正保存数据的大小小于64字节
旧版本使用39为界，新版本使用44为界，这是因为旧版本中sdshdr占用8字节目前的sdshdr8是针对小结构的优化（大结构使用shshdr16，64），仅占用3字节，节省了5字节空间。所以新版本以44为界。
raw和embstr
raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间存储两结构
embstr使用连续内存，更高效的利用缓存，且一次内存操作带来了更好的创建和销毁效率
操作方式与转换
  以一个浮点数的value作为例子，浮点数会被转换成字符串然后存储到数据库内。如果要对V进行操作，他也会先从字符串转换为浮点数然后再进行操作。
  以一个整数2000的value作为例子，该数会被保存为int但使用append进行追加一个字符串“is a good number！”后， 该值会被转换为embstr， 然而embstr的对象从redis视角看来是只读的（未实现操作embstr的方法）， 所以该对象又会被转换raw然后实行相应操作并保存为raw
  List 双向链表，很容易理解，但其node有讲究（压缩时使用ziplist）
列表中数据是否压缩的依据：
 元素长度小于 48，不压缩； 元素压缩前后长度差不超过 8，不压缩；  直接放数据结构，然后分析:</description>
    </item>
    
    <item>
      <title>异常处理</title>
      <link>https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid>
      <description>Try Catch组件实现 先说明try catch组件使用setjump和longjump实现
setjump longjump语法 Int setjmp(jmp_buf env);
返回值：若直接调用则返回0，若从longjmp调用返回则返回非0值的longjmp中的val值
Void longjmp(jmp_buf env,int val);
调用此函数则返回到语句setjmp所在的地方，其中env 就是setjmp中的 env，而val 则是使setjmp的返回值变为val。
当检查到一个错误时,则以两个参数调用longjmp函数，第一个就是在调用setjmp时所用的env，第二个参数是具有非0值val，它将成为从setjmp处返回的值。使用第二个参数的原因是对于一个setjmp可以有多个longjmp。
jmp_buf env;环境
setjump(env)设置回跳点，返回longjump(env,out)传的参数out，配套使用，longjump可穿越函数跳转
jmp_buf env; int c = setjump(env); longjump(env,3); 这里longjump后就会跳回setjump这一行，并且setjump会返回3，也就是c = 3。
int count = 0; jmp_buf env; void a(int indx) { longjump(env,indx); } int main() { int idx = 0; count = setjump(env); if(count == 0) { a(env,idx++); } else if (count == 1) { a(env,idx++); } else { printf(&amp;#34;ok&amp;#34;); } return 0; } 如上，函数a会调回开头setjump处，如果是这样a调用多次，a又没有返回（a运行到longjump处进入了，没返回），a的栈会不会还存在，存在的话如果有无数个a，会不会发生栈溢出。</description>
    </item>
    
    <item>
      <title>线程池进阶版</title>
      <link>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/</guid>
      <description>#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;string.h&amp;gt; #include &amp;lt;pthread.h&amp;gt; #define LL_ADD(item, list) do { \ item-&amp;gt;prev = NULL;	\ item-&amp;gt;next = list;	\ list = item;	\ } while(0)  #define LL_REMOVE(item, list) do {	\ if (item-&amp;gt;prev != NULL) item-&amp;gt;prev-&amp;gt;next = item-&amp;gt;next;	\ if (item-&amp;gt;next != NULL) item-&amp;gt;next-&amp;gt;prev = item-&amp;gt;prev;	\ if (list == item) list = item-&amp;gt;next;	\ item-&amp;gt;prev = item-&amp;gt;next = NULL;	\ } while(0)  typedef struct NWORKER {//工作线程信息 	pthread_t thread; //线程id 	int terminate; //是否要终止 	struct NWORKQUEUE *workqueue; //线程池，用于找到工作队列 	struct NWORKER *prev; struct NWORKER *next; } nWorker; typedef struct NJOB { //工作个体 	void (*job_function)(struct NJOB *job); void *user_data; struct NJOB *prev; struct NJOB *next; } nJob; typedef struct NWORKQUEUE { struct NWORKER *workers; //所有工作线程的链表 	struct NJOB *waiting_jobs; //工作队列 	pthread_mutex_t jobs_mtx; pthread_cond_t jobs_cond; } nWorkQueue; typedef nWorkQueue nThreadPool; static void *ntyWorkerThread(void *ptr) { //工作线程取用工作 	nWorker *worker = (nWorker*)ptr; while (1) { pthread_mutex_lock(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //先获取工作队列的操作互斥锁  while (worker-&amp;gt;workqueue-&amp;gt;waiting_jobs == NULL) { if (worker-&amp;gt;terminate) break; pthread_cond_wait(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_cond, &amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //如果工作队列为空，这个线程就阻塞在条件变量上等待事件发生 	} if (worker-&amp;gt;terminate) { pthread_mutex_unlock(&amp;amp;worker-&amp;gt;workqueue-&amp;gt;jobs_mtx); //如果检测到工作线程被终止，那么这个线程就需要结束工作，但在结束工作前需要将对工作队列的取用权限放开，所以这里在break前需要解锁这个互斥锁 	break; } nJob *job = worker-&amp;gt;workqueue-&amp;gt;waiting_jobs; //从工作队列中获取一个工作 	if (job !</description>
    </item>
    
    <item>
      <title>线程池</title>
      <link>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</guid>
      <description>线程池 基本功能模块：  线程池创建函数 线程池删除函数 线程池回调函数 线程池添加函数 线程池数据结构 线程任务数据结构 线程本身数据结构（由pid唯一确认）  首先实现数据结构： 线程任务数据结构：
struct nTask { void (*task_func)(struct nTask *task); void *user_data; struct nTask *prev; struct nTask *next; }; 这是任务中的一个个体，任务队列头存储在线程池数据结构中 void (*task_func)(struct nTask *task)函数指针表明函数为task_func且参数为struct nTask， 参数若为void是否更好
线程本身数据结构：
struct nWorker { pthread_t threadid; int terminate; struct nManager *manager; struct nWorker *prev; struct nWorker *next; }; pid唯一标识线程，terminate用于标识该线程应被删除，存储manager（也就是所属线程池）是为了通过manager找到task队列以获取task
线程池数据结构：
typedef struct nManager { struct nTask *tasks; struct nWorker *workers; pthread_mutex_t mutex; pthread_cond_t cond; } ThreadPool; 可以看到线程池其实只是一个管理者，使用mutex控制各个线程对进程内公共资源的访问，保证同时只有一个线程在访问公共资源，cond来控制各个线程的状态（处于等待队列（阻塞）或可以运行（运行、就绪态））细节在回调函数中</description>
    </item>
    
    <item>
      <title>Reactor</title>
      <link>https://gao377020481.github.io/p/reactor/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/reactor/</guid>
      <description>Reactor  组成：⾮阻塞的io + io多路复⽤； 特征：基于事件循环，以事件驱动或者事件回调的⽅式来实现业务逻辑； 表述：将连接的io处理转化为事件处理；  单Reactor 
代表：redis 内存数据库 操作redis当中的数据结构 redis 6.0 多线程
单reactor模型 + 任务队列 + 线程池 
代表：skynet
多Reactor 
应⽤： memcached accept(fd, backlog) one eventloop per thread
 多进程应用  
应⽤：nginx
多reactor + 消息队列 + 线程池 </description>
    </item>
    
    <item>
      <title>定时器</title>
      <link>https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/</guid>
      <description>定时器 定时器作用很多，常见于心跳检测，冷却等等 实现时区别主要在于定时器队列的管控（排序）
基本方案： 红黑树： 使用红黑树对定时器排序，排序依据为定时器过期时间，每隔单位时间检查红黑树中最小时间是否小于等于当前时间，如果小于等于就删除节点并触发节点的callback。时间复杂度增删O(logn)，Nginx使用红黑树。删除添加操作自旋。
最小堆： 最小堆根节点最小，直接拿出根节点与当前时间比较即可，删除操作将根节点与末尾节点对换并删除末尾节点然后将新的根节点下沉，添加时加入末尾节点并上升。
时间轮：  时间轮可以分为单层级与多层级。简单的单层级时间轮使用初始化好的链表数组来存储对应的事件节点链表，时间数据结构中一般包含引用计数，该数据结构只有在引用计数置零后销毁，一般也代表着事件对应的资源可以释放。单层时间轮的大小至少需要大于最长定时时间/单位时间，举例：每5秒发送一个心跳包，连接收到心跳包时需要开启一个10秒的定时器并将事件引用计数加一（事件数据结构插入链表数组中10秒后的链表中），也就是最长定时10秒，10秒后检查该连接对应的事件并将引用计数减一，如果减一后为0就说明连接超时，释放所有资源，关闭事件。在该例子中，初始化的链表数组大小至少为11，因为假如在第0秒来一个心跳包，我们就需要在第10号位置将该连接对应的事件节点加入事件链表中，如果小于11，比如为8，那从0开始往后10个的位置就是在2号位置，那2秒后就得触发了，这与我们设置的10秒定时时间不一致。

代码实现： 红黑树： 红黑树数据结构直接使用nginx自带的rbtree头文件，就不自己写了
红黑树定时器头文件： #ifndef _MARK_RBT_ #define _MARK_RBT_  #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdint.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stddef.h&amp;gt; #if defined(__APPLE__) #include &amp;lt;AvailabilityMacros.h&amp;gt;#include &amp;lt;sys/time.h&amp;gt;#include &amp;lt;mach/task.h&amp;gt;#include &amp;lt;mach/mach.h&amp;gt;#else #include &amp;lt;time.h&amp;gt;#endif  #include &amp;#34;rbtree.h&amp;#34; ngx_rbtree_t timer; static ngx_rbtree_node_t sentinel; typedef struct timer_entry_s timer_entry_t; typedef void (*timer_handler_pt)(timer_entry_t *ev); struct timer_entry_s { ngx_rbtree_node_t timer; timer_handler_pt handler; }; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) 	struct timespec ti; clock_gettime(CLOCK_MONOTONIC, &amp;amp;ti); t = (uint32_t)ti.</description>
    </item>
    
    <item>
      <title>进阶TCP服务器</title>
      <link>https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</guid>
      <description>进阶TCP服务器 该模块涉及TCP服务器常见点：并发量、IO模型
IO网络模型 阻塞IO （Blocking IO） 
阻塞IO的情况下，我们如果需要更多的并发，只能使用多线程，一个IO占用一个线程，资源浪费很大但是在并发量小的情况下性能很强。
非阻塞IO （Non-Blocking IO） 
在非阻塞状态下，recv() 接口在被调用后立即返回，返回值代表了不同的含义。如在本例中，
 recv() 返回值大于 0，表示接受数据完毕，返回值即是接受到的字节数； recv() 返回 0，表示连接已经正常断开； recv() 返回 -1，且 errno 等于 EAGAIN，表示 recv 操作还没执行完成； recv() 返回 -1，且 errno 不等于 EAGAIN，表示 recv 操作遇到系统错误 errno。  非阻塞的接口相比于阻塞型接口的显著差异在于，在被调用之后立即返回。使用如下的函数可以将某句柄 fd 设为非阻塞状态：
fcntl( fd, F_SETFL, O_NONBLOCK ); 多路复用IO （IO Multiplexing） 
多路复用IO，select/poll、epoll。
select/poll select和poll很相似，在检测IO时间的时候都需要遍历整个FD存储结构，只是select使用数组存储FD，其具有最大值限制，而poll使用链表无最大值限制（与内存大小相关）。
先来分析select的优缺点，这样就知道epoll相比select的优势等。
select 本质上是通过设置或检查存放fd标志位的数据结构进行下一步处理。 这带来缺点： 单个进程可监视的fd数量被限制，即能监听端口的数量有限 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试 一般该数和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认1024个，64位默认2048。
当socket较多时，每次select都要通过遍历FD_SETSIZE个socket，不管是否活跃，这会浪费很多CPU时间。如果能给 socket 注册某个回调函数，当他们活跃时，自动完成相关操作，即可避免轮询，这就是epoll与kqueue。
select 调用流程</description>
    </item>
    
    <item>
      <title>设计模式</title>
      <link>https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid>
      <description>设计模式 首先先上所有设计模式的原则，这些原则贯彻于每一种设计模式中，是各种设计模式的根
原则   依赖倒置原则
 高层模块不应该依赖低层模块，⼆者都应该依赖抽象。 抽象不应该依赖具体实现，具体实现应该依赖于抽象。    开放封闭原则
 ⼀个类应该对扩展开放，对修改关闭。    面向接口编程
 不将变量类型声明为某个特定的具体类，而是声明为某个接⼝。 客户程序无需获知对象的具体类型，只需要知道对象所具有的接口。 减少系统中各部分的依赖关系，从而实现“高内聚、松耦合”的类型设计⽅案。    封装变化点
 将稳定点和变化点分离，扩展修改变化点；让稳定点与变化点的实现层次分离。    单一职责原则
 ⼀个类应该仅有⼀个引起它变化的原因。    里氏替换原则
 子类型必须能够替换掉它的父类型；主要出现在⼦类覆盖父类实现，原来使用父亲类型的程序可能出现错误；覆盖了父类方法却没实现父类方法的职责；    接口隔离原则
 不应该强迫客户依赖于他们不用的方法。 ⼀般用于处理⼀个类拥有比较多的接口，而这些接口涉及到很多职责    对象组合优于类继承
 继承耦合度⾼，组合耦合度低    模板模式 
首先，先提下使用到的原则：
 依赖倒置原则 单一职责原则 接口隔离原则  定义： 定义⼀个操作中的算法的骨架 ，⽽将⼀些步骤延迟到子类中。 Template Method使得子类可以不 改变⼀个算法的结构即可重定义该算法的某些特定步骤。</description>
    </item>
    
    <item>
      <title>连接池</title>
      <link>https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/</guid>
      <description>连接池实现 mysql连接池 头文件： #ifndef DBPOOL_H_ #define DBPOOL_H_  #include &amp;lt;iostream&amp;gt;#include &amp;lt;list&amp;gt;#include &amp;lt;mutex&amp;gt;#include &amp;lt;condition_variable&amp;gt;#include &amp;lt;map&amp;gt;#include &amp;lt;stdint.h&amp;gt; #include &amp;lt;mysql.h&amp;gt; #define MAX_ESCAPE_STRING_LEN	10240  using namespace std; // 返回结果 select的时候用 class CResultSet { public: CResultSet(MYSQL_RES* res); virtual ~CResultSet(); bool Next(); int GetInt(const char* key); char* GetString(const char* key); private: int _GetIndex(const char* key); MYSQL_RES* m_res; MYSQL_ROW	m_row; map&amp;lt;string, int&amp;gt;	m_key_map; }; // 插入数据用 class CPrepareStatement { public: CPrepareStatement(); virtual ~CPrepareStatement(); bool Init(MYSQL* mysql, string&amp;amp; sql); void SetParam(uint32_t index, int&amp;amp; value); void SetParam(uint32_t index, uint32_t&amp;amp; value); void SetParam(uint32_t index, string&amp;amp; value); void SetParam(uint32_t index, const string&amp;amp; value); bool ExecuteUpdate(); uint32_t GetInsertId(); private: MYSQL_STMT*	m_stmt; MYSQL_BIND*	m_param_bind; uint32_t	m_param_cnt; }; class CDBPool; class CDBConn { public: CDBConn(CDBPool* pDBPool); virtual ~CDBConn(); int Init(); // 创建表 	bool ExecuteCreate(const char* sql_query); // 删除表 	bool ExecuteDrop(const char* sql_query); // 查询 	CResultSet* ExecuteQuery(const char* sql_query); /** * 执行DB更新，修改 * * @param sql_query sql * @param care_affected_rows 是否在意影响的行数，false:不在意；true:在意 * * @return 成功返回true 失败返回false */ bool ExecuteUpdate(const char* sql_query, bool care_affected_rows = true); uint32_t GetInsertId(); // 开启事务 	bool StartTransaction(); // 提交事务 	bool Commit(); // 回滚事务 	bool Rollback(); // 获取连接池名 	const char* GetPoolName(); MYSQL* GetMysql() { return m_mysql; } private: CDBPool* m_pDBPool;	// to get MySQL server information 	MYSQL* m_mysql;	// 对应一个连接 	char	m_escape_string[MAX_ESCAPE_STRING_LEN + 1]; }; class CDBPool {	// 只是负责管理连接CDBConn，真正干活的是CDBConn public: CDBPool() {} CDBPool(const char* pool_name, const char* db_server_ip, uint16_t db_server_port, const char* username, const char* password, const char* db_name, int max_conn_cnt); virtual ~CDBPool(); int Init();	// 连接数据库，创建连接 	CDBConn* GetDBConn(const int timeout_ms = -1);	// 获取连接资源 	void RelDBConn(CDBConn* pConn);	// 归还连接资源  const char* GetPoolName() { return m_pool_name.</description>
    </item>
    
    <item>
      <title>内存池</title>
      <link>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/</guid>
      <description>内存池实现（注释详细） #include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #define MP_ALIGNMENT 32 //对齐信息 #define MP_PAGE_SIZE	4096 //单次分配大块大小 #define MP_MAX_ALLOC_FROM_POOL	(MP_PAGE_SIZE-1)  #define mp_align(n, alignment) (((n)+(alignment-1)) &amp;amp; ~(alignment-1)) #define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) &amp;amp; ~(alignment-1))  struct mp_large_s { struct mp_large_s *next; void *alloc; }; // 当单次分配超过pagesize时就需要一次分配然后归入large的一个链表中保存  struct mp_node_s { unsigned char *last; unsigned char *end; struct mp_node_s *next; size_t failed; };// 页，用于小块的分配,last指向页内使用到的位置  struct mp_pool_s { size_t max; struct mp_node_s *current; struct mp_large_s *large; struct mp_node_s head[0]; }; //内存池  struct mp_pool_s *mp_create_pool(size_t size); void mp_destory_pool(struct mp_pool_s *pool); void *mp_alloc(struct mp_pool_s *pool, size_t size); void *mp_nalloc(struct mp_pool_s *pool, size_t size); void *mp_calloc(struct mp_pool_s *pool, size_t size); void mp_free(struct mp_pool_s *pool, void *p); //首先需要明确，在分配的时候需要将所有的数据结构都存在我们管理的内存池中 //比如struct mp_pool_s *pool这个内存池本身也需要受我们管理 struct mp_pool_s *mp_create_pool(size_t size) { struct mp_pool_s *p; int ret = posix_memalign((void **)&amp;amp;p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s)); //posix_memalign 分配足够的内存，size（page_size：4096） 加上内存池本身和小块结构本身 	if (ret) { return NULL; } p-&amp;gt;max = (size &amp;lt; MP_MAX_ALLOC_FROM_POOL) ?</description>
    </item>
    
    <item>
      <title>请求池</title>
      <link>https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/</guid>
      <description>请求池实现 同步阻塞请求池 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;errno.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt; #include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/socket.h&amp;gt;#include &amp;lt;netinet/in.h&amp;gt; #include &amp;lt;sys/epoll.h&amp;gt;#include &amp;lt;netdb.h&amp;gt;#include &amp;lt;arpa/inet.h&amp;gt; #include &amp;lt;pthread.h&amp;gt; #define DNS_SVR	&amp;#34;114.114.114.114&amp;#34;  #define DNS_HOST	0x01 #define DNS_CNAME	0x05  struct dns_header { unsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item { char *domain; char *ip; }; int dns_create_header(struct dns_header *header) { if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-&amp;gt;id = random(); header-&amp;gt;flags |= htons(0x0100); header-&amp;gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-&amp;gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-&amp;gt;qname == NULL) return -2; question-&amp;gt;length = strlen(hostname) + 2; question-&amp;gt;qtype = htons(1); question-&amp;gt;qclass = htons(1); const char delim[2] = &amp;#34;.</description>
    </item>
    
    <item>
      <title>锁</title>
      <link>https://gao377020481.github.io/p/%E9%94%81/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E9%94%81/</guid>
      <description>锁 自旋锁 当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -&amp;gt; 等待的机制被称为自旋锁(spinlock)。
自旋锁的原理比较简单，如果持有锁的线程能在短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。
因为自旋锁避免了操作系统进程调度和线程切换，所以自旋锁通常适用在时间比较短的情况下。由于这个原因，操作系统的内核经常使用自旋锁。但是，如果长时间上锁的话，自旋锁会非常耗费性能，它阻止了其他线程的运行和调度。线程持有锁的时间越长，则持有该锁的线程将被 OS(Operating System) 调度程序中断的风险越大。如果发生中断情况，那么其他线程将保持旋转状态(反复尝试获取锁)，而持有该锁的线程并不打算释放锁，这样导致的是结果是无限期推迟，直到持有锁的线程可以完成并释放它为止。
解决上面这种情况一个很好的方式是给自旋锁设定一个自旋时间，等时间一到立即释放自旋锁。自旋锁的目的是占着CPU资源不进行释放，等到获取锁立即进行处理。但是如何去选择自旋时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用 CPU 资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！
在计算任务轻的情况下使用自旋锁可以显著提升速度，这是因为线程切换的开销大于等锁的开销，但是计算任务重的话自旋锁的等待时间就成为主要的开销了。
互斥锁 互斥锁实际是一个互斥量，为获得互斥锁的线程会挂起，这就涉及到线程切换的开销，计算任务重的情况下会比较适合使用。
读写锁 读写锁即只能由一人写但可以由多人读的锁，适用于读操作很多但写操作很少的情况下。
原子操作 多线程下使用原子操作确保我们的操作不会被其他线程参与，一般内联汇编 memory 内存屏障，只允许这一缓存写回内存，确保多线程安全。
多进程下对共享内存的操作使用内联汇编lock，锁住总线，同一时刻只允许一个进程通过总线操作内存。
粒度大小排序
互斥锁 &amp;gt; 自旋锁 &amp;gt; 读写锁 &amp;gt; 原子操作
代码实现 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;pthread.h&amp;gt;#include &amp;lt;unistd.h&amp;gt; #include &amp;lt;sys/mman.h&amp;gt; #define THREAD_SIZE 10  int count = 0; pthread_mutex_t mutex; pthread_spinlock_t spinlock; pthread_rwlock_t rwlock; // MOV dest, src; at&amp;amp;t // MOV src, dest; x86  int inc(int *value, int add) { int old; __asm__ volatile ( //lock 锁住总线 只允许一个cpu操作内存（通过总线）确保多进程安全  &amp;#34;lock; xaddl %2, %1;&amp;#34; // &amp;#34;lock; xchg %2, %1, %3;&amp;#34;  : &amp;#34;=a&amp;#34; (old) : &amp;#34;m&amp;#34; (*value), &amp;#34;a&amp;#34; (add) : &amp;#34;cc&amp;#34;, &amp;#34;memory&amp;#34; //memory 内存屏障，只允许这一缓存写回内存，确保多线程安全  ); return old; } // void *func(void *arg) { int *pcount = (int *)arg; int i = 0; while (i++ &amp;lt; 100000) { #if 0//无锁 (*pcount) ++; #elif 0// 互斥锁版本  pthread_mutex_lock(&amp;amp;mutex); (*pcount) ++; pthread_mutex_unlock(&amp;amp;mutex); #elif 0// 互斥锁非阻塞版本  if (0 !</description>
    </item>
    
    <item>
      <title>简易http客户端(C posix API)</title>
      <link>https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/</guid>
      <description>HTTP 实现http客户端程序
基础 HTTP使用TCP连接
HTTP报文：
 
实现 域名到ip地址转换(dns) 直接调用api进行转换比较简单：
char * host_to_ip(const char* hostname) { struct hostent *host_entry = gethostbyname(hostname); if(host_entry) { return inet_ntoa(*(struct in_addr*)*host_entry -&amp;gt; h_addr_list); } return NULL; } host_entry存储了dns请求的接收，从中取出第一个ip地址并将点分十进制转换为字符串返回
创建TCP套接字（建立连接） posix api创建
int http_create_socket(char *ip) { int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in sin = {0}; sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr.s_addr = inet_addr(ip); if(0 != connect(sockfd, (struct sockaddr*)&amp;amp;sin, sizeof(struct sockaddr_in))) { return -1; } fcntl(sockfd, F_SETFL, O_NONBLOCK); return sockfd; } fcntl(sockfd, F_SETFL, O_NONBLOCK);这个函数用于设置该套接字io为非阻塞</description>
    </item>
    
    <item>
      <title>DNS协议解析</title>
      <link>https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/</guid>
      <description>DNS Domain Name System 域名系统，是一个分布式数据库，用于映射IP与域名
每级域名长度限制为63，总长度限制为253，使用TCP UDP端口53
DNS分层 顶级域：com org等 第二级域：baidu google等 第三级域：www edu等
域名解析 静态映射：在本机上配置域名和ip映射并直接使用
动态映射：使用DNS域名解析系统，在DNS服务器上配置ip到域名的映射
域名服务器 根域名服务器： 共a-m十三台（十三个ip）但拥有很多镜像服务器，镜像与本体使用同一个ip，存有顶级域名服务器的ip 顶级域名服务器：管理在该顶级域名服务器下注册的二级域名 权限域名服务器：一个区域的域名解析 本地域名服务器：处理本地的请求，保存本地的映射
域名解析方式 迭代查询：本机请求本地域名服务器，本地域名服务器开始迭代的查询各个层级服务器，先查询根获得顶级的ip然后根据获得的ip查询顶级在获得区域的ip依次迭代查到请求的映射
递归查询：递归查询时只发出一次请求然后等待接收到最终结果，在上面的步骤中本机使用的就是递归查询
协议报文格式 dns_dp dns_dp dns_dp dns_dp
具体查看文档
DNS client UDP编程 首先需要自己定义数据结构用于存储dns报文
struct dns_header{ unsigned short id; unsigned short flags; unsigned short questions; unsigned short answer; unsigned short authority; unsigned short additional; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; unsigned char *name; }; 这里只需要question和header是因为我们作为client只实现发送A请求也就是获取域名的ipv4地址，在实现中header的授权码和附加码都不需要使用只需要使用questions id和flags即可</description>
    </item>
    
    <item>
      <title>简易Tcp服务器</title>
      <link>https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/</guid>
      <description>Tcp服务器 一请求一线程 首先说明问题： 已请求一线程能承载的请求数量极少，posix标准线程8M，请求数量多时极其占用内存
简单实现 实现一请求一线程很简单：
#define BUFFER_SIZE 1024 void *client_routine(void *arg) { int clientfd = *(int *) arg; while(1) { char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len &amp;lt;0 ) { close(clientfd); break; } else if(len ==0 ) { close(clientfd); break; } else{ printf(&amp;#34;Recv: %s, %d btye(s) from %d\n&amp;#34;, buffer, len, clientfd); } } } int main(int argc, char *argv[]) { if(argc &amp;lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(&amp;amp;addr, 0, sizeof(struct sockaddr_in)); addr.</description>
    </item>
    
    <item>
      <title>Mysql基本知识</title>
      <link>https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</guid>
      <description>MYSQL mysql安装与配置 在虚拟机上安装mysql,使用apt-get install就可以 这里我只检索到了mysql-server-5.7就安装了5.7
在本地win10上安装mysqlbench用于连接虚拟机的mysql服务器 这里使用网络连接，可能是因为mysql版本的原因，本来应该在/etc/mysql中的my.cnf文件中显式的配置有基本信息，我只需要修改部分，但5.7在/etc/mysql/mysql.conf.d/mysqld.cnf,在它的基础上修改对应的bind-address为0.0.0.0保证回环地址可访问：
# # The MySQL database server configuration file. # # You can copy this to one of: # - &amp;#34;/etc/mysql/my.cnf&amp;#34; to set global options, # - &amp;#34;~/.my.cnf&amp;#34; to set user-specific options. # # One can use all long options that the program supports. # Run program with --help to get a list of available options and with # --print-defaults to see which it would actually understand and use.</description>
    </item>
    
    <item>
      <title>ubuntu-server虚拟机配置</title>
      <link>https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/</guid>
      <description>ubuntu-server虚拟机配置 Sh配置 安装完ubuntu后先配置sh这样可以通过xshell连接 只需要：
sudo apt-get install openssh-server ssh即可 Samba配置 然后安装samba
sudo apt-get install samba 创建share文件夹
cd /home sudo mkdir share sudo chmod 777 share 然后在/etc/samba里配置smb.conf 文件 在文件尾部添加：
[share] comment = My Samba path = /home/gao/share browseable = yes writeable = yes 然后设置密码
sudo smbpasswd -a gao 然后去主机上映射盘符就可以方便的访问 在文件框内输入\192.168.134.xxx 也就是虚拟机ip就可以 把share映射为盘符
gcc配置 换apt阿里源：
cd /etc/apt sudo mv source.list source.list.back sudo vim source.list 改为：
deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties deb http://mirrors.</description>
    </item>
    
  </channel>
</rss>

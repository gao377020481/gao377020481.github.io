<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OS on Gao&#39;s Happy Day</title>
    <link>https://gao377020481.github.io/tags/os/</link>
    <description>Recent content in OS on Gao&#39;s Happy Day</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://gao377020481.github.io/tags/os/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>一次关于proc的分析</title>
      <link>https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/</guid>
      <description>一次关于proc的分析 运行：
strace file /proc/version来一小段
execve(&amp;quot;/usr/bin/file&amp;quot;, [&amp;quot;file&amp;quot;, &amp;quot;/proc/version&amp;quot;], 0x7ffc3c1ee838 /* 25 vars */) = 0brk(NULL) = 0x5591347ae000access(&amp;quot;/etc/ld.so.nohwcap&amp;quot;, F_OK) = -1 ENOENT (No such file or directory)access(&amp;quot;/etc/ld.so.preload&amp;quot;, R_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, &amp;quot;/etc/ld.so.cache&amp;quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, {st_mode=S_IFREG|0644, st_size=36168, ...}) = 0mmap(NULL, 36168, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f48f3973000close(3) = 0/etc/ld. so.nohwcap 这个文件存在，链接器就只会去load完全无优化的库版本（哪怕CPU支持优化）
/etc/ld.so.preload 这个文件就跟LD_PRELOAD 干一个事情，这玩意在GekkoFS的系统调用截取时用过，preload一下自己的IO库，当发生IO操作，链接器会讲call的func链接到自己的库上去而不是走正常的posix fileIOapi。</description>
    </item>
    
    <item>
      <title>File System</title>
      <link>https://gao377020481.github.io/p/file-system/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/file-system/</guid>
      <description>JOS JOS不像其他操作系统一样在内核添加磁盘驱动，然后提供系统调用。我们实现一个文件系统进程来作为磁盘驱动。
 引入一个文件系统进程（FS进程）的特殊进程，该进程提供文件操作的接口,并被赋予io权限。（x86处理器使用EFLAGS寄存器的IOPL为来控制保护模式下代码是否能执行设备IO指令，比如in和out。） 建立RPC机制，客户端进程向FS进程发送请求，FS进程真正执行文件操作，并将数据返回给客户端进程。 更高级的抽象，引入文件描述符。通过文件描述符这一层抽象就可以将控制台，pipe，普通文件，统统按照文件来对待。（文件描述符和pipe实现原理） 支持从磁盘加载程序并运行。  结构 superblock
依然使用超级块来记录文件系统的元数据
file
File struct用来描述文件：文件名，大小，类型，保存文件内容的block号
tips: Directories与file结构相同，只是内容是一些file
Block Cache 在FS进程的虚拟内存空间中映射一段磁盘区域。
FS进程在创建时，set特殊的缺页处理函数。
当发生缺页中断时call那个缺页处理函数，从磁盘上把数据读入物理内存。
根据FS进程内存地址空间的映射关系，FS可以很方便的通过虚拟内存找到刚读入的数据在物理内存中的位置。
The Block Bitmap 磁盘块的是否使用的bitmap
The file system interface 文件系统建立好后，还需要通过ipc来构建供用户进程操作文件的API栈，课程的图拿来用一下：
 Regular env FS env+---------------+ +---------------+| read | | file_read || (lib/fd.c) | | (fs/fs.c) |...|.......|.......|...|.......^.......|...............| v | | | | RPC mechanism| devfile_read | | serve_read || (lib/file.c) | | (fs/serv.c) || | | | ^ || v | | | || fsipc | | serve || (lib/file.</description>
    </item>
    
    <item>
      <title>Preemptive Multitasking</title>
      <link>https://gao377020481.github.io/p/preemptive-multitasking/</link>
      <pubDate>Mon, 16 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/preemptive-multitasking/</guid>
      <description>JOS 多CPU支持 支持&amp;quot;symmetric multiprocessing&amp;quot; (SMP)， 启动阶段CPU分为BSP和AP，BSP负责初始化系统和启动操作系统，AP由BSP激活。哪一个CPU是BSP由硬件和BISO决定，到目前位置所有JOS代码都运行在BSP上。在SMP系统中，每个CPU都有一个对应的local APIC（LAPIC），负责传递中断。CPU通过内存映射IO(MMIO)访问它对应的APIC，这样就能通过访问内存达到访问设备寄存器的目的。BSP读取mp configuration table中保存的CPU信息，初始化cpus数组，ncpu（总共多少可用CPU），bootcpu指针（指向BSP对应的CpuInfo结构）。然后BSP通过在内存上写值向AP传递中断来启动其他cpu（为他们设置寄存器值等操作）。
CPU私有数据：1.内核栈 2.TSS 3.env 4.寄存器
显然以上私有数据都需要创建新的一份。
多CPU执行内核代码，需要锁来避免竞争，使用CAS机制实现一个内核锁就可以。当然这个粒度太大，在linux内核中有各种粒度的实现。
协作调度 实现yield函数由进程调用主动让出cpu，显然需要将yield注册为一项系统调用，由内核来真正的做切换进程的工作。这里的调度也就是最简单的FIFO。
fork 提供系统调用fork给用户创建进程的能力，fork()拷贝父进程的地址空间和寄存器状态到子进程。父进程从fork()返回的是子进程的进程ID，而子进程从fork()返回的是0。父进程和子进程有独立的地址空间，任何一方修改了内存，不会影响到另一方。
基于写时复制的原理，子进程只需要在一开始拷贝父进程的页目录就可以，当真正触发写操作的时候再在缺页处理函数里做真正的拷贝。因为用户进程中已经有拷贝需要的所有信息（物理页位置等），所以只需要在用户进程中调用用户进程自己的缺页处理函数就可以。所以：
 需要在进程fork的时候就设置新进程的缺页处理函数 同时fork的时候也要对页复制做对应处理（共享，写时复制，只读三种情况不同） 因为要在用户进程中处理异常，所以需要新建一个用户异常栈保存用于异常处理的函数需要的参数。 缺页中断发生时：trap()-&amp;gt;trap_dispatch()-&amp;gt;page_fault_handler() 这个page_fault_handler会进入汇编代码然后给用户异常栈赋好值再切换到用户栈，基于刚赋好的值，用户进程会直接执行真正的用户缺页处理函数，在这个却也处理函数里会判断是否因为写时复制导致的触发，是的话就拷贝这个物理页到新的地方然后建立虚拟地址到物理页的映射关系。 还有一点需要注意的是内核代码组织十分严格，所以默认内核态不会出现缺页异常，一旦出现可能内核被攻击了，所以在一开始page_fault_handler里需要判断由内核进程触发的话就要panic整个系统。  定时 外部时钟中断强制进入内核，内核判断当前周期到了没，可以将中断号+偏移量来控制时钟周期，到了就触发对应的处理函数。拿时间片轮转调度进程举例：在SMP上首先通过LAPIC来通知各个cpu然后让出进程。
IPC Inter-Process communication
进程间通信，这里进程间通信使用使两个进程的虚拟地址指向同一块物理页的机制来完成。调用recv的进程阻塞（让出cpu），调用send的进程陷入内核查找对应的recv进程，和其要接受到的虚拟地址，首先将要发送的物理地址找到，然后修改recv进程的要接受到的虚拟地址对应的页表项，将其映射到那个要发送的物理地址处。然后设置接收进程为就绪态等待内核调度。
Linux Kernel 涉及进程调度、锁、进程通信
进程调度 调度器 核心调度器
调度器的实现基于两个函数：周期性调度器函数和主调度器函数。这些函数根据现有进程的优先级分配CPU时间。这也是为什么整个方法称之为优先调度的原因。
a.主调度器函数 在内核中的许多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数（schedule）。 主调度器负责将CPU的使用权从一个进程切换到另一个进程。周期性调度器只是定时更新调度相关的统计信息。cfs队列实际上是用红黑树组织的，rt队列是用链表组织的。
b.周期性调度器函数
周期性调度器在scheduler_tick中实现，如果系统正在活动中，内核会按照频率HZ自动调用该函数。该函数主要有两个任务如下：
 更新相关统计量：管理内核中与整个系统和各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1。比如运行时间。 激活负责当前进程的调度类的周期性调度方法。  调度类
为方便添加新的调度策略，Linux内核抽象一个调度类sched_class，允许不同进程有针对性的选择调度算法。
运行队列
每个处理器有一个运行队列，结构体是rq。rq是描述就绪队列，其设计是为每一个CPU就绪队列，本地进程在本地队列上排序。cfs和rt。
调度进程
主动调度进程的函数是schedule() ，它会把主要工作委托给__schedule()去处理。
函数__shcedule的主要处理过程如下：
  调用pick_next_task()以选择下一个进程。
  调用context_switch()以切换进程。 调用context_switch：
  切换用户虚拟地址空间
  切换寄存器</description>
    </item>
    
    <item>
      <title>User Environments</title>
      <link>https://gao377020481.github.io/p/user-environments/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/user-environments/</guid>
      <description>JOS 这一部分做三件事：
 建立进程概念 异常处理，陷入内核 系统调用  进程和用户环境 JOS里进程是一个env结构，里面保存了简单的信息：页目录、寄存器缓存项（用于切换），连接指针（用于调度），状态等
首先提供基础设施：内核中保存进程的链表，这个数组需要初始化在物理内存上
显然创建一个进程，需要创建其对应的页目录，那就要在物理内存上分配对应的页目录和页表结构然后把他映射到虚拟内存里的页目录上。
要在进程上运行程序，就需要加载和解析ELF文件，同样分配物理页，载入elf文件各个segment，建立虚拟地址到物理地址的映射关系，然后修改寄存器的值为elf的entry。就可以运行起来elf上的程序了。
运行一个进程就只需要将进程结构内保存的寄存器值弹出到寄存器里。
trap与异常处理 一般来讲当异常发生，cpu会触发电信号，触发硬中断然后由中断控制器找到中断处理函数，但也有软中断的时候，通过指令提供的中断号结合IDT来查找到对应的中断处理函数。
中断发生，陷入内核，处理器根据TSS寄存器找到TSS结构，将栈寄存器SS和ESP分别设置为其中的SS0和ESP0两个字段的值，这样就切换到内核栈了
缺页异常与系统调用 缺页异常是一个中断，中断号是14，将这个号压入内核栈，然后call trap函数，在trap里dispatch到对应的处理函数就行。缺页的话就要分配物理页然后加载磁盘数据再建立映射关系，这一套操作目前由内核完成。
系统调用是一个中断，我们设置中断处理函数sys_call，并在trap内部根据传入的系统调用号做对应的dispatch到对应的系统调用处理。上面内核栈已经切换成功，其实cpu就在内核态了，接下来只需要压入触发系统调用对应的int指令需要的中断号和其它参数就可以call trap，然后dispatch到sys_call并传递参数（参数保存在一个trapframe结构中）。
Linux Kernel 进程 Linux内核把进程称为任务(task)，进程的虚拟地址空间分为用户虚拟地址空间和内核虚拟地址空间，所有进程共享内核虚拟地址空间，每个进程有独立的用户空间虚拟地址空间。
进程有两种特殊形式：没有用户虚拟地址空间的进程称为内核线程，共享用户虚拟地址空间的进程称为用户线程。通用在不会引起混淆的情况下把用户线程简称为线程。共享同一个用户虚拟地址空间的所有用户线程组成一个线程组。
四要素：
a.有一段程序供其执行。
b.有进程专用的系统堆栈空间。
c.在内核有task_struct数据结构。
d.有独立的存储空间，拥有专有的用户空间。
如果只具备前三条而缺少第四条，则称为“线程”。如果完全没有用户空间，就称为“内核线程”。而如果共享用户空间映射就称为“用户线程”。内核为每个进程分配一个task_struct结构时。实际分配两个连续物理页面(8192字节)，数据结构task_struct的大小约占1kb字节左右，进程的系统空间堆栈的大小约为7kb字节（不能扩展，是静态确定的）。</description>
    </item>
    
    <item>
      <title>Memory Management</title>
      <link>https://gao377020481.github.io/p/memory-management/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/memory-management/</guid>
      <description>内存管理
JOS JOS中主要是在做：
 线性地址到物理地址的映射（通过操作修改页表项实现） 在物理内存上划分出内核的页目录和页表存储区域并完成映射 保护模式的映射关系与段寄存器相关，32位的JOS通过段选择子找到段描述符，再解析段描述符的基地址，检查权限位后加上虚拟地址来实现 物理内存的分配在JOS内目前还无相应算法，只是单纯的连续分配  Linux kernel 最上层拿C++来说，其STL用于容器的alloc分配器的实现，又是基于malloc的，使用开链数组来存放多个线性增长大小的空闲内存块，当有分配需要时一次申请多块（应该是20块）该大小内存。且找不到对应块时还具有相应的回收，拼接，查找功能。对于大块内存的分配使用单独的路径来分配和存储。分层分级分区的思路是各级内存分配器的惯用思路。
然后低一层的malloc根据选择动态链接库的不同（就使用不同的分配器lib），有不同实现，以下以glibc为例。
至于内核中内存分配，由于其确定性，多用slab来做分配，也存在buddy allocation机制来管理。
虚拟地址一般都是大于物理地址的，所以只需要搞好虚拟地址上的内存管理就行了，至于物理页如果dirty，自然有page fault interrupt 来操心。当然以下提到物理页连续等，是默认了虚拟页到物理页的映射关系，实际上mmap,brk这样的系统调用都不会真的操作物理页，很多时候都是系统告诉我们：“你要的物理页已经准备好了”，实际上他也就是改了一下task_struct里mm_struct里的一些页表项罢了（准备好的是映射，真正的物理页可还是dirty的）。
所以内存分配器到底在哪一层呢，实际上内存分配器能看到的还就是虚拟内存，他一直在做修改页映射的操作，他并不会做将某个文件放到内存里的工作，这个工作显然是由缺页中断完成的紧跟其后的就是磁盘IO（以一个read操作为例&amp;ndash;，read -&amp;gt; vfs_read(首先要找到inode: 顺序是：check fd 在不在进程的打开文件列表里， 在的话找到file结构然后顺着找到dentry接着找到inode，check一下在不在page cache里这里就要通过inode里的address_space找到那颗组织page cache的基数树在上面查询了，查询用的是文件偏移量，不在就通过VFS里的inode做对应文件系统的operation，以xfs为例子，那就是解析inode数据结构，首先肯定要去它的data fork去找，xfs的data fork组织成两种形式，extent list or b+ tree, 无论是哪一种都能找到所有相关的block号了，一个一个把他们读出来就行了，这个工作显然是交给通用块设备层去做了，这层有点像VFS，下面马上就是各种不同的硬件驱动，上面是统一的请求，所以这一层用于整合，再下去就是到IO调度层，内核根据一些策略来做IO调度，这是因为磁盘读一次比较慢，尽量把连续扇区放一起读更快，再下去就真的从磁盘上读数据了。），只是它感性的认为自己在操作物理内存，比如：“太好了，这次我分配了0X10到0X2000这么多连续的内存空间”，实际上他就是把0X10到0X2000这一块虚拟内存的页表项改了一下，让虚拟页映射到了某一段连续物理页，实际的物理的IO并未发生呢
glibc (example for user-space) 以ptmalloc为例：
malloc时，如果内存分配区（主，非主）真的无空间，调用mmap或brk/sbrk来在虚拟内存上取一块作为新的空间，mmap取得内存在mmap区域，brk/sbrk在heap上（在进程的描述符task_struct中的mm_struct里可以找到）。主分配区可以使用sbrk和mmap向os申请内存，而非分配区只能通过mmap向os申请内存。接下来os可能（因为mmap只是给一块虚拟地址，真实数据还未拷贝到物理内存上）要去物理内存上找可用区块然后将真实数据拷贝上去（用户触发缺页syscall），这里再涉及物理页的分配。
ptmalloc的内存管理方法就可以很好的解决每次要内存都要去向OS要的问题。它回收管理分配出的内存而不是每次都还给操作系统而是放入bin中留待下次使用，bin的整理等一些其他操作会在特定时候触发。
首先每个进程都有一个主分配区和几个非主分配区，分配区在多线程间共享，对分配区的操作需要加线程互斥锁。主分配区由一号或主线程拥有。
最小的内存管理单位为chunk，一段连续的内存被分成多个chunk，chunk内记录当前和前一个chunk的大小，用于合并。
下一层级是分配区中的bins，bins分为:
 fast bin: 保存小块的chunk bins: 2.1 unsorted bin： 是一个chunk缓冲区，这里的chunk都是在等待整理的chunk（释放或合并出来的），同时也有可能是即将被用得到的chunk，因为很多程序会频繁的分配释放相同大小的内存，它在fastbin里找不到就会直接来这里找，速度快。chunk的size 没有限制。 2.2 small bin： 类似alloc分配器的开链数组实现，大小小于512字节的chunk被称为small chunk，分级每个相差8KB放入small bin对应槽位。共62个不同大小的bin 2.3 large bin： 与small bin类似，只是其中存的是大chunk，且不单纯线性增长，共63个不同大小的bin  chunk的分配与释放是一个很复杂的管理流程，这里只说管理层级，不谈细致流程。
kernel space buddy 连续的物理页称为页块（page block），阶（order）是页的数量单位，2的n次方个连续页称为n阶页块。 如下条件的两个n阶页块称为伙伴（buddy）：</description>
    </item>
    
    <item>
      <title>Boot</title>
      <link>https://gao377020481.github.io/p/boot/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://gao377020481.github.io/p/boot/</guid>
      <description>以MIT6.828的分节方式来记录操作系统的笔记。
所以第一章： Boot 启动
JOS 在JOS里，Boot作为Lab1存在。
实模式下运行BIOS，BIOS设置eip以启动bootloader，boot loader检查系统状态，开启保护模式，将磁盘上的kernel加载入物理内存，设置eip启动kernel。Kernel开启分页。
BIOS BIOS的启动依赖于硬件的电信号，在qemu虚拟机里模拟了这样一个信号来启动BIOS。
Bootloader Bootloader：
 从实模式进入保护模式，加载全局描述符表（虚拟地址到线性（物理地址）的映射开启） 从磁盘加载kernel到内存（通过读取ELF文件的方式）  Kernel 进入Kernel后：
 开启分页（就是在物理内存的特定位置创建内核页目录和页表数组，实现线性地址到物理地址的映射关系） 这里还基于内存映射的关系，实现了向IO HOLE区域（VGA显存）写值的功能，得以在终端上输出了字符  Linux kernel 在linux kernel中，这一环节的基本流程很相似，参考深入理解Linux内核附录1记录一个很简要的流程：
BIOS 硬件电信号拉起ROM上的BIOS程序，BIOS启动后简单检查和初始化一下硬件，随后在磁盘扇区上搜索操作系统来启动，找到磁盘第一个扇区（引导扇区）后将其拷贝到物理内存的0X00007C00处。
Bootloader 物理内存上有bootloader的第一部分了，第一部分可能会移动他的位置并将第二部分再装入物理内存的特定位置，第二部分会从磁盘中读取OS的映射表，提供给用户选项，选择启动哪一个操作系统，选中后bootloader就会调用BIOS过程来不停的装载内核映像，其中setup()函数在0X00090200处，下一步会跳转到这里
setup() 初始化硬件、和一些寄存器等，并跳转到startup_32()
startup_32() 初始化临时堆栈，段寄存器并解压内核映像放置到物理内存上，然后跳转到内核映像上启动
解压的内核映像启动点仍是一个叫做startup_32()的函数，它会再检查一下硬件软件信息然后的跳转到start_kernel()函数
start_kernel() 完成linux内核初始化工作，具体工作过多，这里不说</description>
    </item>
    
  </channel>
</rss>

[{"content":"Openmpi 初步使用 安装与测试 直接官网下载release包\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.1.tar.gz linux下解压:\ntar -zxf openmpi-4.1.1.tar.gz 进入开始configure： prefix 为指定安装路径\ncd openmpi-4.1.1/ ./configure --prefix=/usr/local/openmpi 安装：\nmake sudo make install 设置环境变量\nsudo vim /etc/profile 加入：\nexport PATH=/usr/local/openmpi/bin:$PATH export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH 生效：\nsource /etc/profile 测试\nmpicc --version 写代码测试：hello.c\n#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;mpi.h\u0026#34; int main(int argc, char* argv[]) { int rank, size, len; char version[MPI_MAX_LIBRARY_VERSION_STRING]; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); MPI_Get_library_version(version, \u0026amp;len); printf(\u0026#34;Hello, world, I am %d of %d, (%s, %d)\\n\u0026#34;, rank, size, version, len); MPI_Finalize(); return 0; } 编译并运行,我这里是四核虚拟机\nmpicc hello.c -o hello mpirun -np 4 hello 管理 使用cmake管理\n模板：\ncmake_minimum_required(VERSION 3.5.1)SET(SRC_LIST hello_c.c)find_package(MPI REQUIRED)include_directories(${MPI_INCLUDE_PATH})add_executable(hello_c ${SRC_LIST})target_link_libraries(hello_c ${MPI_LIBRARIES})if(MPI_COMPILE_FLAGS) set_target_properties(hello_c PROPERTIES COMPILE_FLAGS \u0026#34;${MPI_COMPILE_FLAGS}\u0026#34;)endif()if(MPI_LINK_FLAGS) set_target_properties(hello_c PROPERTIES LINK_FLAGS \u0026#34;${MPI_LINK_FLAGS}\u0026#34;)endif()编译与运行：\nmkdir build cd build cmake .. make mpirun -np 4 hello_c ","date":"2021-09-17T00:00:00Z","image":"https://gao377020481.github.io/p/cmake/405_huaea55e1e24566107c91b0c14c5461267_4592730_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/cmake/","title":"Openmpi"},{"content":"视图 视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。其内容由查询定义。 基表：用来创建视图的表叫做基表。 通过视图，可以展现基表的部分数据。 视图数据来自定义视图的查询中使用的表，使用视图动态生成。\n优点  简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。  CREATEVIEW\u0026lt;视图名\u0026gt;AS\u0026lt;SELECT语句\u0026gt;触发器 触发器（trigger）是MySQL提供给程序员和数据分析员来保证数据完整性的一种方法，它是与表事件相关的特殊的存储过程，它的执行不是由程序调用，也不是手工启动，而是由事件来触发，比如当对一个表进行DML操作（ insert ， delete ， update ）时就会激活它执行。\n监视对象： table\n监视事件： insert 、 update 、 delete\n触发时间： before ， after\n触发事件： insert 、 update 、 delete\nCREATETABLE`work`(`id`INTPRIMARYKEYauto_increment,`address`VARCHAR(32))DEFAULTcharset=utf8ENGINE=INNODB;CREATETABLE`time`(`id`INTPRIMARYKEYauto_increment,`time`DATETIME)DEFAULTcharset=utf8ENGINE=INNODB;CREATETRIGGERtrig_test1AFTERINSERTON`work`FOREACHROWINSERTINTO`time`VALUES(NULL,NOW());存储过程 SQL语句需要先编译然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。\n存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。\n优点\n 能完成较复杂的判断和运算 有限的编程 可编程行强，灵活 SQL编程的代码可重复使用 执行的速度相对快一些 减少网络之间的数据传输，节省开销  CREATEPROCEDURE过程名([[IN|OUT|INOUT]参数名数据类型[,[IN|OUT|INOUT]参数名数据类型…]])[特性...]过程体存储过程根据需要可能会有输入、输出、输入输出参数，如果有多个参数用\u0026quot;,\u0026ldquo;分割开。\nMySQL 存储过程的参数用在存储过程的定义，共有三种参数类型 IN , OUT , INOUT 。\nIN ：参数的值必须在调用存储过程时指定，0在存储过程中修改该参数的值不能被返回，可以设置默认值\nOUT ：该值可在存储过程内部被改变，并可返回\nINOUT ：调用时指定，并且可被改变和返回\n过程体的开始与结束使用 BEGIN 与 END 进行标识。\n游标 游标是针对行操作的，对从数据库中 select 查询得到的结果集的每一行可以进行分开的独立的相同或者不相同的操作。\n对于取出多行数据集，需要针对每行操作；可以使用游标；游标常用于存储过程、函数、触发器、事件。\n游标相当于迭代器\n直接来个例子就知道：\nCREATEPROCEDUREproc_while(INage_inINT,OUTtotal_outINT)BEGIN-- 创建 用于接收游标值的变量 DECLAREp_id,p_age,p_totalINT;DECLAREp_sexTINYINT;-- 注意:接收游标值为中文时,需要给变量 指定字符集utf8 DECLAREp_nameVARCHAR(32)CHARACTERSETutf8;-- 游标结束的标志 DECLAREdoneINTDEFAULT0;-- 声明游标 DECLAREcur_teacherCURSORFORSELECTteacher_id,teacher_name,teacher_sex,teacher_ageFROMteacherWHEREteacher_age\u0026gt;age_in;-- 指定游标循环结束时的返回值 DECLARECONTINUEHANDLERFORNOTfoundSETdone=1;-- 打开游标 OPENcur_teacher;-- 初始化 变量 SETp_total=0;-- while 循环 WHILEdone!=1DOFETCHcur_teacherINTOp_id,p_name,p_sex,p_age;IFdone!=1THENSETp_total=p_total+1;ENDIF;ENDWHILE;-- 关闭游标 CLOSEcur_teacher;-- 将累计的结果复制给输出参数 SETtotal_out=p_total;END//delimiter;-- 调用 SET@p_age=20;CALLproc_while(@p_age,@total);SELECT@total;索引 分类：主键索引、唯一索引、普通索引、组合索引、以及全文索引；\n主键索引 非空唯一索引，一个表只有一个主键索引；在 innodb 中，主键索引的B+树包含表数据信息。\nPRIMARYKEY(key)唯一索引 不可以出现相同的值，可以有NULL值。\nUNIQUE(key)普通索引 允许出现相同的索引内容。\nINDEX(key)-- OR KEY(key[,...])组合索引 对表上的多个列进行索引。 符合最左匹配原则：从左到右依次匹配，遇到 \u0026gt; \u0026lt; between like 就停止匹配；\nINDEXidx(key1,key2[,...]);UNIQUE(key1,key2[,...]);PRIMARYKEY(key1,key2[,...]);全文索引 将存储在数据库当中的整本书和整篇文章中的任意内容信息查找出来的技术；关键词 FULLTEXT; 在短字符串中用 LIKE % ；在全文索引中用 match 和 against。\n主键选择规则 innodb 中表是索引组织表，每张表有且仅有一个主键。\n 如果显示设置 PRIMARY KEY ，则该设置的key为该表的主键。 如果没有显示设置，则从非空唯一索引中选择。  只有一个非空唯一索引，则选择该索引为主键。 有多个非空唯一索引，则选择声明的第一个为主键。   没有非空唯一索引，则自动生成一个 6 字节的 _rowid 作为主键。  索引实现 索引存储 innodb由段、区、页组成；段分为数据段、索引段、回滚段等。区大小为 1 MB（一个区由64个连续页构成）。页的默认值为16k。页为逻辑页，磁盘物理页大小一般为 4K 或者 8K。为了保证区中的页的连续，存储引擎一般一次从磁盘中申请 4~5 个区。\n聚集索引与辅助索引 聚集索引就是以主键为key构造B+树\n辅助索引就是以辅助索引为key构造B+树，value为对应的主键\n页和B+树 B+树单个节点是页，页中由id再向下分裂指向多个页。\n我们知道主键索引的id就是B+树节点的key，为了范围查询我们不使用hashmap，那为什么不用B树呢？\n这是因为B树在叶子节点上保存数据，执行范围查询时，可能需要反复载入不连续的页，cache和内存的利用率都不是很高。而B+树直接进入叶子节点，然后范围查询只需要根据叶子节点之间的指针载入连续的页就行，这种确定的内存访问优化空间更大，效率更高。\n索引失效  select \u0026hellip; where A and B 若 A 和 B 中有一个不包含索引，则索引失效。 索引字段参与运算，则索引失效；例如： from_unixtime(idx) = \u0026lsquo;2021-04-30\u0026rsquo;。 索引字段发生隐式转换，则索引失效；例如： \u0026lsquo;1\u0026rsquo; 隐式转换为 1 。 LIKE 模糊查询，通配符 % 开头，则索引失效；例如： select * from user where name like \u0026lsquo;%ark\u0026rsquo;。 在索引字段上使用 NOT \u0026lt;\u0026gt; != 索引失效；如果判断 id \u0026lt;\u0026gt; 0 则修改为 idx \u0026gt; 0 or idx \u0026lt; 0 。 组合索引中，没使用第一列索引，索引失效。  索引原则  查询频次较⾼且数据量⼤的表建⽴索引；索引选择使⽤频次较⾼，过滤效果好的列或者组合。 使⽤短索引；节点包含的信息多，较少磁盘io操作。 对于很长的动态字符串，考虑使用前缀索引。 对于组合索引，考虑最左侧匹配原则和覆盖索引。 尽量选择区分度⾼的列作为索引；该列的值相同的越少越好。 尽量扩展索引，在现有索引的基础上，添加复合索引。 不要 select * ； 尽量只列出需要的列字段。 索引列，列尽量设置为非空。  约束 为了实现数据的完整性，对于innodb，提供了以下几种约束，primary key，unique key，foreign key， default, not null。\n外键约束 外键用来关联两个表，来保证参照完整性；MyISAM存储引擎本身并不支持外键，只起到注释作用。而innodb完整支持外键。\n约束于索引的区别\n创建主键索引或者唯一索引的时候同时创建了相应的约束；但是约束是逻辑上的概念；索引是一个数据结构既包含逻辑的概念也包含物理的存储方式；\n事务 事务将数据库从一种一致性状态转换为另一种一致性状态。\n在数据库提交事务时，可以确保要么所有修改都已经保存，要么所有修改都不保存。\n事务是访问并更新数据库各种数据项的一个程序执行单元。\n在 MySQL innodb 下，每一条语句都是事务。可以通过 set autocommit = 0， 设置当前会话手动提交。\n-- 显示开启事务 STARTTRANSACTION|BEGIN-- 提交事务，并使得已对数据库做的所有修改持久化 COMMIT-- 回滚事务，结束用户的事务，并撤销正在进行的所有未提交的修改 ROLLBACK-- 创建一个保存点，一个事务可以有多个保存点 SAVEPOINTidentifier-- 删除一个保存点 RELEASESAVEPOINTidentifier-- 事务回滚到保存点 ROLLBACKTO[SAVEPOINT]identifierACID特性 原子性（A） 事务操作要么都做（提交），要么都不做（回滚）。事务是访问并更新数据库各种数据项的一个程序执行单元，是不可分割的工作单位。通过undolog来实现回滚操作。undolog记录的是事务每步具体操作，当回滚时，回放事务具体操作的逆运算。\n隔离性（I） 事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离，也就是事务提交前对其他事务都不可见。通过 MVCC 和 锁来实现。MVCC 时多版本并发控制，主要解决一致性非锁定读，通过记录和获取行版本，而不是使用锁来限制读操作，从而实现高效并发读性能。锁用来处理并发 DML 操作。数据库中提供粒度锁的策略，针对表（聚集索引B+树）、页（聚集索引B+树叶子节点）、行（叶子节点当中某一段记录行）三种粒度加锁。\n持久性（D） 事务提交后，事务DML操作将会持久化（写入redolog磁盘文件 哪一个页 页偏移值 具体数据）。即使发生宕机等故障，数据库也能将数据恢复。redolog记录的是物理日志。\n一致性（C） 一致性指事务将数据库从一种一致性状态转变为下一种一致性的状态，在事务执行前后，数据库完整性约束没有被破坏。例如：一个表的姓名是唯一键，如果一个事务对姓名进行修改，但是在事务提交或事务回滚后，表中的姓名变得不唯一了，这样就破坏了一致性。一致性由原子性、隔离性以及持久性共同来维护的。\n事务并发异常 脏读 事务（A）可以读到另外一个事务（B）中未提交的数据，也就是事务A读到脏数据。在读写分离的场景下，可以将slave节点设置为 READ UNCOMMITTED。此时脏读不影响，在slave上查询并不需要特别精准的返回值。\n不可重复读 事务（A) 可以读到另外一个事务（B）中提交的数据。通常发生在一个事务中两次读到的数据是不一样的情况。不可重复读在隔离级别 READ COMMITTED 存在。一般而言，不可重复读的问题是可以接受的，因为读到已经提交的数据，一般不会带来很大的问题，所以很多厂商（如Oracle、SQL Server）默认隔离级别就是READ COMMITTED。\n幻读 事务中一次读操作不能支撑接下来的业务逻辑。通常发生在一个事务中一次读判断接下来写操作失败的情况。例如：以name为唯一键的表，一个事务中查询 select * from t where name =\u0026lsquo;mark\u0026rsquo;。不存在，接下来 insert into t(name) values (\u0026lsquo;mark\u0026rsquo;)。 出现错误，此时另外一个事务也执行了 insert 操作。幻读在隔离级别 REPEATABLE READ 及以下存在。但是可以在REPEATABLE READ 级别下通过读加锁（使用next-key locking）解决。\n隔离级别 ISO和ANIS SQL标准制定了四种事务隔离级别的标准，各数据库厂商在正确性和性能之间做了妥协，并没有严格遵循这些标准。MySQL innodb默认支持的隔离级别是 REPEATABLE READ。\nREAD UNCOMMITTED 读未提交：该级别下读不加锁，写加排他锁，写锁在事务提交或回滚后释放锁。\nREAD COMMITTED 读已提交：从该级别后支持 MVCC (多版本并发控制)，也就是提供一致性非锁定读。此时读取操作读取历史快照数据。该隔离级别下选择读取快照中历史版本的最新数据，所以读取的是已提交的数据。\nREPEATABLE READ 可重复读：该级别下也支持 MVCC，此时读取操作读取事务开始时的版本数据。\nSERIALIZABLE 可串行化：该级别下给读加了共享锁。所以事务都是串行化的执行，此时隔离级别最严苛。\n-- 设置隔离级别 SET[GLOBAL|SESSION]TRANSACTIONISOLATIONLEVELREPEATABLEREAD;-- 或者采用下面的方式设置隔离级别 SET@@tx_isolation=\u0026#39;REPEATABLE READ\u0026#39;;SET@@global.tx_isolation=\u0026#39;REPEATABLE READ\u0026#39;;-- 查看全局隔离级别 SELECT@@global.tx_isolation;-- 查看当前会话隔离级别 SELECT@@session.tx_isolation;SELECT@@tx_isolation;-- 手动给读加 S 锁 SELECT...LOCKINSHAREMODE;-- 手动给读加 X 锁 SELECT...FORUPDATE;-- 查看当前锁信息 SELECT*FROMinformation_schema.innodb_locks;锁 锁机制用于管理对共享资源的并发访问，用来实现事务的隔离级别。\n粒度 共享锁和排他锁都是行级锁。MySQL当中事务采用的是粒度锁。针对表（B+树）、页（B+树叶子节点）、行（B+树叶子节点当中某一段记录行）三种粒度加锁。\n意向共享锁和意向排他锁都是表级别的锁。\n共享锁（S） 事务读操作加的锁，对某一行加锁。\n在 SERIALIZABLE 隔离级别下，默认帮读操作加共享锁。\n在 REPEATABLE READ 隔离级别下，需手动加共享锁，可解决幻读问题。\n在 READ COMMITTED 隔离级别下，没必要加共享锁，采用的是 MVCC。\n在 READ UNCOMMITTED 隔离级别下，既没有加锁也没有使用 MVCC。\n排他锁（X） 事务删除或更新加的锁；对某一行加锁。\n在4种隔离级别下，都添加了排他锁，事务提交或事务回滚后释放锁。\n意向共享锁（IS） 对一张表中某几行加的共享锁。\n意向排他锁（IX） 对一张表中某几行加的排他锁。\n兼容性 由于innodb支持的是行级别的锁，意向锁并不会阻塞除了全表扫描以外的任何请求。\n意向锁之间是互相兼容的。\nIX 对共享锁和排他锁都不兼容。\nIS 只对排他锁不兼容。\n当想为某一行添加 S 锁，先自动为所在的页和表添加意向锁 IS，再为该行添加 S 锁。\n当想为某一行添加 X 锁，先自动为所在的页和表添加意向锁 IX，再为该行添加 X 锁。\n锁算法  Record Lock: 记录锁，单个行记录上的锁。 Gap Lock： 间隙锁，锁定一个范围，但不包含记录本身。全开区间。REPEATABLE READ级别及以上支持间隙锁。 Next-Key Lock： 记录锁+间隙锁，锁定一个范围，并且锁住记录本身。左开右闭区间。 Insert Intention Lock： 插入意向锁，insert操作的时候产生。在多事务同时写入不同数据至同一索引间隙的时候，并不需要等待其他事务完成，不会发生锁等待。 AUTO-INC Lock： 自增锁，是一种特殊的表级锁，发生在 AUTO_INCREMENT 约束下的插入操作。采用的一种特殊的表锁机制。完成对自增长值插入的SQL语句后立即释放。在大数据量的插入会影响插入性能，因为另一个事务中的插入会被阻塞。从MySQL 5.1.22开始提供一种轻量级互斥量的自增长实现机制，该机制提高了自增长值插入的性能。  MVCC 多版本并发控制，用来实现一致性的非锁定读。非锁定读是指不需要等待访问的行上X锁的释放。\n在 read committed 和 repeatable read下，innodb使用MVCC。\n对于快照数据的定义不同；\n 在 read committed 隔离级别下，对于快照数据总是读取被锁定行的最新一份快照数据。 repeatable read 隔离级别下，对于快照数据总是读取事务开始时的行数据版本。  redolog redo log用来实现事务的持久性。内存中包含 redo log buffer，磁盘中包含 redo log file。\n当事务提交时，必须先将该事务的所有日志写入到redolog进行持久化，待事务的commit操作完成才完成了事务的提交。\nredo log 顺序写，记录的是对每个页的修改（页、页偏移量、以及修改的内容）。在数据库运行时不需要对 redo log 的文件进行读取操作，只有发生宕机的时候，才会拿redo log进行恢复。\nundolog undo log用来帮助事务回滚以及MVCC的功能。\n存储在共享表空间中， undo 是逻辑日志，回滚时将数据库逻辑地恢复到原来的样子，根据 undo log 的记录，做之前的逆运算。\n比如事务中有insert 操作，那么执行 delete 操作， 对于 update 操作执行相反的 update 操作。\n同时 undo 日志记录行的版本信息，用于处理 MVCC 功能。\n主从复制  主库更新事件(update、insert、delete)通过io-thread写到binlog。 从库请求读取binlog，通过io-thread写⼊（write）从库本地 relay log（中继⽇志）。 从库通过sql-thread读取（read） relay log，并把更新事件在从库中执⾏（replay）⼀遍。  读写分离 ","date":"2020-06-06T00:00:00Z","image":"https://gao377020481.github.io/p/mysql/537_huc4722358eae36c77295feb62b7419b0c_34031398_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mysql/","title":"Mysql"},{"content":"Redis\nRemote Dictionary Service， 远程字典服务，内存式数据库，非关系型，KV结构\n三张网图描述redis基本数据结构： \r \r \r\n数据与编码 String 字符数组，该字符串是动态字符串，字符串长度小于1M时，加倍扩容；超过1M每次只多扩1M；字符串最大长度为512M；\nTips：redis字符串是二进制安全字符串；可以存储图片，二进制协议等二进制数据；\n 字符串长度小于等于 20 且能转成整数，则使用 int 存储； 字符串长度小于等于 44，则使用 embstr 存储； 字符串长度大于 44，则使用 raw 存储；  44为界\n首先说明redis以64字节作为大小结构分界点，但其sdshdr和redisobject结构会占用一些空间，所以真正保存数据的大小小于64字节\n旧版本使用39为界，新版本使用44为界，这是因为旧版本中sdshdr占用8字节目前的sdshdr8是针对小结构的优化（大结构使用shshdr16，64），仅占用3字节，节省了5字节空间。所以新版本以44为界。\nraw和embstr\nraw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间存储两结构\nembstr使用连续内存，更高效的利用缓存，且一次内存操作带来了更好的创建和销毁效率\n操作方式与转换\n  以一个浮点数的value作为例子，浮点数会被转换成字符串然后存储到数据库内。如果要对V进行操作，他也会先从字符串转换为浮点数然后再进行操作。\n  以一个整数2000的value作为例子，该数会被保存为int但使用append进行追加一个字符串“is a good number！”后， 该值会被转换为embstr， 然而embstr的对象从redis视角看来是只读的（未实现操作embstr的方法）， 所以该对象又会被转换raw然后实行相应操作并保存为raw\n  List 双向链表，很容易理解，但其node有讲究（压缩时使用ziplist）\n列表中数据是否压缩的依据：\n 元素长度小于 48，不压缩； 元素压缩前后长度差不超过 8，不压缩；  直接放数据结构，然后分析:\n/* Minimum ziplist size in bytes for attempting compression. */ #define MIN_COMPRESS_BYTES 48 /* quicklistNode is a 32 byte struct describing a ziplist for a quicklist. * We use bit fields keep the quicklistNode at 32 bytes. * count: 16 bits, max 65536 (max zl bytes is 65k, so max count actually \u0026lt; 32k). * encoding: 2 bits, RAW=1, LZF=2. * container: 2 bits, NONE=1, ZIPLIST=2. * recompress: 1 bit, bool, true if node is temporary decompressed for usage. * attempted_compress: 1 bit, boolean, used for verifying during testing. * extra: 10 bits, free for future use; pads out the remainder of 32 bits */ typedef struct quicklistNode { struct quicklistNode *prev; struct quicklistNode *next; unsigned char *zl; unsigned int sz; /* ziplist size in bytes */ unsigned int count : 16; /* count of items in ziplist */ unsigned int encoding : 2; /* RAW==1 or LZF==2 */ unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ unsigned int recompress : 1; /* was this node previous compressed? */ unsigned int attempted_compress : 1; /* node can\u0026#39;t compress; too small*/ unsigned int extra : 10; /* more bits to steal for future usage */ } quicklistNode; typedef struct quicklist { quicklistNode *head; quicklistNode *tail; unsigned long count; /* total count of all entries in all ziplists */ unsigned long len; /* number of quicklistNodes */ int fill : QL_FILL_BITS; /* fill factor for individual nodes */ unsigned int compress : QL_COMP_BITS; /* depth of end nodes not to compress;0=off */ unsigned int bookmark_count: QL_BM_BITS; quicklistBookmark bookmarks[]; } quicklist; OK, 可以很明显看到list本身是一个双向链表，但他会记录所有的entry的数目，node本身可能用来描述一个ziplist，ziplist本身是一块连续的内存空间，ziplist内的node不保存前后指针，因为其连续内存的特性，只需要保存当前size和前一项size即可完成向前和向后寻址，提供和双项链表一致的操作。\nhash 哈希表，很容易理解\n但底层使用ziplist和dict两种结构进行存储。\n 节点数量大于 512（hash-max-ziplist-entries） 或所有字符串长度大于 64（hash-max-ziplistvalue），则使用 dict 实现； 节点数量小于等于 512 且有一个字符串长度小于 64，则使用 ziplist 实现；  ziplist存储时将field与value（就是KV）连着放在一起，提供更高的存储效率。在未序列化的情况下，该方式相比于string更节省内存。不过如果把对象的各个KV序列化为一体然后存储为string，也许占用空间还更小。\ndict就是标准的哈希表实现，不过一个dict内部保存两个哈希表ht0和ht1，ht1用来进行rehash的中转。该表使用开链法解决hash冲突。一张引用自redisbook的图很清晰的说明了结构： \r\n当rehash工作量太大时，需要使用渐进式rehash，此时不会发生\nset 集合，不要求有序\n 元素都为整数且节点数量小于等于 512（set-max-intset-entries），则使用整数数组存储； 元素当中有一个不是整数或者节点数量大于 512，则使用字典存储；  类比hash内部，这里不再详细分析，字典编码Value为NULL即可\nzset 有序的集合\n 节点数量大于 128或者有一个字符串长度大于64，则使用跳表（skiplist）； 节点数量小于等于128（zset-max-ziplist-entries）且所有字符串长度小于等于64（zset-maxziplist-value），则使用 ziplist 存储；  ziplist编码\n|\u0026lt;-- element 1 --\u0026gt;|\u0026lt;-- element 2 --\u0026gt;|\u0026lt;-- ....... --\u0026gt;| +---------+---------+--------+---------+--------+---------+---------+---------+ | ZIPLIST | | | | | | | ZIPLIST | | ENTRY | member1 | score1 | member2 | score2 | ... | ... | ENTRY | | HEAD | | | | | | | END | +---------+---------+--------+---------+--------+---------+---------+---------+ score1 \u0026lt;= score2 \u0026lt;= ... ziplist 内连续的有序的保存entry\nskiplist编码\n/* * 有序集 */ typedef struct zset { // 字典  dict *dict; // 跳跃表  zskiplist *zsl; } zset; 使用dict来检索，使用skiplist维持顺序 引用redisbook的图，说的很清晰： \r\n协议与网络 网络层 redis6.0 单reactor模型，并发处理连接，线程串行处理命令。\nreactor管理触发的socket，调用其对应的callback（accept,recv or send），这里callback使用单线程串行处理\n\r\n事务 MULTI 开启事务，事务执行过程中，单个命令是入队列操作，直到调用 EXEC 才会一起执行；\n也可以使用DISCARD取消事务\nWATCH 检测key的变动，若在事务执行中，key变动则取消事务，在事务开启前调用，乐观锁实现（cas）， 若被取消则事务返回 nil。\nlua 脚本  lua 脚本实现原子性。 redis中加载了一个lua虚拟机，用来执行redis lua脚本。redis lua 脚本的执行是原子性的，当某个脚本正在执行的时候，不会有其他命令或者脚本被执行。 lua脚本当中的命令会直接修改数据状态。  Tips：如果项目中使用了lua脚本，不需要使用上面的事务命令。\n发布订阅 为了支持消息的多播机制，redis引入了发布订阅模块。disque 消息队列\n订阅\nstruct redisServer { // ...  dict *pubsub_channels; // ... }; redisServer 中一项pubsub_channels的字典保存了订阅频道的信息。 字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。\n发布\n首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端。\n订阅模式\n一个发布的信息，与其发布频道相匹配的所有模式频道也会将这个信息发布给自己旗下的订阅用户。\nstruct redisServer { // ...  list *pubsub_patterns; // ... }; 这次是pubsub_patterns链表了，每个节点都有一个pubsubPattern结构\ntypedef struct pubsubPattern { redisClient *client; robj *pattern; } pubsubPattern; client 属性保存着订阅模式的客户端，而 pattern 属性则保存着被订阅的模式。\n通过遍历整个 pubsub_patterns 链表，程序可以检查所有正在被订阅的模式，以及订阅这些模式的客户端。\n发布到模式\nPUBLISH 除了将 message 发送到所有订阅 channel 的客户端之外， 它还会将 channel 和 pubsub_patterns 中的模式进行对比， 如果 channel 和某个模式匹配的话， 那么也将 message 发送到订阅那个模式的客户端。\nredis6.0 io多线程 redis6.0版本后添加的 io多线程主要解决redis协议的压缩以及解压缩的耗时问题；一般项目中不 需要开启；如果有大量并发请求，且返回数据包一般比较大的场景才有它的用武之地；\n单线程\n正常的redis单线程处理事件，但是其read write操作都需要经过内核栈，需要从内核的内存空间中取出或发送包，内核到用户内存空间的复制又涉及页表项的修改等。所以总的来说IO占用的CPU时间较多。如果这时引入多线程IO，有效利用多核可以进一步提升性能。\n多线程实现\n外部来看还是单线程的状态，这是因为多线程参考主线程的phase。只有主线程处于read phase，其他线程才做read操作，只有主线程处于write phase其他线程才做write。并且任务的分配也由主线程完成，每个thread维护一个任务队列（链表组织），主线程唤醒thread去执行任务，可以使用cond去唤醒。\nredis扩展 Redis 通过对外提供一套 API 和一些数据类型，可以供开发者开发自己的模块并且加载到 redis 中。类似插件。在不侵入 redis 源码基础上，提供一种高效的扩展数据结构的方式，也可以用来实现原子操作。\n入口函数 int RedisModule_OnLoad(RedisModuleCtx *ctx, RedisModuleString **argv, int argc); // RedisModule_Init 应该要是第一个被调用的函数 static int RedisModule_Init(RedisModuleCtx *ctx, const char *name, int ver, int apiver); // RedisModule_CreateCommand 应该要是第二个被调用的函数 REDISMODULE_API int (*RedisModule_CreateCommand)(RedisModuleCtx *ctx, const char *name, RedisModuleCmdFunc cmdfunc, const char *strflags, int firstkey, int lastkey, int keystep); // 回调函数 typedef int (*RedisModuleCmdFunc)(RedisModuleCtx *ctx, RedisModuleString **argv, int argc); RedisBloom 布隆过滤器，准确判断不存在的值\n\rgit clone https://github.com/RedisBloom/RedisBloom.git\rcd RedisBloom\rmake\rcp redisbloom.so /path/to\rvi redis.conf\r# loadmodules /path/to/redisbloom.so\r具体使用直接看github\nhyperloglog 少量内存下统计一个集合中唯一元素数量的近似值。\n在Redis实现中，每个键只使用 12kb 进行计数，使用 16384 个桶子，每个桶子6bit，标准误差为0.8125% ，并且对可以计数的项目数没有限制，除非接近 个项目数（这似乎不太可能）\n持久化 redis 的数据全部在内存中，如果突然宕机，数据就会全部丢失，因此需要持久化来保证 Redis 的数据不会因为故障而丢失，redis 重启的时候可以重新加载持久化文件来恢复数据；\n三种方式：\n  aof (append only file)\n  rdb (Redis Database)\n  rdb + aof\n  AOF 可以认为是一系列的操作log，以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF 文件，以此达到记录数据库状态的目的。\n同步命令到 AOF 文件的整个过程可以分为三个阶段：\n 命令传播：Redis 将执行完的命令、命令的参数、命令的参数个数等信息发送到 AOF 程序中。 缓存追加：AOF 程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的 AOF 缓存中。 文件写入和保存：AOF 缓存中的内容被写入到 AOF 文件末尾，如果设定的 AOF 保存条件被满足的话， fsync 函数或者 fdatasync 函数会被调用，将写入的内容真正地保存到磁盘中。  保存模式\nRedis 目前支持三种 AOF 保存模式，它们分别是：\n AOF_FSYNC_NO ：不保存。 AOF_FSYNC_EVERYSEC ：每一秒钟保存一次。 AOF_FSYNC_ALWAYS ：每执行一个命令保存一次。  AOF 重写\nAOF文件会积累变大，所以积累到一定量时，会创建一个新的 AOF 文件来代替原有的 AOF 文件， 新 AOF 文件和原有 AOF 文件保存的数据库状态完全一样， 但新 AOF 文件的体积小于等于原有 AOF 文件的体积。相当于对冗杂的指令进行简化，使最终结果一样就行。\n重写在后台子进程中发生，子进程带有主进程的数据副本，使用子进程而不是线程，可以在避免锁的情况下，保证数据的安全性。\n不过， 使用子进程也有一个问题需要解决： 因为子进程在进行 AOF 重写期间， 主进程还需要继续处理命令， 而新的命令可能对现有的数据进行修改， 这会让当前数据库的数据和重写后的 AOF 文件中的数据不一致。\n为了解决这个问题， Redis 增加了一个 AOF 重写缓存， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个缓存中。\n重写完毕后会将缓存并入新的AOF文件中去。\nRDB 可以认为是当前存储的内存映像or快照，将数据库的快照（snapshot）以二进制的方式保存到磁盘中。\n核心方法：保存与载入\n保存\nrdbSave 函数负责将内存中的数据库数据以 RDB 格式保存到磁盘中， 如果 RDB 文件已存在， 那么新的 RDB 文件将替换已有的 RDB 文件。\n在保存 RDB 文件期间， 主进程会被阻塞， 直到保存完成为止。 有两种方法：SAVE 和 BGSAVE SAVE： 会阻塞主进程开始保存RDB，直到保存完毕返回 BGSAVE： 会fork出子进程进行保存RDB，但直到保存完毕中间的一段数据有可能会丢失 SAVE与BGSAVE不能同时执行，且BGSAVE也不能与REWRITEAOF同时执行，这是因为创建俩进程来做这个事情效率太低 SAVE的时候AOF一样会写入，因为AOF写入又后台线程完成\n载入\n在载入期间， 服务器每载入 1000 个键就处理一次所有已到达的请求， 不过只有 PUBLISH 、 SUBSCRIBE 、 PSUBSCRIBE 、 UNSUBSCRIBE 、 PUNSUBSCRIBE 五个命令的请求会被正确地处理， 其他命令一律返回错误。 等到载入完成之后， 服务器才会开始正常处理所有命令。\nTips:发布与订阅功能和其他数据库功能是完全隔离的，前者不写入也不读取数据库，所以在服务器载入期间，订阅与发布功能仍然可以正常使用，而不必担心对载入数据的完整性产生影响。\nRDB + AOF 因为RDB的保存过程较长可能会错过一些数据，所以在RDB进行保存的时候使用AOF缓存来记录保存时发生的写入操作，然后将AOF缓存持久化为AOF，通过AOF+RDB就可以完整的表述数据\n主从复制 主要用来实现 redis 数据的可靠性。防止主 redis 所在磁盘损坏，造成数据永久丢失。\n主从之间采用异步复制的方式。\n集群 Redis cluster集群\n","date":"2020-05-22T00:00:00Z","image":"https://gao377020481.github.io/p/redis/353_hue47bacef190d5647cba0a16e01e338ad_4596808_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/redis/","title":"Redis"},{"content":"Try Catch组件实现 先说明try catch组件使用setjump和longjump实现\nsetjump longjump语法 jmp_buf env;环境\nsetjump(env)设置回跳点，返回longjump(env,out)传的参数out，配套使用，longjump可穿越函数跳转\njmp_buf env; int c = setjump(env); longjump(env,3); 这里longjump后就会跳回setjump这一行，并且setjump会返回3，也就是c = 3。\nint count = 0; jmp_buf env; void a(int indx) { longjump(env,indx); } int main() { int idx = 0; count = setjump(env); if(count == 0) { a(env,idx++); } else if (count == 1) { a(env,idx++); } else { printf(\u0026#34;ok\u0026#34;); } return 0; } 如上，函数a会调回开头setjump处，如果是这样a调用多次，a又没有返回（a运行到longjump处进入了，没返回），a的栈会不会还存在，存在的话如果有无数个a，会不会发生栈溢出。\n答案是不会，因为a在进入longjump后，其栈指针直接失效，a的栈直接失效，在setjump函数所在函数block中被覆盖，所以a的多次调用不会发生栈溢出。\nsetjump 与 longjump本身是线程安全的\nsetjump longjump与try catch的关系 先来个 代码：\ntry{ //setjump to catch  throw(); // longjump(para)  } catch (para) { } finally () { } 来看对应的：\nint count = 0; jmp_buf env; void a(int indx) { longjump(env,indx); } int main() { int idx = 0; count = setjump(env); if(count == 0) // try()  { a(env,idx++); //throw(1)  } else if (count == 1)//catch(1)  { a(env,idx++);//throw(2)  } //finally  { printf(\u0026#34;ok\u0026#34;); } return 0; } 再上代码，直接用宏定义try catch throw finally：\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;setjmp.h\u0026gt; typedef struct _tagExcepSign { jmp_buf _stackinfo; int _exceptype; } tagExcepSign; #define ExcepType(ExcepSign) ((ExcepSign)._exceptype)  #define Try(ExcepSign) if (((ExcepSign)._exceptype = setjmp((ExcepSign)._stackinfo)) == 0)  #define Catch(ExcepSign, ExcepType) else if ((ExcepSign)._exceptype == ExcepType)  #define Finally\telse  #define Throw(ExcepSign, ExcepType)\tlongjmp((ExcepSign)._stackinfo, ExcepType)  void ExceptionTest(int expType) { tagExcepSign ex; expType = expType \u0026lt; 0 ? -expType : expType; Try (ex) { if (expType \u0026gt; 0) { Throw(ex, expType); } else { printf(\u0026#34;no exception\\n\u0026#34;); } } Catch (ex, 1) { printf(\u0026#34;no exception 1\\n\u0026#34;); } Catch (ex, 2) { printf(\u0026#34;no exception 2\\n\u0026#34;); } Finally { printf(\u0026#34;other exp\\n\u0026#34;); } } int main() { ExceptionTest(0); ExceptionTest(1); ExceptionTest(2); ExceptionTest(3); } 现在有个新问题，try catch嵌套怎么办。\n以下三个问题需要实现：\n 多个ex依次入栈，每次只处理栈顶ex，解决嵌套问题 需要在throw的时候抛出异常发生的位置（文件、行号、函数名），可使用编译器自带的宏： __FILE__ __LINE__ __func__  虽然函数setjump和longjump是线程安全的，但ex变量在多线程时被共用，那ex就成为临界变量，如何保证ex的线程安全  以下代码实现以上功能：\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdarg.h\u0026gt; #include \u0026lt;pthread.h\u0026gt;#include \u0026lt;setjmp.h\u0026gt; #define ntyThreadData\tpthread_key_t #define ntyThreadDataSet(key, value)\tpthread_setspecific((key), (value)) #define ntyThreadDataGet(key)\tpthread_getspecific((key)) #define ntyThreadDataCreate(key)\tpthread_key_create(\u0026amp;(key), NULL)  #define EXCEPTIN_MESSAGE_LENGTH\t512  typedef struct _ntyException { const char *name; } ntyException; ntyException SQLException = {\u0026#34;SQLException\u0026#34;}; ntyException TimeoutException = {\u0026#34;TimeoutException\u0026#34;}; ntyThreadData ExceptionStack; typedef struct _ntyExceptionFrame { jmp_buf env; int line; const char *func; const char *file; ntyException *exception; struct _ntyExceptionFrame *prev; char message[EXCEPTIN_MESSAGE_LENGTH+1]; } ntyExceptionFrame; #define ntyExceptionPopStack\t\\ ntyThreadDataSet(ExceptionStack, ((ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack))-\u0026gt;prev)  #define ReThrow\tntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, NULL) #define Throw(e, cause, ...) ntyExceptionThrow(\u0026amp;(e), __func__, __FILE__, __LINE__, cause, ##__VA_ARGS__, NULL)  enum { ExceptionEntered = 0, ExceptionThrown, ExceptionHandled, ExceptionFinalized }; #define Try do {\t\\ volatile int Exception_flag;\t\\ ntyExceptionFrame frame;\t\\ frame.message[0] = 0;\t\\ frame.prev = (ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack);\t\\ ntyThreadDataSet(ExceptionStack, \u0026amp;frame);\t\\ Exception_flag = setjmp(frame.env);\t\\ if (Exception_flag == ExceptionEntered) {\t\t#define Catch(e) \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } else if (frame.exception == \u0026amp;(e)) { \\ Exception_flag = ExceptionHandled;  #define Finally \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } { \\ if (Exception_flag == ExceptionEntered)\t\\ Exception_flag = ExceptionFinalized;  #define EndTry \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } if (Exception_flag == ExceptionThrown) ReThrow; \\ } while (0)\t static pthread_once_t once_control = PTHREAD_ONCE_INIT; static void init_once(void) { ntyThreadDataCreate(ExceptionStack); } void ntyExceptionInit(void) { pthread_once(\u0026amp;once_control, init_once); } void ntyExceptionThrow(ntyException *excep, const char *func, const char *file, int line, const char *cause, ...) { va_list ap; ntyExceptionFrame *frame = (ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack); if (frame) { frame-\u0026gt;exception = excep; frame-\u0026gt;func = func; frame-\u0026gt;file = file; frame-\u0026gt;line = line; if (cause) { va_start(ap, cause); vsnprintf(frame-\u0026gt;message, EXCEPTIN_MESSAGE_LENGTH, cause, ap); va_end(ap); } ntyExceptionPopStack; longjmp(frame-\u0026gt;env, ExceptionThrown); } else if (cause) { char message[EXCEPTIN_MESSAGE_LENGTH+1]; va_start(ap, cause); vsnprintf(message, EXCEPTIN_MESSAGE_LENGTH, cause, ap); va_end(ap); printf(\u0026#34;%s: %s\\nraised in %s at %s:%d\\n\u0026#34;, excep-\u0026gt;name, message, func ? func : \u0026#34;?\u0026#34;, file ? file : \u0026#34;?\u0026#34;, line); } else { printf(\u0026#34;%s: %p\\nraised in %s at %s:%d\\n\u0026#34;, excep-\u0026gt;name, excep, func ? func : \u0026#34;?\u0026#34;, file ? file : \u0026#34;?\u0026#34;, line); } } /* ** **** ******** **************** debug **************** ******** **** ** */ ntyException A = {\u0026#34;AException\u0026#34;}; ntyException B = {\u0026#34;BException\u0026#34;}; ntyException C = {\u0026#34;CException\u0026#34;}; ntyException D = {\u0026#34;DException\u0026#34;}; void *thread(void *args) { pthread_t selfid = pthread_self(); Try { Throw(A, \u0026#34;A\u0026#34;); } Catch (A) { printf(\u0026#34;catch A : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(B, \u0026#34;B\u0026#34;); } Catch (B) { printf(\u0026#34;catch B : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(C, \u0026#34;C\u0026#34;); } Catch (C) { printf(\u0026#34;catch C : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(D, \u0026#34;D\u0026#34;); } Catch (D) { printf(\u0026#34;catch D : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(A, \u0026#34;A Again\u0026#34;); Throw(B, \u0026#34;B Again\u0026#34;); Throw(C, \u0026#34;C Again\u0026#34;); Throw(D, \u0026#34;D Again\u0026#34;); } Catch (A) { printf(\u0026#34;catch A again : %ld\\n\u0026#34;, selfid); } Catch (B) { printf(\u0026#34;catch B again : %ld\\n\u0026#34;, selfid); } Catch (C) { printf(\u0026#34;catch C again : %ld\\n\u0026#34;, selfid); } Catch (D) { printf(\u0026#34;catch B again : %ld\\n\u0026#34;, selfid); } EndTry; } #define THREADS\t50  int main(void) { ntyExceptionInit(); Throw(D, NULL); Throw(C, \u0026#34;null C\u0026#34;); printf(\u0026#34;\\n\\n=\u0026gt; Test1: Try-Catch\\n\u0026#34;); Try { Try { Throw(B, \u0026#34;recall B\u0026#34;); } Catch (B) { printf(\u0026#34;recall B \\n\u0026#34;); } EndTry; Throw(A, NULL); } Catch(A) { printf(\u0026#34;\\tResult: Ok\\n\u0026#34;); } EndTry; printf(\u0026#34;=\u0026gt; Test1: Ok\\n\\n\u0026#34;); printf(\u0026#34;=\u0026gt; Test2: Test Thread-safeness\\n\u0026#34;); #if 1 \tint i = 0; pthread_t threads[THREADS]; for (i = 0;i \u0026lt; THREADS;i ++) { pthread_create(\u0026amp;threads[i], NULL, thread, NULL); } for (i = 0;i \u0026lt; THREADS;i ++) { pthread_join(threads[i], NULL); } #endif \tprintf(\u0026#34;=\u0026gt; Test2: Ok\\n\\n\u0026#34;); } ","date":"2020-05-12T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/529_huc139e517d88aa84bf66b7ba5d5d134e7_26286259_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","title":"异常处理"},{"content":"#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define LL_ADD(item, list) do { \\ item-\u0026gt;prev = NULL;\t\\ item-\u0026gt;next = list;\t\\ list = item;\t\\ } while(0)  #define LL_REMOVE(item, list) do {\t\\ if (item-\u0026gt;prev != NULL) item-\u0026gt;prev-\u0026gt;next = item-\u0026gt;next;\t\\ if (item-\u0026gt;next != NULL) item-\u0026gt;next-\u0026gt;prev = item-\u0026gt;prev;\t\\ if (list == item) list = item-\u0026gt;next;\t\\ item-\u0026gt;prev = item-\u0026gt;next = NULL;\t\\ } while(0)  typedef struct NWORKER {//工作线程信息 \tpthread_t thread; //线程id \tint terminate; //是否要终止 \tstruct NWORKQUEUE *workqueue; //线程池，用于找到工作队列 \tstruct NWORKER *prev; struct NWORKER *next; } nWorker; typedef struct NJOB { //工作个体 \tvoid (*job_function)(struct NJOB *job); void *user_data; struct NJOB *prev; struct NJOB *next; } nJob; typedef struct NWORKQUEUE { struct NWORKER *workers; //所有工作线程的链表 \tstruct NJOB *waiting_jobs; //工作队列 \tpthread_mutex_t jobs_mtx; pthread_cond_t jobs_cond; } nWorkQueue; typedef nWorkQueue nThreadPool; static void *ntyWorkerThread(void *ptr) { //工作线程取用工作 \tnWorker *worker = (nWorker*)ptr; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //先获取工作队列的操作互斥锁  while (worker-\u0026gt;workqueue-\u0026gt;waiting_jobs == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_cond, \u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果工作队列为空，这个线程就阻塞在条件变量上等待事件发生 \t} if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果检测到工作线程被终止，那么这个线程就需要结束工作，但在结束工作前需要将对工作队列的取用权限放开，所以这里在break前需要解锁这个互斥锁 \tbreak; } nJob *job = worker-\u0026gt;workqueue-\u0026gt;waiting_jobs; //从工作队列中获取一个工作 \tif (job != NULL) { LL_REMOVE(job, worker-\u0026gt;workqueue-\u0026gt;waiting_jobs); //从工作队列中移除掉获取的这个工作 \tif (job != NULL) { } pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //已经取到工作了，就可以放开对工作队列的占有了  if (job == NULL) continue; job-\u0026gt;job_function(job); //针对工作调用他的回调函数处理，处理结束后继续循环去工作队列中取 \t} free(worker); //工作线程被终止那当然需要释放其堆上内存 \tpthread_exit(NULL); } int ntyThreadPoolCreate(nThreadPool *workqueue, int numWorkers) { //创建线程池  if (numWorkers \u0026lt; 1) numWorkers = 1; memset(workqueue, 0, sizeof(nThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; //条件变量用于通知所有的工作线程事件发生 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_cond, \u0026amp;blank_cond, sizeof(workqueue-\u0026gt;jobs_cond)); pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; // 互斥锁用于锁住工作队列确保同时只有一个工作线程在从工作队列中取工作 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_mtx, \u0026amp;blank_mutex, sizeof(workqueue-\u0026gt;jobs_mtx)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { nWorker *worker = (nWorker*)malloc(sizeof(nWorker)); //这里在堆上创建线程，那就需要在线程终止时释放 \tif (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return 1; } memset(worker, 0, sizeof(nWorker)); worker-\u0026gt;workqueue = workqueue; //初始化工作线程信息，线程池用于找到工作队列  int ret = pthread_create(\u0026amp;worker-\u0026gt;thread, NULL, ntyWorkerThread, (void *)worker); //创建线程，传入该线程信息，达到信息和线程的绑定关系 \tif (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return 1; } LL_ADD(worker, worker-\u0026gt;workqueue-\u0026gt;workers); //头插法在所有工作线程的链表中插入新建的工作线程 \t} return 0; } void ntyThreadPoolShutdown(nThreadPool *workqueue) { //关闭线程池 \tnWorker *worker = NULL; for (worker = workqueue-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate = 1; //所有工作线程的terminate关键字置为1 \t} pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); workqueue-\u0026gt;workers = NULL; //清空工作线程链表 \tworkqueue-\u0026gt;waiting_jobs = NULL; // 清空工作队列  pthread_cond_broadcast(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉所有工作线程有事件发生(shutdown，下一步检查terminate关键字)  pthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } void ntyThreadPoolQueue(nThreadPool *workqueue, nJob *job) { //向工作队列中添加工作  pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); LL_ADD(job, workqueue-\u0026gt;waiting_jobs); pthread_cond_signal(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉任意一个工作线程有事件发生(目前有新的工作出现在工作队列里了，下一步get互斥锁并取工作) \tpthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } /************************** debug thread pool **************************/ //sdk --\u0026gt; software develop kit // 提供SDK给其他开发者使用  #if 1  #define KING_MAX_THREAD\t80 #define KING_COUNTER_SIZE\t1000  void king_counter(nJob *job) { int index = *(int*)job-\u0026gt;user_data; printf(\u0026#34;index : %d, selfid : %lu\\n\u0026#34;, index, pthread_self()); free(job-\u0026gt;user_data); free(job); } int main(int argc, char *argv[]) { nThreadPool pool; ntyThreadPoolCreate(\u0026amp;pool, KING_MAX_THREAD); int i = 0; for (i = 0;i \u0026lt; KING_COUNTER_SIZE;i ++) { nJob *job = (nJob*)malloc(sizeof(nJob)); if (job == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } job-\u0026gt;job_function = king_counter; job-\u0026gt;user_data = malloc(sizeof(int)); *(int*)job-\u0026gt;user_data = i; ntyThreadPoolQueue(\u0026amp;pool, job); } getchar(); printf(\u0026#34;\\n\u0026#34;); } #endif  ","date":"2020-05-07T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/241_hudd714e7c3305b4c239dc02d266d283b9_7211866_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/","title":"线程池进阶版"},{"content":"线程池 基本功能模块：  线程池创建函数 线程池删除函数 线程池回调函数 线程池添加函数 线程池数据结构 线程任务数据结构 线程本身数据结构（由pid唯一确认）  首先实现数据结构： 线程任务数据结构：\nstruct nTask { void (*task_func)(struct nTask *task); void *user_data; struct nTask *prev; struct nTask *next; }; 这是任务中的一个个体，任务队列头存储在线程池数据结构中 void (*task_func)(struct nTask *task)函数指针表明函数为task_func且参数为struct nTask， 参数若为void是否更好\n线程本身数据结构：\nstruct nWorker { pthread_t threadid; int terminate; struct nManager *manager; struct nWorker *prev; struct nWorker *next; }; pid唯一标识线程，terminate用于标识该线程应被删除，存储manager（也就是所属线程池）是为了通过manager找到task队列以获取task\n线程池数据结构：\ntypedef struct nManager { struct nTask *tasks; struct nWorker *workers; pthread_mutex_t mutex; pthread_cond_t cond; } ThreadPool; 可以看到线程池其实只是一个管理者，使用mutex控制各个线程对进程内公共资源的访问，保证同时只有一个线程在访问公共资源，cond来控制各个线程的状态（处于等待队列（阻塞）或可以运行（运行、就绪态））细节在回调函数中\n然后实现API： 线程池创建函数：\nint nThreadPoolCreate(ThreadPool *pool, int numWorkers) { if (pool == NULL) return -1; if (numWorkers \u0026lt; 1) numWorkers = 1; memset(pool, 0, sizeof(ThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;cond, \u0026amp;blank_cond, sizeof(pthread_cond_t)); //pthread_mutex_init(\u0026amp;pool-\u0026gt;mutex, NULL); \tpthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;mutex, \u0026amp;blank_mutex, sizeof(pthread_mutex_t)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { struct nWorker *worker = (struct nWorker*)malloc(sizeof(struct nWorker)); if (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return -2; } memset(worker, 0, sizeof(struct nWorker)); worker-\u0026gt;manager = pool; //  int ret = pthread_create(\u0026amp;worker-\u0026gt;threadid, NULL, nThreadPoolCallback, worker); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return -3; } LIST_INSERT(worker, pool-\u0026gt;workers); } // success \treturn 0; } 根据传入线程数量参数，创建含有指定数量线程的线程池，初始化条件变量和互斥量，初始化线程本身然后放入队列\n线程池回调函数：\nstatic void *nThreadPoolCallback(void *arg) { struct nWorker *worker = (struct nWorker*)arg; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); while (worker-\u0026gt;manager-\u0026gt;tasks == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;manager-\u0026gt;cond, \u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); } if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); break; } struct nTask *task = worker-\u0026gt;manager-\u0026gt;tasks; LIST_REMOVE(task, worker-\u0026gt;manager-\u0026gt;tasks); pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); task-\u0026gt;task_func(task); //go task \t} free(worker); } 线程在创建并将其pid放入队列中后就运行回调函数，通过回调函数可以看到线程会阻塞在mutex上也可能阻塞在cond上 pthread_cond_wait函数使用两个参数： cond和mutex 这个函数等待在cond上并在收到signal或broadcast后返回，使主函数继续运行 在函数等待的时候，首先将该线程放到等待队列上然后释放mutex，这样可以保证其他线程对公共资源的访问 在收到cond的single或broadcast后线程会争夺mutex锁住临界区资源，然后自己消费，消费完后释放互斥锁 使用while循环可以保证在有资源到来的时候也就是signal cond的时候，速度慢的线程（没有抢到互斥锁的线程）可以发现资源已经被消耗完并重新通过pthread_cond_wait进入等待区\n可以看到只有在对临界区资源的访问中才加锁：访问任务队列并从中获取任务\n线程池添加函数：\nint nThreadPoolPushTask(ThreadPool *pool, struct nTask *task) { pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); LIST_INSERT(task, pool-\u0026gt;tasks); pthread_cond_signal(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); } 添加后通知整个线程队列，让他们消费\n线程池删除函数：\nint nThreadPoolDestory(ThreadPool *pool, int nWorker) { struct nWorker *worker = NULL; for (worker = pool-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate; } pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); pthread_cond_broadcast(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); pool-\u0026gt;workers = NULL; pool-\u0026gt;tasks = NULL; return 0; } 设置删除位terminate之后唤醒所有等待队列中的线程叫他们检查自己的删除位terminate 如果要删除该线程就退出while循环然后释放worker再退出\n附一个使用代码：\n#if 1  #define THREADPOOL_INIT_COUNT\t20 #define TASK_INIT_SIZE\t1000  void task_entry(struct nTask *task) { //type  //struct nTask *task = (struct nTask*)task; \tint idx = *(int *)task-\u0026gt;user_data; printf(\u0026#34;idx: %d\\n\u0026#34;, idx); free(task-\u0026gt;user_data); free(task); } int main(void) { ThreadPool pool = {0}; nThreadPoolCreate(\u0026amp;pool, THREADPOOL_INIT_COUNT); // pool --\u0026gt; memset(); \tint i = 0; for (i = 0;i \u0026lt; TASK_INIT_SIZE;i ++) { struct nTask *task = (struct nTask *)malloc(sizeof(struct nTask)); if (task == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } memset(task, 0, sizeof(struct nTask)); task-\u0026gt;task_func = task_entry; task-\u0026gt;user_data = malloc(sizeof(int)); *(int*)task-\u0026gt;user_data = i; nThreadPoolPushTask(\u0026amp;pool, task); } getchar(); } #endif ","date":"2020-04-26T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/240_huc6553c3d943592fc8492e4a8a4385797_5666304_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","title":"线程池"},{"content":"Reactor  组成：⾮阻塞的io + io多路复⽤； 特征：基于事件循环，以事件驱动或者事件回调的⽅式来实现业务逻辑； 表述：将连接的io处理转化为事件处理；  单Reactor \r\n代表：redis 内存数据库 操作redis当中的数据结构 redis 6.0 多线程\n单reactor模型 + 任务队列 + 线程池 \r\n代表：skynet\n多Reactor \r\n应⽤： memcached accept(fd, backlog) one eventloop per thread\n 多进程应用  \r\n应⽤：nginx\n多reactor + 消息队列 + 线程池 ","date":"2020-04-24T00:00:00Z","image":"https://gao377020481.github.io/p/reactor/516_huf7159154d5ef9e1554acf4947cd6a0ec_16113861_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/reactor/","title":"Reactor"},{"content":"定时器 定时器作用很多，常见于心跳检测，冷却等等 实现时区别主要在于定时器队列的管控（排序）\n基本方案： 红黑树： 使用红黑树对定时器排序，排序依据为定时器过期时间，每隔单位时间检查红黑树中最小时间是否小于等于当前时间，如果小于等于就删除节点并触发节点的callback。时间复杂度增删O(logn)，Nginx使用红黑树。删除添加操作自旋。\n最小堆： 最小堆根节点最小，直接拿出根节点与当前时间比较即可，删除操作将根节点与末尾节点对换并删除末尾节点然后将新的根节点下沉，添加时加入末尾节点并上升。\n时间轮：  时间轮可以分为单层级与多层级。简单的单层级时间轮使用初始化好的链表数组来存储对应的事件节点链表，时间数据结构中一般包含引用计数，该数据结构只有在引用计数置零后销毁，一般也代表着事件对应的资源可以释放。单层时间轮的大小至少需要大于最长定时时间/单位时间，举例：每5秒发送一个心跳包，连接收到心跳包时需要开启一个10秒的定时器并将事件引用计数加一（事件数据结构插入链表数组中10秒后的链表中），也就是最长定时10秒，10秒后检查该连接对应的事件并将引用计数减一，如果减一后为0就说明连接超时，释放所有资源，关闭事件。在该例子中，初始化的链表数组大小至少为11，因为假如在第0秒来一个心跳包，我们就需要在第10号位置将该连接对应的事件节点加入事件链表中，如果小于11，比如为8，那从0开始往后10个的位置就是在2号位置，那2秒后就得触发了，这与我们设置的10秒定时时间不一致。\n\r\n代码实现： 红黑树： 红黑树数据结构直接使用nginx自带的rbtree头文件，就不自己写了\n红黑树定时器头文件： #ifndef _MARK_RBT_ #define _MARK_RBT_  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdint.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stddef.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026#34;rbtree.h\u0026#34; ngx_rbtree_t timer; static ngx_rbtree_node_t sentinel; typedef struct timer_entry_s timer_entry_t; typedef void (*timer_handler_pt)(timer_entry_t *ev); struct timer_entry_s { ngx_rbtree_node_t timer; timer_handler_pt handler; }; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int init_timer() { ngx_rbtree_init(\u0026amp;timer, \u0026amp;sentinel, ngx_rbtree_insert_timer_value); return 0; } void add_timer(timer_entry_t *te, uint32_t msec) { msec += current_time(); printf(\u0026#34;add_timer expire at msec = %u\\n\u0026#34;, msec); te-\u0026gt;timer.key = msec; ngx_rbtree_insert(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void del_timer(timer_entry_t *te) { ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void expire_timer() { timer_entry_t *te; ngx_rbtree_node_t *sentinel, *root, *node; sentinel = timer.sentinel; uint32_t now = current_time(); for (;;) { root = timer.root; if (root == sentinel) break; node = ngx_rbtree_min(root, sentinel); if (node-\u0026gt;key \u0026gt; now) break; printf(\u0026#34;touch timer expire time=%u, now = %u\\n\u0026#34;, node-\u0026gt;key, now); te = (timer_entry_t *) ((char *) node - offsetof(timer_entry_t, timer)); te-\u0026gt;handler(te); ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); free(te); } } #endif  expire_timer() 检查红黑树中是否有过期定时器，清理所有过期定时器并触发对应的回调函数\nadd_timer 向红黑树中插入事件，使用nginx定时器专用的红黑树插入函数，这里红黑树使用黑色哨兵sentinel，其含义如下：\n红黑树有一个性质是：\r叶结点的左右两边必须为黑色。也就是本来叶结点如果没有左右孩子直接初始化为NULL就是了，但它居然要黑色，意味着我需要新分配两块内存空间啥数据也不保存，tm就是为了给它涂上黑色然后挂到叶结点的left、right。\r当叶结点多起来的时候你说多浪费内存空间？理想的二叉树结构是为了让我们保存数据（key），而不是为了保存颜色吧？\r所以哨兵这个外援就来了，我们申请一块内存命名为哨兵，然后把这块内存涂上黑色，之后所有没有孩子的叶结点left、right都指向这个已涂上黑色的哨兵。以上是红黑树哨兵的作用。\r其他函数较简单\n红黑树定时器主文件： #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;rbt-timer.h\u0026#34; void print_hello(timer_entry_t *te) { printf(\u0026#34;hello world time = %u\\n\u0026#34;, te-\u0026gt;timer.key); } int main() { init_timer(); timer_entry_t *te = malloc(sizeof(timer_entry_t)); memset(te, 0, sizeof(timer_entry_t)); te-\u0026gt;handler = print_hello; add_timer(te, 3000); for (;;) { expire_timer(); usleep(10000); } return 0; } 主函数中主事件循环使用单位10秒usleep(10000),usleep中参数单位为ms\n最小堆： 最小堆定时器头文件： #pragma once  #include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;using namespace std; typedef void (*TimerHandler) (struct TimerNode *node); struct TimerNode { int idx = 0; int id = 0; unsigned int expire = 0; TimerHandler cb = NULL; }; class MinHeapTimer { public: MinHeapTimer() { _heap.clear(); _map.clear(); } static inline int Count() { return ++_count; } int AddTimer(uint32_t expire, TimerHandler cb); bool DelTimer(int id); void ExpireTimer(); private: inline bool _lessThan(int lhs, int rhs) { return _heap[lhs]-\u0026gt;expire \u0026lt; _heap[rhs]-\u0026gt;expire; } bool _shiftDown(int pos); void _shiftUp(int pos); void _delNode(TimerNode *node); private: vector\u0026lt;TimerNode*\u0026gt; _heap; map\u0026lt;int, TimerNode*\u0026gt; _map; static int _count; }; int MinHeapTimer::_count = 0; 需要分析的是struct TimerNode这个数据结构，idx指示在最小堆数组中的位置，用于在定时器过期时确认位置，id用于与_map协作以快速定位并删除特定id的节点，id的值自增长，expire为定时长度，cd是回调函数,具体可以看代码。\n最小堆定时器主文件： #include \u0026lt;unistd.h\u0026gt;#if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026lt;iostream\u0026gt; #include \u0026#34;minheap.h\u0026#34; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int MinHeapTimer::AddTimer(uint32_t expire, TimerHandler cb) { int64_t timeout = current_time() + expire; TimerNode* node = new TimerNode; int id = Count(); node-\u0026gt;id = id; node-\u0026gt;expire = timeout; node-\u0026gt;cb = cb; node-\u0026gt;idx = (int)_heap.size(); _heap.push_back(node); _shiftUp((int)_heap.size() - 1); _map.insert(make_pair(id, node)); return id; } bool MinHeapTimer::DelTimer(int id) { auto iter = _map.find(id); if (iter == _map.end()) return false; _delNode(iter-\u0026gt;second); return true; } void MinHeapTimer::_delNode(TimerNode *node) { int last = (int)_heap.size() - 1; int idx = node-\u0026gt;idx; if (idx != last) { std::swap(_heap[idx], _heap[last]); _heap[idx]-\u0026gt;idx = idx; if (!_shiftDown(idx)) { _shiftUp(idx); } } _heap.pop_back(); _map.erase(node-\u0026gt;id); delete node; } void MinHeapTimer::ExpireTimer() { if (_heap.empty()) return; uint32_t now = current_time(); do { TimerNode* node = _heap.front(); if (now \u0026lt; node-\u0026gt;expire) break; for (int i = 0; i \u0026lt; _heap.size(); i++) std::cout \u0026lt;\u0026lt; \u0026#34;touch idx: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34; id: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;id \u0026lt;\u0026lt; \u0026#34; expire: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;expire \u0026lt;\u0026lt; std::endl; if (node-\u0026gt;cb) { node-\u0026gt;cb(node); } _delNode(node); } while(!_heap.empty()); } bool MinHeapTimer::_shiftDown(int pos){ int last = (int)_heap.size()-1; int idx = pos; for (;;) { int left = 2 * idx + 1; if ((left \u0026gt;= last) || (left \u0026lt; 0)) { break; } int min = left; // left child  int right = left + 1; if (right \u0026lt; last \u0026amp;\u0026amp; !_lessThan(left, right)) { min = right; // right child  } if (!_lessThan(min, idx)) { break; } std::swap(_heap[idx], _heap[min]); _heap[idx]-\u0026gt;idx = idx; _heap[min]-\u0026gt;idx = min; idx = min; } return idx \u0026gt; pos; } void MinHeapTimer::_shiftUp(int pos) { for (;;) { int parent = (pos - 1) / 2; // parent node  if (parent == pos || !_lessThan(pos, parent)) { break; } std::swap(_heap[parent], _heap[pos]); _heap[parent]-\u0026gt;idx = parent; _heap[pos]-\u0026gt;idx = pos; pos = parent; } } void print_hello(TimerNode *te) { std::cout \u0026lt;\u0026lt; \u0026#34;hello world time = \u0026#34; \u0026lt;\u0026lt; te-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34;\\t\u0026#34; \u0026lt;\u0026lt; te-\u0026gt;id \u0026lt;\u0026lt; std::endl; } int main() { MinHeapTimer mht; mht.AddTimer(0, print_hello); mht.AddTimer(1000, print_hello); mht.AddTimer(7000, print_hello); mht.AddTimer(2000, print_hello); mht.AddTimer(9000, print_hello); mht.AddTimer(10000, print_hello); mht.AddTimer(6000, print_hello); mht.AddTimer(3000, print_hello); for (;;) { mht.ExpireTimer(); usleep(10000); } return 0; } 思路与红黑树的大致相同，不做赘述\n单层时间轮： 与最小堆不同，我们这里需要使用静态数组，宏定义数组大小即可，最长定时时间10秒，单位时间1秒，所以使用大于10的数组大小。\n#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdint.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #define MAX_TIMER ((1\u0026lt;\u0026lt;17)-1) #define MAX_CONN ((1\u0026lt;\u0026lt;16)-1)  typedef struct conn_node { //也可以考虑添加callback  uint8_t used; int id; } conn_node_t; typedef struct timer_node { //定时器节点 \tstruct timer_node *next; struct conn_node *node; uint32_t idx; } timer_node_t; static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储 static conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储  static uint32_t t_iter = 0; //创建过的定时器节点数目 static uint32_t c_iter = 0;//创建过的连接节点数目  timer_node_t * get_timer_node() { // 注意：没有检测定时任务数超过 MAX_TIMER 的情况！！！  t_iter++; while (timer_nodes[t_iter \u0026amp; MAX_TIMER].idx \u0026gt; 0) { t_iter++; //这个定时器节点正在被使用，换下一个试试  } timer_nodes[t_iter].idx = t_iter; return \u0026amp;timer_nodes[t_iter]; } conn_node_t * get_conn_node() { // 注意：没有检测连接数超过 MAX_CONN 的情况  c_iter++; while (conn_nodes[c_iter \u0026amp; MAX_CONN].used \u0026gt; 0) { c_iter++;//这个连接节点正在被使用，换下一个试试  } return \u0026amp;conn_nodes[c_iter]; } #define TW_SIZE 16 #define EXPIRE 10 #define TW_MASK (TW_SIZE - 1) static uint32_t tick = 0; typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; void add_conn(link_list_t *tw, conn_node_t *cnode, int delay) { link_list_t *list = \u0026amp;tw[(tick+EXPIRE+delay) \u0026amp; TW_MASK]; timer_node_t * tnode = get_timer_node(); cnode-\u0026gt;used++; tnode-\u0026gt;node = cnode; list-\u0026gt;tail-\u0026gt;next = tnode; list-\u0026gt;tail = tnode; tnode-\u0026gt;next = NULL; } //尾插，因为先到的定时任务需要先访问，比较严谨  void link_clear(link_list_t *list) { list-\u0026gt;head.next = NULL; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); } void check_conn(link_list_t *tw) { int32_t itick = tick; //获取当前时间戳  tick++;//全局时间戳加一  link_list_t *list = \u0026amp;tw[itick \u0026amp; TW_MASK];// 获取当前时间戳对应的定时器链表，对其进行检查  timer_node_t *current = list-\u0026gt;head.next;//定时器链表的头并不是一个实际的定时器节点，所以获取head的next，这是第一个实际的定时器节点  while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; //下一个  conn_node_t *cn = temp-\u0026gt;node; //从定时器节点中拿到对应的连接  cn-\u0026gt;used--; //连接的引用计数减一  temp-\u0026gt;idx = 0;//该定时器可以回收留待下次使用  if (cn-\u0026gt;used == 0) { //引用计数为0，说明连接失活，可以做相应处理  printf(\u0026#34;fd:%d kill down\\n\u0026#34;, cn-\u0026gt;id); temp-\u0026gt;next = NULL; continue; } printf(\u0026#34;fd:%d used:%d\\n\u0026#34;, cn-\u0026gt;id, cn-\u0026gt;used); } link_clear(list);//检查完这一时间戳的事件就可以删除了，等待下一次继续添加 } static time_t current_time() { time_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (time_t)ti.tv_sec; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (time_t)tv.tv_sec; #endif \treturn t; } int main() { memset(timer_nodes, 0, MAX_TIMER * sizeof(timer_node_t)); memset(conn_nodes, 0, MAX_CONN * sizeof(conn_node_t)); // init link list  link_list_t tw[TW_SIZE]; memset(tw, 0, TW_SIZE * sizeof(link_list_t)); for (int i = 0; i \u0026lt; TW_SIZE; i++) { link_clear(\u0026amp;tw[i]); } // 该测试起始时间为0秒，所以 delay 不能添加超过10的数。  { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10001; add_conn(tw, node, 0); add_conn(tw, node, 5); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10002; add_conn(tw, node, 0); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10003; add_conn(tw, node, 3); } time_t start = current_time(); for (;;) { time_t now = current_time(); if (now - start \u0026gt; 0) { for (int i=0; i\u0026lt;now-start; i++) //循环保证单位时间移动（1秒走一格）  check_conn(tw); start = now; printf(\u0026#34;check conn tick:%d\\n\u0026#34;, tick); } usleep(20000); } return 0; } static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储\nstatic conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储\n创建节点存储数组可以做到节点的回收利用，减少节点的创建和删除，静态数组连续存储，访问时间快\n#define TW_SIZE 16 时间轮大小\n#define EXPIRE 10 定时时间\n#define TW_MASK (TW_SIZE - 1) 用于轮转的掩码\nstatic uint32_t tick = 0; 时间戳,指示当前位置 add_conn(link_list_t *tw, conn_node_t *cnode, int delay)\n可以设置delay，根据提供的conn_node_t初始化定时任务，并放入tw中正确位置：(tick+EXPIRE+delay) \u0026amp; TW_MASK\nlink_clear(link_list_t *list)清空一个时间戳上的整个定时器链表\ncheck_conn(link_list_t *tw) 检查当前时间戳对应定时器链表的连接，并对齐引用计数减一，引用计数为0可以做相应处理（回调函数中可以关闭conn，或者给一个信号到管理连接的线程等等）\n多层时间轮： 多层时间轮就是在单层的基础上添加几个粒度更大的层，我们只在粒度最小的层上运行定时器（粒度最小的层运行与单层时间轮一样）即可，用钟表举例：最小层运行一周代表秒针转一圈也就是需要转分针了，这时就从第二层上映射下一分钟的所有定时任务到最小层上，第二层每一分钟的任务同时也需要前移（只要移动指针就可以，不用移动所有任务链表），前移操作很容易理解，但映射需要说一下。举例：\n我们有一个定时任务是在1分27秒后触发，模拟时钟，第一层的数组大小为60，代表60秒，第二层也是60代表60分钟，第三层24。假设此时tick（时间戳）为0，我们需要将这个定时任务放置于第二层的第0号位置（这里先不区分粒度），并在任务内保存准确的expire时间（1分27），当第一层60秒转完后，我们就需要开始映射，将第二层0号位置上的一串定时任务遍历，一个一个根据其保存的expire时间对60秒取余获得其应该在第一层的位置（27秒对应26号位置），映射到第一层上的26号位置（串到26号下面的链表中）。在下一轮的第一层运转中就会触发定时任务啦。同理，第三层（时针）到第二层（分针）的映射也是相同，使用mask取出定时任务的分位（比如1小时32分15秒就是取出32分，对应第二层的31号位置），找到对应位置放到对应链表中去。\n写的代码的时间轮结构如图：\n\r\n这里所有层级的位数加起来刚好是32位，最低层级使用2的8次方，后续四层都为2的四次方，这样的设计正好用完一个unsigned INT数，在使用mask的时候十分方便。最低层级粒度为10ms。\n这里直接附上代码，自行阅读即可：\n头文件\n#ifndef _MARK_TIMEWHEEL_ #define _MARK_TIMEWHEEL_  #include \u0026lt;stdint.h\u0026gt; #define TIME_NEAR_SHIFT 8 #define TIME_NEAR (1 \u0026lt;\u0026lt; TIME_NEAR_SHIFT) #define TIME_LEVEL_SHIFT 6 #define TIME_LEVEL (1 \u0026lt;\u0026lt; TIME_LEVEL_SHIFT) #define TIME_NEAR_MASK (TIME_NEAR-1) #define TIME_LEVEL_MASK (TIME_LEVEL-1)  typedef struct timer_node timer_node_t; typedef void (*handler_pt) (struct timer_node *node); struct timer_node { struct timer_node *next; uint32_t expire;//过期时间（非定时时间）  handler_pt callback; uint8_t cancel; //由于idx很多且不断变化很难找到特定节点，通过设计cancel成员来在触发时取消触发操作 \tint id; // 此时携带参数 }; timer_node_t* add_timer(int time, handler_pt func, int threadid); void expire_timer(void); void del_timer(timer_node_t* node); void init_timer(void); void clear_timer(); #endif  主文件\n#include \u0026#34;spinlock.h\u0026#34;#include \u0026#34;timewheel.h\u0026#34;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stddef.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; typedef struct timer { link_list_t near[TIME_NEAR]; link_list_t t[4][TIME_LEVEL]; struct spinlock lock; uint32_t time; uint64_t current; uint64_t current_point; }s_timer_t; static s_timer_t * TI = NULL; timer_node_t * link_clear(link_list_t *list) { timer_node_t * ret = list-\u0026gt;head.next; list-\u0026gt;head.next = 0; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); return ret; } void link(link_list_t *list, timer_node_t *node) { list-\u0026gt;tail-\u0026gt;next = node; list-\u0026gt;tail = node; node-\u0026gt;next=0; } void add_node(s_timer_t *T, timer_node_t *node) { uint32_t time=node-\u0026gt;expire; uint32_t current_time=T-\u0026gt;time; uint32_t msec = time - current_time; if (msec \u0026lt; TIME_NEAR) { //[0, 0x100) \tlink(\u0026amp;T-\u0026gt;near[time\u0026amp;TIME_NEAR_MASK],node); } else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+TIME_LEVEL_SHIFT))) {//[0x100, 0x4000) \tlink(\u0026amp;T-\u0026gt;t[0][((time\u0026gt;\u0026gt;TIME_NEAR_SHIFT) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+2*TIME_LEVEL_SHIFT))) {//[0x4000, 0x100000) \tlink(\u0026amp;T-\u0026gt;t[1][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+3*TIME_LEVEL_SHIFT))) {//[0x100000, 0x4000000) \tlink(\u0026amp;T-\u0026gt;t[2][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 2*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else {//[0x4000000, 0xffffffff] \tlink(\u0026amp;T-\u0026gt;t[3][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 3*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} } timer_node_t* add_timer(int time, handler_pt func, int threadid) { timer_node_t *node = (timer_node_t *)malloc(sizeof(*node)); spinlock_lock(\u0026amp;TI-\u0026gt;lock); node-\u0026gt;expire = time+TI-\u0026gt;time;// 每10ms加1 0 \tnode-\u0026gt;callback = func; node-\u0026gt;id = threadid; if (time \u0026lt;= 0) { node-\u0026gt;callback(node); free(node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return NULL; } add_node(TI, node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return node; } void move_list(s_timer_t *T, int level, int idx) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;t[level][idx]); while (current) { timer_node_t *temp=current-\u0026gt;next; add_node(T,current); current=temp; } } void timer_shift(s_timer_t *T) { int mask = TIME_NEAR; uint32_t ct = ++T-\u0026gt;time; if (ct == 0) { move_list(T, 3, 0); } else { // ct / 256 \tuint32_t time = ct \u0026gt;\u0026gt; TIME_NEAR_SHIFT; int i=0; // ct % 256 == 0 \twhile ((ct \u0026amp; (mask-1))==0) { int idx=time \u0026amp; TIME_LEVEL_MASK; if (idx!=0) { move_list(T, i, idx); break;\t} mask \u0026lt;\u0026lt;= TIME_LEVEL_SHIFT; time \u0026gt;\u0026gt;= TIME_LEVEL_SHIFT; ++i; } } } void dispatch_list(timer_node_t *current) { do { timer_node_t * temp = current; current=current-\u0026gt;next; if (temp-\u0026gt;cancel == 0) temp-\u0026gt;callback(temp); free(temp); } while (current); } void timer_execute(s_timer_t *T) { int idx = T-\u0026gt;time \u0026amp; TIME_NEAR_MASK; while (T-\u0026gt;near[idx].head.next) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;near[idx]); spinlock_unlock(\u0026amp;T-\u0026gt;lock); dispatch_list(current); spinlock_lock(\u0026amp;T-\u0026gt;lock); } } void timer_update(s_timer_t *T) { spinlock_lock(\u0026amp;T-\u0026gt;lock); timer_execute(T); timer_shift(T); timer_execute(T); spinlock_unlock(\u0026amp;T-\u0026gt;lock); } void del_timer(timer_node_t *node) { node-\u0026gt;cancel = 1; } s_timer_t * timer_create_timer() { s_timer_t *r=(s_timer_t *)malloc(sizeof(s_timer_t)); memset(r,0,sizeof(*r)); int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_clear(\u0026amp;r-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_clear(\u0026amp;r-\u0026gt;t[i][j]); } } spinlock_init(\u0026amp;r-\u0026gt;lock); r-\u0026gt;current = 0; return r; } uint64_t gettime() { uint64_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint64_t)ti.tv_sec * 100; t += ti.tv_nsec / 10000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint64_t)tv.tv_sec * 100; t += tv.tv_usec / 10000; #endif \treturn t; } void expire_timer(void) { uint64_t cp = gettime(); if (cp != TI-\u0026gt;current_point) { uint32_t diff = (uint32_t)(cp - TI-\u0026gt;current_point); TI-\u0026gt;current_point = cp; int i; for (i=0; i\u0026lt;diff; i++) { timer_update(TI); } } } void init_timer(void) { TI = timer_create_timer(); TI-\u0026gt;current_point = gettime(); } void clear_timer() { int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_list_t * list = \u0026amp;TI-\u0026gt;near[i]; timer_node_t* current = list-\u0026gt;head.next; while(current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_list_t * list = \u0026amp;TI-\u0026gt;t[i][j]; timer_node_t* current = list-\u0026gt;head.next; while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;t[i][j]); } } } ","date":"2020-04-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/427_hud2c5f23b59d3c6c6e1b3f4674af3ff5b_6078631_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/","title":"定时器"},{"content":"进阶TCP服务器 该模块涉及TCP服务器常见点：并发量、IO模型\nIO网络模型 阻塞IO （Blocking IO） \r\n阻塞IO的情况下，我们如果需要更多的并发，只能使用多线程，一个IO占用一个线程，资源浪费很大但是在并发量小的情况下性能很强。\n非阻塞IO （Non-Blocking IO） \r\n在非阻塞状态下，recv() 接口在被调用后立即返回，返回值代表了不同的含义。如在本例中，\n recv() 返回值大于 0，表示接受数据完毕，返回值即是接受到的字节数； recv() 返回 0，表示连接已经正常断开； recv() 返回 -1，且 errno 等于 EAGAIN，表示 recv 操作还没执行完成； recv() 返回 -1，且 errno 不等于 EAGAIN，表示 recv 操作遇到系统错误 errno。  非阻塞的接口相比于阻塞型接口的显著差异在于，在被调用之后立即返回。使用如下的函数可以将某句柄 fd 设为非阻塞状态：\nfcntl( fd, F_SETFL, O_NONBLOCK ); 多路复用IO （IO Multiplexing） \r\n多路复用IO，select/poll、epoll。\nselect/poll select和poll很相似，在检测IO时间的时候都需要遍历整个FD存储结构，只是select使用数组存储FD，其具有最大值限制，而poll使用链表无最大值限制（与内存大小相关）。\n先来分析select的优缺点，这样就知道epoll相比select的优势等。\nselect 本质上是通过设置或检查存放fd标志位的数据结构进行下一步处理。 这带来缺点： 单个进程可监视的fd数量被限制，即能监听端口的数量有限 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试 一般该数和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认1024个，64位默认2048。\n当socket较多时，每次select都要通过遍历FD_SETSIZE个socket，不管是否活跃，这会浪费很多CPU时间。如果能给 socket 注册某个回调函数，当他们活跃时，自动完成相关操作，即可避免轮询，这就是epoll与kqueue。\nselect 调用流程\n  使用copy_from_user从用户空间拷贝fd_set到内核空间\n  注册回调函数__pollwait\n  遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）\n  以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n  __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk-\u0026gt;sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。\n  poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。\n  如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n  把fd_set从内核空间拷贝到用户空间。\n    内核需要将消息传递到用户空间，都需要内核拷贝动作。需要维护一个用来存放大量fd的数据结构，使得用户空间和内核空间在传递该结构时复制开销大。\n  每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大\n  同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大\n  select支持的文件描述符数量太小了，默认是1024\n  epoll  没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口） 效率提升，不是轮询，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数 即Epoll最大的优点就在于它只关心“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 epoll通过内核和用户空间共享一块内存来实现的  异步 IO（Asynchronous I/O） \r\n异步IO工作流程可以看图\n信号驱动 IO（signal driven I/O， SIGIO） \r\n服务器模型 Reactor 首先来回想一下普通函数调用的机制：程序调用某函数，函数执行，程序等待，函数将 结果和控制权返回给程序，程序继续处理。Reactor 释义“反应堆”，是一种事件驱动机制。 和普通函数调用的不同之处在于：应用程序不是主动的调用某个 API 完成处理，而是恰恰 相反，Reactor 逆置了事件处理流程，应用程序需要提供相应的接口并注册到 Reactor 上， 如果相应的时间发生，Reactor 将主动调用应用程序注册的接口，这些接口又称为“回调函 数”。\nReactor 模式是处理并发 I/O 比较常见的一种模式，用于同步 I/O，中心思想是将所有要 处理的 I/O 事件注册到一个中心 I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上； 一旦有 I/O 事件到来或是准备就绪(文件描述符或 socket 可读、写)，多路复用器返回并将事 先注册的相应 I/O 事件分发到对应的处理器中。\nReactor 模型有三个重要的组件：\n 多路复用器：由操作系统提供，在 linux 上一般是 select, poll, epoll 等系统调用。 事件分发器：将多路复用器中返回的就绪事件分到对应的处理函数中。 事件处理器：负责处理特定事件的处理函数。  Proactor Proactor 最大的特点是 使用异步 I/O。所有的 I/O 操作都交由系统提供的异步 I/O 接口去执行。工作线程仅仅负责业务逻辑。在 Proactor 中，用户函数启动一个异步的文件操作。同时将这个操作注册到多路复用器上。多路复用器并不关心文件是否可读或可写而是关心这个异步读操作是否完成。异步操作是操作系统完成，用户程序不需要关心。多路复用器等待直到有完成通知到来。当操作系统完成了读文件操作——将读到的数据复制到了用户先前提供的缓冲区之后，通知多路复用器相关操作已完成。多路复用器再调用相应的处理程序，处理数据。\nProactor 增加了编程的复杂度，但给工作线程带来了更高的效率。Proactor 可以在 系统态将读写优化，利用 I/O 并行能力，提供一个高性能单线程模型。在 windows 上，由于没有 epoll 这样的机制，因此提供了 IOCP 来支持高并发， 由于操作系统做了较好的优化，windows 较常采用 Proactor 的模型利用完成端口来实现服务器。在 linux 上，在2.6 内核出现了 aio 接口，但 aio 实际效果并不理想，它的出现，主要是解决 poll 性能不佳的问题，但实际上经过测试，epoll 的性能高于 poll+aio，并且 aio 不能处理 accept，因此 linux 主要还是以 Reactor 模型为主。在不使用操作系统提供的异步 I/O 接口的情况下，还可以使用 Reactor 来模拟 Proactor， 差别是：使用异步接口可以利用系统提供的读写并行能力，而在模拟的情况下，这需要在用户态实现。具体的做法只需要这样：\n 注册读事件（同时再提供一段缓冲区） 事件分离器等待可读事件 事件到来，激活分离器，分离器（立即读数据，写缓冲区）调用事件处理器 事件处理器处理数据，删除事件(需要再用异步接口注册)  我们知道，Boost.asio 库采用的即为 Proactor 模型。不过 Boost.asio 库在 Linux 平台采用 epoll 实现的 Reactor 来模拟 Proactor，并且另外开了一个线程来完成读写调度。\n并发量 通过linux系统的配置、使用多路复用的IO就可以很容易达到百万的并发量这个数量级。\n配置： net.ipv4.tcp_mem = 252144 524288 786432 net.ipv4.tcp_wmem = 2048 2048 4096 net.ipv4.tcp_rmem = 2048 2048 4096 fs.file-max = 1048576 net.nf_conntrack_max = 1048576 net.netfilter.nf_conntrack_tcp_timeout_established = 1200 fs.file-max为fd可以达到的最大值\nnet.nf_conntrack_max为防火墙允许的最大连接数\nnet.ipv4.tcp_mem 三道台阶，协议栈占有空间大于这三道台阶时采取不同的优化方案\nnet.ipv4.tcp_wmem 和 net.ipv4.tcp_rmem 为写和收buffer大小阶梯，默认为中间大小，动态变化的最大值为第三个值。\n上面的buffer根据传输的文件类型可以采用不同的值。\nulimit -a 可以看文件描述符数量限制，调整到1048576（ulimit -n）\nIO： 使用epoll，监听多个端口（100就够）\n实现（Reactor-Epoll） #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt;#include \u0026lt;netinet/tcp.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt;#include \u0026lt;pthread.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;sys/epoll.h\u0026gt; #define BUFFER_LENGTH\t1024  struct sockitem { // \tint sockfd; int (*callback)(int fd, int events, void *arg); char recvbuffer[BUFFER_LENGTH]; // \tchar sendbuffer[BUFFER_LENGTH]; int rlength; int slength; }; // mainloop / eventloop --\u0026gt; epoll --\u0026gt; struct reactor { int epfd; struct epoll_event events[512]; }; struct reactor *eventloop = NULL; int recv_cb(int fd, int events, void *arg); int send_cb(int fd, int events, void *arg) { struct sockitem *si = (struct sockitem*)arg; send(fd, si-\u0026gt;sendbuffer, si-\u0026gt;slength, 0); //  struct epoll_event ev; ev.events = EPOLLIN | EPOLLET; //ev.data.fd = clientfd; \tsi-\u0026gt;sockfd = fd; si-\u0026gt;callback = recv_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_MOD, fd, \u0026amp;ev); } // ./epoll 8080  int recv_cb(int fd, int events, void *arg) { //int clientfd = events[i].data.fd; \tstruct sockitem *si = (struct sockitem*)arg; struct epoll_event ev; //char buffer[1024] = {0}; \tint ret = recv(fd, si-\u0026gt;recvbuffer, BUFFER_LENGTH, 0); if (ret \u0026lt; 0) { if (errno == EAGAIN || errno == EWOULDBLOCK) { // \treturn -1; } else { } ev.events = EPOLLIN; //ev.data.fd = fd; \tepoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_DEL, fd, \u0026amp;ev); close(fd); free(si); } else if (ret == 0) { //  // \tprintf(\u0026#34;disconnect %d\\n\u0026#34;, fd); ev.events = EPOLLIN; //ev.data.fd = fd; \tepoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_DEL, fd, \u0026amp;ev); close(fd); free(si); } else { printf(\u0026#34;Recv: %s, %d Bytes\\n\u0026#34;, si-\u0026gt;recvbuffer, ret); si-\u0026gt;rlength = ret; memcpy(si-\u0026gt;sendbuffer, si-\u0026gt;recvbuffer, si-\u0026gt;rlength); si-\u0026gt;slength = si-\u0026gt;rlength; struct epoll_event ev; ev.events = EPOLLOUT | EPOLLET; //ev.data.fd = clientfd; \tsi-\u0026gt;sockfd = fd; si-\u0026gt;callback = send_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_MOD, fd, \u0026amp;ev); } } int accept_cb(int fd, int events, void *arg) { struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(fd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); if (clientfd \u0026lt;= 0) return -1; char str[INET_ADDRSTRLEN] = {0}; printf(\u0026#34;recv from %s at port %d\\n\u0026#34;, inet_ntop(AF_INET, \u0026amp;client_addr.sin_addr, str, sizeof(str)), ntohs(client_addr.sin_port)); struct epoll_event ev; ev.events = EPOLLIN | EPOLLET; //ev.data.fd = clientfd;  struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem)); si-\u0026gt;sockfd = clientfd; si-\u0026gt;callback = recv_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_ADD, clientfd, \u0026amp;ev); return clientfd; } int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { return -1; } int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); if (sockfd \u0026lt; 0) { return -1; } struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if (bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt; 0) { return -2; } if (listen(sockfd, 5) \u0026lt; 0) { return -3; } eventloop = (struct reactor*)malloc(sizeof(struct reactor)); // epoll opera  eventloop-\u0026gt;epfd = epoll_create(1); struct epoll_event ev; ev.events = EPOLLIN; struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem)); si-\u0026gt;sockfd = sockfd; si-\u0026gt;callback = accept_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); while (1) { int nready = epoll_wait(eventloop-\u0026gt;epfd, eventloop-\u0026gt;events, 512, -1); if (nready \u0026lt; -1) { break; } int i = 0; for (i = 0;i \u0026lt; nready;i ++) { if (eventloop-\u0026gt;events[i].events \u0026amp; EPOLLIN) { //printf(\u0026#34;sockitem\\n\u0026#34;); \tstruct sockitem *si = (struct sockitem*)eventloop-\u0026gt;events[i].data.ptr; si-\u0026gt;callback(si-\u0026gt;sockfd, eventloop-\u0026gt;events[i].events, si); } if (eventloop-\u0026gt;events[i].events \u0026amp; EPOLLOUT) { struct sockitem *si = (struct sockitem*)eventloop-\u0026gt;events[i].data.ptr; si-\u0026gt;callback(si-\u0026gt;sockfd, eventloop-\u0026gt;events[i].events, si); } } } } 实现（Reactor-Epoll的百万并发） 前面有reactor-epoll自己实现的了，这里直接使用netty的reactor实现就可以。\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;errno.h\u0026gt; #define BUFFER_LENGTH\t1024 #define MAX_EPOLL_EVENTS\t1024*1024 //connection #define MAX_EPOLL_ITEM\t102400 //con #define SERVER_PORT\t8888  #define LISTEN_PORT_COUNT\t100  typedef int NCALLBACK(int ,int, void*); struct ntyevent { int fd; int events; void *arg; int (*callback)(int fd, int events, void *arg); int status; char buffer[BUFFER_LENGTH]; int length; long last_active; }; struct ntyreactor { int epfd; struct ntyevent *events; // 1024 * 1024 }; int recv_cb(int fd, int events, void *arg); int send_cb(int fd, int events, void *arg); void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) { ev-\u0026gt;fd = fd; ev-\u0026gt;callback = callback; ev-\u0026gt;events = 0; ev-\u0026gt;arg = arg; ev-\u0026gt;last_active = time(NULL); return ; } int nty_event_add(int epfd, int events, struct ntyevent *ev) { struct epoll_event ep_ev = {0, {0}}; ep_ev.data.ptr = ev; ep_ev.events = ev-\u0026gt;events = events; int op; if (ev-\u0026gt;status == 1) { op = EPOLL_CTL_MOD; } else { op = EPOLL_CTL_ADD; ev-\u0026gt;status = 1; } if (epoll_ctl(epfd, op, ev-\u0026gt;fd, \u0026amp;ep_ev) \u0026lt; 0) { printf(\u0026#34;event add failed [fd=%d], events[%d]\\n\u0026#34;, ev-\u0026gt;fd, events); return -1; } return 0; } int nty_event_del(int epfd, struct ntyevent *ev) { struct epoll_event ep_ev = {0, {0}}; if (ev-\u0026gt;status != 1) { return -1; } ep_ev.data.ptr = ev; ev-\u0026gt;status = 0; epoll_ctl(epfd, EPOLL_CTL_DEL, ev-\u0026gt;fd, \u0026amp;ep_ev); return 0; } int recv_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; struct ntyevent *ev = reactor-\u0026gt;events+fd; int len = recv(fd, ev-\u0026gt;buffer, BUFFER_LENGTH, 0); nty_event_del(reactor-\u0026gt;epfd, ev); if (len \u0026gt; 0) { ev-\u0026gt;length = len; ev-\u0026gt;buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;C[%d]:%s\\n\u0026#34;, fd, ev-\u0026gt;buffer); nty_event_set(ev, fd, send_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLOUT, ev); } else if (len == 0) { close(ev-\u0026gt;fd); printf(\u0026#34;[fd=%d] pos[%ld], closed\\n\u0026#34;, fd, ev-reactor-\u0026gt;events); } else { close(ev-\u0026gt;fd); printf(\u0026#34;recv[fd=%d] error[%d]:%s\\n\u0026#34;, fd, errno, strerror(errno)); } return len; } int send_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; struct ntyevent *ev = reactor-\u0026gt;events+fd; int len = send(fd, ev-\u0026gt;buffer, ev-\u0026gt;length, 0); if (len \u0026gt; 0) { printf(\u0026#34;send[fd=%d], [%d]%s\\n\u0026#34;, fd, len, ev-\u0026gt;buffer); nty_event_del(reactor-\u0026gt;epfd, ev); nty_event_set(ev, fd, recv_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, ev); } else { close(ev-\u0026gt;fd); nty_event_del(reactor-\u0026gt;epfd, ev); printf(\u0026#34;send[fd=%d] error %s\\n\u0026#34;, fd, strerror(errno)); } return len; } int accept_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; if (reactor == NULL) return -1; struct sockaddr_in client_addr; socklen_t len = sizeof(client_addr); int clientfd; if ((clientfd = accept(fd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;len)) == -1) { if (errno != EAGAIN \u0026amp;\u0026amp; errno != EINTR) { } printf(\u0026#34;accept: %s\\n\u0026#34;, strerror(errno)); return -1; } int i = 0; do { #if 0for (i = 0;i \u0026lt; MAX_EPOLL_EVENTS;i ++) { if (reactor-\u0026gt;events[i].status == 0) { break; } } if (i == MAX_EPOLL_EVENTS) { printf(\u0026#34;%s: max connect limit[%d]\\n\u0026#34;, __func__, MAX_EPOLL_EVENTS); break; } #endif \tint flag = 0; if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) \u0026lt; 0) { printf(\u0026#34;%s: fcntl nonblocking failed, %d\\n\u0026#34;, __func__, MAX_EPOLL_EVENTS); break; } nty_event_set(\u0026amp;reactor-\u0026gt;events[clientfd], clientfd, recv_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, \u0026amp;reactor-\u0026gt;events[clientfd]); } while (0); printf(\u0026#34;new connect [%s:%d][time:%ld], pos[%d]\\n\u0026#34;, inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), reactor-\u0026gt;events[i].last_active, i); return 0; } int init_sock(short port) { int fd = socket(AF_INET, SOCK_STREAM, 0); fcntl(fd, F_SETFL, O_NONBLOCK); struct sockaddr_in server_addr; memset(\u0026amp;server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = htonl(INADDR_ANY); server_addr.sin_port = htons(port); bind(fd, (struct sockaddr*)\u0026amp;server_addr, sizeof(server_addr)); if (listen(fd, 20) \u0026lt; 0) { printf(\u0026#34;listen failed : %s\\n\u0026#34;, strerror(errno)); } printf(\u0026#34;listen port : %d\\n\u0026#34;, port); return fd; } int ntyreactor_init(struct ntyreactor *reactor) { if (reactor == NULL) return -1; memset(reactor, 0, sizeof(struct ntyreactor)); reactor-\u0026gt;epfd = epoll_create(1); if (reactor-\u0026gt;epfd \u0026lt;= 0) { printf(\u0026#34;create epfd in %s err %s\\n\u0026#34;, __func__, strerror(errno)); return -2; } reactor-\u0026gt;events = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent)); if (reactor-\u0026gt;events == NULL) { printf(\u0026#34;create epfd in %s err %s\\n\u0026#34;, __func__, strerror(errno)); close(reactor-\u0026gt;epfd); return -3; } } int ntyreactor_destory(struct ntyreactor *reactor) { close(reactor-\u0026gt;epfd); free(reactor-\u0026gt;events); } int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) { if (reactor == NULL) return -1; if (reactor-\u0026gt;events == NULL) return -1; nty_event_set(\u0026amp;reactor-\u0026gt;events[sockfd], sockfd, acceptor, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, \u0026amp;reactor-\u0026gt;events[sockfd]); return 0; } int ntyreactor_run(struct ntyreactor *reactor) { if (reactor == NULL) return -1; if (reactor-\u0026gt;epfd \u0026lt; 0) return -1; if (reactor-\u0026gt;events == NULL) return -1; struct epoll_event events[MAX_EPOLL_ITEM]; int checkpos = 0, i; while (1) { #if 0long now = time(NULL); for (i = 0;i \u0026lt; 100;i ++, checkpos ++) { if (checkpos == MAX_EPOLL_EVENTS) { checkpos = 0; } if (reactor-\u0026gt;events[checkpos].status != 1) { continue; } long duration = now - reactor-\u0026gt;events[checkpos].last_active; if (duration \u0026gt;= 60) { close(reactor-\u0026gt;events[checkpos].fd); printf(\u0026#34;[fd=%d] timeout\\n\u0026#34;, reactor-\u0026gt;events[checkpos].fd); nty_event_del(reactor-\u0026gt;epfd, \u0026amp;reactor-\u0026gt;events[checkpos]); } } #endif  int nready = epoll_wait(reactor-\u0026gt;epfd, events, MAX_EPOLL_ITEM, 1000); if (nready \u0026lt; 0) { printf(\u0026#34;epoll_wait error, exit\\n\u0026#34;); continue; } for (i = 0;i \u0026lt; nready;i ++) { struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr; if ((events[i].events \u0026amp; EPOLLIN) \u0026amp;\u0026amp; (ev-\u0026gt;events \u0026amp; EPOLLIN)) { ev-\u0026gt;callback(ev-\u0026gt;fd, events[i].events, ev-\u0026gt;arg); } if ((events[i].events \u0026amp; EPOLLOUT) \u0026amp;\u0026amp; (ev-\u0026gt;events \u0026amp; EPOLLOUT)) { ev-\u0026gt;callback(ev-\u0026gt;fd, events[i].events, ev-\u0026gt;arg); } } } } int main(int argc, char *argv[]) { unsigned short port = SERVER_PORT; if (argc == 2) { port = atoi(argv[1]); } struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor)); ntyreactor_init(reactor); int listenfd[LISTEN_PORT_COUNT] = {0}; int i = 0; for (i = 0;i \u0026lt; LISTEN_PORT_COUNT;i ++) { listenfd[i] = init_sock(port+i); ntyreactor_addlistener(reactor, listenfd[i], accept_cb); } ntyreactor_run(reactor); ntyreactor_destory(reactor); for (i = 0;i \u0026lt; LISTEN_PORT_COUNT;i ++) { close(listenfd[i]); } return 0; } ","date":"2020-04-03T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/406_huf20403e8966ebf3b89c1d187871e8eda_4587138_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"进阶TCP服务器"},{"content":"设计模式 首先先上所有设计模式的原则，这些原则贯彻于每一种设计模式中，是各种设计模式的根\n原则   依赖倒置原则\n 高层模块不应该依赖低层模块，⼆者都应该依赖抽象。 抽象不应该依赖具体实现，具体实现应该依赖于抽象。    开放封闭原则\n ⼀个类应该对扩展开放，对修改关闭。    面向接口编程\n 不将变量类型声明为某个特定的具体类，而是声明为某个接⼝。 客户程序无需获知对象的具体类型，只需要知道对象所具有的接口。 减少系统中各部分的依赖关系，从而实现“高内聚、松耦合”的类型设计⽅案。    封装变化点\n 将稳定点和变化点分离，扩展修改变化点；让稳定点与变化点的实现层次分离。    单一职责原则\n ⼀个类应该仅有⼀个引起它变化的原因。    里氏替换原则\n 子类型必须能够替换掉它的父类型；主要出现在⼦类覆盖父类实现，原来使用父亲类型的程序可能出现错误；覆盖了父类方法却没实现父类方法的职责；    接口隔离原则\n 不应该强迫客户依赖于他们不用的方法。 ⼀般用于处理⼀个类拥有比较多的接口，而这些接口涉及到很多职责    对象组合优于类继承\n 继承耦合度⾼，组合耦合度低    模板模式 \r\n首先，先提下使用到的原则：\n 依赖倒置原则 单一职责原则 接口隔离原则  定义： 定义⼀个操作中的算法的骨架 ，⽽将⼀些步骤延迟到子类中。 Template Method使得子类可以不 改变⼀个算法的结构即可重定义该算法的某些特定步骤。\n直接上代码，代码分两部分，第一部分为未使用模板模式时的代码，第二部分为使用模板模式的代码：\n未使用模板模式： #include \u0026lt;iostream\u0026gt;using namespace std; class ZooShow { public: void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show0\u0026#34; \u0026lt;\u0026lt; endl; } void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show2\u0026#34; \u0026lt;\u0026lt; endl; } }; class ZooShowEx { public: void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } void Show3(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; int main () { ZooShow *zs = new ZooShow; ZooShowEx *zs1 = new ZooShowEx; // 流程是固定的（稳定点），应该抽象出来；另外子流程是不应该暴露给客户，违反了接口隔离原则；  zs-\u0026gt;Show0(); zs1-\u0026gt;Show1(); zs-\u0026gt;Show2(); zs1-\u0026gt;Show3(); return 0; } 可以看到主函数中流程一览无余，应该由框架提供接口给客户调用一下走完整个流程\n使用模板模式： #include \u0026lt;iostream\u0026gt;using namespace std; class ZooShow { public: // 固定流程封装到这里  void Show() { Show0(); Show1(); Show2(); Show3(); } protected: // 子流程 使用protected保护起来 不被客户调用 但允许子类扩展  virtual void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show0\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show2\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show1() { } virtual void Show3() { } }; class ZooShowEx : public ZooShow { protected: virtual void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show3(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show4() { //  } }; class ZooShowEx1 : public ZooShow { protected: virtual void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; class ZooShowEx2 : public ZooShow { protected: virtual void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; /* 依赖倒置原则 单一职责原则 接口隔离原则 反向控制：应用程序 框架 应用程序（变化的）应该依赖框架（稳定的），应该是框架去调应用程序，而不是应用程序去调框架 */ int main () { ZooShow *zs = new ZooShowEx; ZooShow *zs1 = new ZooShowEx1; ZooShow *zs2 = new ZooShowEx2; zs-\u0026gt;Show(); return 0; } 使用虚函数，根据不同子类做不同扩展，且流程封装在show内部，满足了接口隔离和依赖倒置原则，主类只负责展示流程，流程的具体细节由子类自己实现，职责单一\n观察者模式 \r\n定义： 定义对象间的⼀种⼀对多（变化）的依赖关系，以便当⼀个对象(Subject)的状态发生改变时，所有 依赖于它的对象都得到通知并自动更新。\n未使用观察者模式： class DisplayA { public: void Show(float temperature); }; class DisplayB { public: void Show(float temperature); }; class WeatherData { }; class DataCenter { public: float CalcTemperature() { WeatherData * data = GetWeatherData(); // ...  float temper/* = */; return temper; } private: WeatherData * GetWeatherData(); // 不同的方式 }; int main() { DataCenter *center = new DataCenter; DisplayA *da = new DisplayA; DisplayB *db = new DisplayB; float temper = center-\u0026gt;CalcTemperature(); da-\u0026gt;Show(temper); db-\u0026gt;Show(temper); return 0; } // 终端变化（增加和删除） 数据中心 不应该受终端变化的影响  使用观察者模式： #include \u0026lt;vector\u0026gt; class IDisplay { public: virtual void Show(float temperature) = 0; virtual ~IDisplay() {} }; class DisplayA : public IDisplay { public: virtual void Show(float temperature); }; class DisplayB : public IDisplay{ public: virtual void Show(float temperature); }; class WeatherData { }; class DataCenter { public: void Attach(IDisplay * ob); void Detach(IDisplay * ob); void Notify() { float temper = CalcTemperature(); for (auto iter = obs.begin(); iter != obs.end(); iter++) { (*iter)-\u0026gt;Show(temper); } } private: virtual WeatherData * GetWeatherData(); virtual float CalcTemperature() { WeatherData * data = GetWeatherData(); // ...  float temper/* = */; return temper; } std::vector\u0026lt;IDisplay*\u0026gt; obs; }; int main() { DataCenter *center = new DataCenter; IDisplay *da = new DisplayA(); IDisplay *db = new DisplayB(); center-\u0026gt;Attach(da); center-\u0026gt;Attach(db); center-\u0026gt;Notify(); //-----  center-\u0026gt;Detach(db); center-\u0026gt;Notify(); return 0; } 可以看到我们将接口统一到datacenter（气象中心）里，由气象中心来对外开放功能，同样的原材料（地点）也要交给我们设置的中心来处理，相当于我们买房需要经过房产中介，卖方也需要经过房产中介，我们都基于房产中介的原则来做事，这就是依赖倒置原则。同时，也符合面向接口编程原则。\n责任链模式 \r\n定义： 使多个对象都有机会处理请求，从⽽避免请求的发送者和接收者之间的耦合关系。将这些对象连成⼀条链，并沿着这条链传递请求，直到有⼀个对象处理它为⽌。\n未使用责任链模式： #include \u0026lt;string\u0026gt; class Context { public: std::string name; int day; }; class LeaveRequest { public: // 随着判断的增加，LeaveRequest类变得不稳定  bool HandleRequest(const Context \u0026amp;ctx) { if (ctx.day \u0026lt;= 3) HandleByMainProgram(ctx); else if (ctx.day \u0026lt;= 10) HandleByProjMgr(ctx); else HandleByBoss(ctx); } private: bool HandleByMainProgram(const Context \u0026amp;ctx) { } bool HandleByProjMgr(const Context \u0026amp;ctx) { } bool HandleByBoss(const Context \u0026amp;ctx) { } }; 可以看到我们每次都需要在handlerequest里加一个判断，作为中间的稳定框架不应被用户修改，我们需要找出稳定的部分独立出来。不稳定的部分可以看到是判断条件和对应的处理方式，我们需要将这一部分通过子类重载，使用虚函数即可。\n使用责任链模式： #include \u0026lt;string\u0026gt; class Context { public: std::string name; int day; }; class IHandler { public: virtual ~IHandler() {} void SetNextHandler(IHandler *next) { next = next; } bool Handle(ctx) { if (CanHandle(ctx)) { return HandleRequest(); } else if (GetNextHandler()) { return GetNextHandler()-\u0026gt;HandleRequest(ctx); } else { // err  } } protected: virtual bool HandleRequest(const Context \u0026amp;ctx) = 0; virtual bool CanHandle(const Context \u0026amp;ctx) =0; IHandler * GetNextHandler() { return next; } private: IHandler *next; }; class HandleByMainProgram : public IHandler { protected: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } virtual bool CanHandle() { //  } }; class HandleByProjMgr : public IHandler { protected: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } virtual bool CanHandle() { //  } }; class HandleByBoss : public IHandler { public: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } protected: virtual bool CanHandle() { //  } }; int main () { IHandler * h1 = new MainProgram(); IHandler * h2 = new HandleByProjMgr(); IHandler * h3 = new HandleByBoss(); h1-\u0026gt;SetNextHandler(h2); Context ctx; h1-\u0026gt;handle(ctx); return 0; } CanHandle 是判断条件 HandleRequest 就是对应的处理方式了\n我们只需要重载这两个函数就可以\n责任链顾名思义就是一条链，所以需要在对象中保存链中下一个对象的指针，IHandler *next;\n可以看到：依赖倒置原则、开放封闭原则比较明显\n装饰器模式 \r\n定义： 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成⼀条链，并沿着这条链传递请求，直到有一个对象处理它为止。\n未使用装饰器模式： // 普通员工有销售奖金，累计奖金，部门经理除此之外还有团队奖金；后面可能会添加环比增长奖金，同时可能产生不同的奖金组合； // 销售奖金 = 当月销售额 * 4% // 累计奖金 = 总的回款额 * 0.2% // 部门奖金 = 团队销售额 * 1% // 环比奖金 = (当月销售额-上月销售额) * 1% // 销售后面的参数可能会调整 class Context { public: bool isMgr; // User user;  // double groupsale; }; class Bonus { public: double CalcBonus(Context \u0026amp;ctx) { double bonus = 0.0; bonus += CalcMonthBonus(ctx); bonus += CalcSumBonus(ctx); if (ctx.isMgr) { bonus += CalcGroupBonus(ctx); } return bonus; } private: double CalcMonthBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } double CalcSumBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } double CalcGroupBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } }; int main() { Context ctx; // 设置 ctx  Bonus *bonus = new Bonus; bonus-\u0026gt;CalcBonus(ctx); } 使用装饰器模式： // 普通员工有销售奖金，累计奖金，部门经理除此之外还有团队奖金；后面可能会添加环比增长奖金，同时可能产生不同的奖金组合； // 销售奖金 = 当月销售额 * 4% // 累计奖金 = 总的回款额 * 0.2% // 部门奖金 = 团队销售额 * 1% // 环比奖金 = (当月销售额-上月销售额) * 1% // 销售后面的参数可能会调整 class Context { public: bool isMgr; // User user;  // double groupsale; }; // 试着从职责出发，将职责抽象出来 class CalcBonus { public: CalcBonus(CalcBonus * c = nullptr) {} virtual double Calc(Context \u0026amp;ctx) { return 0.0; // 基本工资  } virtual ~CalcBonus() {} protected: CalcBonus* cc; }; class CalcMonthBonus : public CalcBonus { public: CalcMonthBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double mbonus /*= 计算流程忽略*/; return mbonus + cc-\u0026gt;Calc(ctx); } }; class CalcSumBonus : public CalcBonus { public: CalcSumBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double sbonus /*= 计算流程忽略*/; return sbonus + cc-\u0026gt;Calc(ctx); } }; class CalcGroupBonus : public CalcBonus { public: CalcGroupBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double gbnonus /*= 计算流程忽略*/; return gbnonus + cc-\u0026gt;Calc(ctx); } }; class CalcCycleBonus : public CalcBonus { public: CalcGroupBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double gbnonus /*= 计算流程忽略*/; return gbnonus + cc-\u0026gt;Calc(ctx); } }; int main() { // 1. 普通员工  Context ctx1; CalcBonus *base = new CalcBonus(); CalcBonus *cb1 = new CalcMonthBonus(base); CalcBonus *cb2 = new CalcSumBonus(cb1); cb2-\u0026gt;Calc(ctx1); // 2. 部门经理  Context ctx2; CalcBonus *cb3 = new CalcGroupBonus(cb2); cb3-\u0026gt;Calc(ctx2); } 与责任链模式不同的是，装饰器模式是累加而非替换，所以我们不需要一个完全新的类（重载掉整个处理函数）只需要在新的类中调用旧的类中的方法（那就需要传入旧类的对象）并在此基础上加入自己的新的处理部分，这就是装饰。\n我们把稳定的部分（计算）抽象了出来，在此基础上装饰，当我们需要新的接口的时候只需要在稳定的基础上扩展子类就可以，满足了面向接口、封装变化点等\n单例模式 \r\n定义： 保证⼀个类仅有⼀个实例，并提供⼀个该实例的全局访问点。\n通过多个版本来说明单例模式\n单例模式版本1： 简单的单例模式\n// 内存栈区 // 内存堆区 // 常数区 // 静态区 系统释放 // ⼆进制代码区 class Singleton { public: static Singleton * GetInstance() { if (_instance == nullptr) { _instance = new Singleton(); } return _instance; } private: Singleton(){}//构造  Singleton(const Singleton \u0026amp;clone){} //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化  问题在于我们要销毁的时候没有对应的处理函数\n单例模式版本2： class Singleton { public: static Singleton * GetInstance() { if (_instance == nullptr) { _instance = new Singleton(); atexit(Destructor); } return _instance; } ~Singleton() {} private: static void Destructor() { if (nullptr != _instance) { delete _instance; _instance = nullptr; } } Singleton();//构造  Singleton(const Singleton \u0026amp;cpy); //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化  加入销毁（atexit在函数退出时调用）但是存在线程安全问题，单例模式创建的对象需要全局唯一，多线程下如果同时进入创建函数内可能创建多个对象。\n单例模式版本3： 使用互斥锁保证线程安全\n#include \u0026lt;mutex\u0026gt;class Singleton { // 懒汉模式 lazy load get的时候才创建 public: static Singleton * GetInstance() { //std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); // 3.1 切换线程  if (_instance == nullptr) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); // 3.2  if (_instance == nullptr) { _instance = new Singleton(); atexit(Destructor); } } return _instance; } private: static void Destructor() { if (nullptr != _instance) { delete _instance; _instance = nullptr; } } Singleton(){} //构造  Singleton(const Singleton \u0026amp;cpy){} //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; static std::mutex _mutex; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化 std::mutex Singleton::_mutex; //互斥锁初始化  这一版本还存在问题，首先在3.1处加锁的话锁的粒度太大，线程切换调用的开销巨大，试想即使另一个线程拿到锁已经初始化完成，此时有线程要来取得单例对象，可是锁已经被拿走，即使对象已经创建还是需要将这个线程挂起，开销巨大。\n第二个问题，如果我们将锁加到3.2位置，多线程环境下可能有线程取到空值的单例对象，这是因为有线程已经拿到锁开始创建对象，但是从底层汇编的角度来看，对象的构建分三步：1、取得内存2、构造3、赋值给指针。但是汇编器可能进行优化（reorder，指令重排，为什么会重排呢，因为指令操作重排后可能具有更好的性能，对于指令来说可能提高分支预测的成功率等等），使得对象构建的顺序由123变成132，如果13进行完毕，代表其他线程会认为对象已经构建完成，因为指针指向了一块地址，但其实这一块内存还未进行构造就是未知的，其他线程就会取到未知的数据，产生错误。\n综上，在3.1处加锁粒度太大浪费计算资源，在3.2处加锁还是无法保证线程安全。\n那我们该怎么办呢？ 就有了版本4：\n单例模式版本4： 使用编译屏障\n一般来讲屏障是汇编器带有的功能，我们也可以内联汇编来实现但是比较复杂且可读性不高。\n汇编：（定义一个显式屏障）\n#define barrier() __asm__ __volatile__(\u0026quot;\u0026quot;: : :\u0026quot;memory\u0026quot;)\r当然显式屏障的作用远不止关闭reorder，还有：\n告诉compiler内存中的值已经改变，之前对内存的缓存（缓存到寄存器）都需要抛弃，barrier()之后的内存操作需要重新从内存load，而不能使用之前寄存器缓存的值。并且可以防止compiler优化barrier()前后的内存访问顺序。\n可以看到，memory还具备刷新缓存的功能，所以不仅可以防止reorder优化，还可以防止其他指令优化，比如真值替换等。至于多核心的L1,L2缓存，MESI协议下的多核缓存一致性已经保持的很好了\nc++11 中的编译屏障：\n#include \u0026lt;mutex\u0026gt;#include \u0026lt;atomic\u0026gt;class Singleton { public: static Singleton * GetInstance() { Singleton* tmp = _instance.load(std::memory_order_relaxed); std::atomic_thread_fence(std::memory_order_acquire);//获取内存屏障  if (tmp == nullptr) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); tmp = _instance.load(std::memory_order_relaxed); if (tmp == nullptr) { tmp = new Singleton; std::atomic_thread_fence(std::memory_order_release);//释放内存屏障  _instance.store(tmp, std::memory_order_relaxed); atexit(Destructor); } } return tmp; } private: static void Destructor() { Singleton* tmp = _instance.load(std::memory_order_relaxed); if (nullptr != tmp) { delete tmp; } } Singleton(){} Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static std::atomic\u0026lt;Singleton*\u0026gt; _instance; static std::mutex _mutex; }; std::atomic\u0026lt;Singleton*\u0026gt; Singleton::_instance;//静态成员需要初始化 std::mutex Singleton::_mutex; //互斥锁初始化 // g++ Singleton.cpp -o singleton -std=c++11  添加内存屏障后，cpu不会reorder，因为这块内存只能在这里操作，优化器并不会进行优化，只有一个线程可以操作这一块内存，也就不存在其他线程来取用的时候取到未知值的情况了，因为其他线程都无法取用，只能等待。同时这种操作下粒度也是很小的，并不影响大部分情况下的其他线程获得未创建成功的信息。\n单例模式版本5： // c++11 magic static 特性：如果当变量在初始化的时候，并发同时进⼊声明语句，并发 线程将会阻塞等待初始化结束。 class Singleton { public: ~Singleton(){} static Singleton\u0026amp; GetInstance() { static Singleton instance; return instance; } private: Singleton(){} Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} }; // 继承 Singleton // g++ Singleton.cpp -o singleton -std=c++11 /*该版本具备 版本5 所有优点： 1. 利⽤静态局部变量特性，延迟加载； 2. 利⽤静态局部变量特性，系统⾃动回收内存，⾃动调⽤析构函数； 3. 静态局部变量初始化时，没有 new 操作带来的cpu指令reorder操作； 4. c++11 静态局部变量初始化时，具备线程安全； */ 这一版本优点很多，但是当其他类需要用到这个单例的时候，我们想到继承，可是构造函数为private，显然无法继承，因为继承后不能使用构造函数。\n这里我们只需要在单例中添加友元类就可以。 可是扩展性极差，因为可能需要写特别多的friend，所以用版本6：\n单例模式版本6： template\u0026lt;typename T\u0026gt; class Singleton { public: static T\u0026amp; GetInstance() { static T instance; // 这⾥要初始化DesignPattern，需要调⽤DesignPattern 构造函数，同时会调⽤⽗类的构造函数。  return instance; } protected: virtual ~Singleton() {} Singleton() {} // protected修饰构造函数，才能让别⼈继承  Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator =(const Singleton\u0026amp;) {} }; class DesignPattern : public Singleton\u0026lt;DesignPattern\u0026gt; { friend class Singleton\u0026lt;DesignPattern\u0026gt;; // friend 能让 Singleton\u0026lt;T\u0026gt; 访问到 DesignPattern构造函数 private: DesignPattern(){} DesignPattern(const DesignPattern\u0026amp;) {} DesignPattern\u0026amp; operator=(const DesignPattern\u0026amp;) {} }; 显然，使用protected来标记构造函数就可以避免继承问题了。\n工厂方法模式 \r\n定义： 定义⼀个用于创建对象的接口，让子类决定实例化哪⼀个类。Factory Method使得⼀个类的实例化延迟到子类。\n未使用工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; // =====1 int main() { std::string choose/* = */; if (choose == \u0026#34;txt\u0026#34;) { IExport *e = new ExportTxt(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } else if (choose == \u0026#34;json\u0026#34;) { IExport *e = new ExportJson(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } else if (choose == \u0026#34;xml\u0026#34;) { IExport *e = new ExportXml(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } } 使用工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv  class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class IExportFactory { public: virtual IExport * NewExport(/* ... */) = 0; }; class ExportXmlFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportXml; // 可能之后有什么操作  return temp; } }; class ExportJsonFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportJson; // 可能之后有什么操作  return temp; } }; class ExportTxtFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportTxt; // 可能之后有什么操作  return temp; } }; class ExportData { public: ExportData(IExportFactory *factory) : _factory(factory) {} ~ExportData() { if (_factory) { delete _factory; _factory = nullptr; } } bool Export(const std::string \u0026amp;data) { IExport * e = _factory-\u0026gt;NewExport(); e-\u0026gt;Export(data); } private: IExportFactory *_factory; }; int main() { ExportData ed(new ExportTxtFactory); ed.Export(\u0026#34;hello world\u0026#34;); return 0; } 抽象工厂 \r\n定义： 提供一个接口，让该接口负责创建一系列“相关或者相互依赖的对象”，无需指定它们具体的类。\n使用抽象工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportCSV : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class IImport { public: virtual bool Import(const std::string \u0026amp;data) = 0; virtual ~IImport(){} }; class ImportXml : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportJson : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportTxt : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportCSV : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { // ....  return true; } }; class IDataApiFactory { public: IDataApiFactory() { _export = nullptr; _import = nullptr; } virtual ~IDataApiFactory() { if (_export) { delete _export; _export = nullptr; } if (_import) { delete _import; _import = nullptr; } } bool Export(const std::string \u0026amp;data) { if (_export == nullptr) { _export = NewExport(); } return _export-\u0026gt;Export(data); } bool Import(const std::string \u0026amp;data) { if (_import == nullptr) { _import = NewImport(); } return _import-\u0026gt;Import(data); } protected: virtual IExport * NewExport(/* ... */) = 0; virtual IImport * NewImport(/* ... */) = 0; private: IExport *_export; IImport *_import; }; class XmlApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportXml; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportXml; // 可能之后有什么操作  return temp; } }; class JsonApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportJson; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportJson; // 可能之后有什么操作  return temp; } }; class TxtApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportTxt; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportTxt; // 可能之后有什么操作  return temp; } }; class CSVApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportCSV; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportCSV; // 可能之后有什么操作  return temp; } }; int main () { IDataApiFactory *factory = new CSVApiFactory(); factory-\u0026gt;Import(\u0026#34;hello world\u0026#34;); factory-\u0026gt;Export(\u0026#34;hello world\u0026#34;); return 0; } 适配器模式 \r\n定义： 将⼀个类的接口转换成客户希望的另一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以⼀起工作。\n使用适配器模式： #include \u0026lt;string\u0026gt;#include \u0026lt;vector\u0026gt;using namespace std; class LogSys { public: LogSys() {} void WriteLog(const vector\u0026lt;string\u0026gt; \u0026amp;) { // ... 日志id 时间戳 服务器id 具体日志内容 roleid  } vector\u0026lt;string\u0026gt;\u0026amp; ReadLog() { // ...  vector\u0026lt;string\u0026gt; data /* = ...*/; return data; } }; class DB; // 面向接口编程 而不是具体类 强依赖 耦合性高 mysql mongo  class LogSysEx : public LogSys { public: LogSysEx(DB *db) : _db(db) {} void AddLog(const vector\u0026lt;string\u0026gt; \u0026amp;data) { LogSys::WriteLog(data); /* 这里调用 _db 的方法将 data 数据存储到数据库 */ } void DelLog(const int logid) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); // 从 vector\u0026lt;string\u0026gt; 中删除 logid的日志  LogSys::WriteLog(data); // 调用 _db 的方法将 logid的日志删除  } void UpdateLog(const int logid, const string \u0026amp;udt) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); // 从 vector\u0026lt;string\u0026gt; 中更新 logid的日志 udt  LogSys::WriteLog(data); // 调用 _db 的方法将 logid的日志更改  } string\u0026amp; LocateLog(const int logid) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); string log1 /* = from log file*/; string log2 /* = from db */; string temp = log1 + \u0026#34;;\u0026#34; + log2; return temp; } private: DB* _db; }; 代理模式 \r\n定义： 为其他对象提供一种代理以控制对这对象的访问。\n使用代理模式： class ISubject { public: virtual void Handle() = 0; virtual ~ISubject() {} }; // 该类在当前进程，也可能在其他进程当中 class RealSubject : public ISubject { public: virtual void Handle() { // 只完成功能相关的操作，不做其他模块的判断  } }; // 在当前进程当中 只会在某个模块中使用 class Proxy1 : public ISubject { public: Proxy1(ISubject *subject) : _subject(subject) {} virtual void Handle() { // 在访问 RealSubject 之前做一些处理  //if (不满足条件)  // return;  _subject-\u0026gt;Handle(); count++; // 在访问 RealSubject 之后做一些处理  } private: ISubject* _subject; static int count; }; int Proxy1::count = 0; // 在分布式系统当中 skynet actor class Proxy2 : public ISubject { public: virtual void Handle() { // 在访问 RealSubject 之前做一些处理  // 发送到数据到远端 网络处理 同步非阻塞 ntyco c协程  //IResult * val = rpc-\u0026gt;call(\u0026#34;RealSubject\u0026#34;, \u0026#34;Handle\u0026#34;);  // 在访问 RealSubject 之后做一些处理  } private: /*void callback(IResult * val) { // 在访问 RealSubject 之后做一些处理 }*/ }; 策略模式 \r\n定义： 定义一系列算法，把它们一个个封装起来，并且使它们可互相替换。该模式使得算法可独立于使用它的客户程序而变化。\n未使用策略模式： enum VacationEnum { VAC_Spring, VAC_QiXi, VAC_Wuyi, VAC_GuoQing, //VAC_ShengDan, }; // 稳定的 变化的 class Promotion { VacationEnum vac; public: double CalcPromotion(){ if (vac == VAC_Spring){ // 春节  } else if (vac == VAC_QiXi){ // 七夕  } else if (vac == VAC_Wuyi){ // 五一  } else if (vac == VAC_GuoQing){ // 国庆 \t} } }; 使用策略模式： class Context { }; class ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx) = 0; virtual ~ProStategy(); }; // cpp class VAC_Spring : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_QiXi : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; class VAC_QiXi1 : public VAC_QiXi { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_Wuyi : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_GuoQing : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; class VAC_Shengdan : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // 稳定的 变化的 class Promotion { public: Promotion(ProStategy *sss) : s(sss){} ~Promotion(){} double CalcPromotion(const Context \u0026amp;ctx){ return s-\u0026gt;CalcPro(ctx); } private: ProStategy *s; }; int main () { Context ctx; ProStategy *s = new VAC_QiXi1(); Promotion *p = new Promotion(s); p-\u0026gt;CalcPromotion(ctx); return 0; } ","date":"2020-03-24T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/462_hu609bcc78771f538873d0b8560ebecb73_10199040_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":"连接池实现 mysql连接池 头文件： #ifndef DBPOOL_H_ #define DBPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;list\u0026gt;#include \u0026lt;mutex\u0026gt;#include \u0026lt;condition_variable\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;stdint.h\u0026gt; #include \u0026lt;mysql.h\u0026gt; #define MAX_ESCAPE_STRING_LEN\t10240  using namespace std; // 返回结果 select的时候用 class CResultSet { public: CResultSet(MYSQL_RES* res); virtual ~CResultSet(); bool Next(); int GetInt(const char* key); char* GetString(const char* key); private: int _GetIndex(const char* key); MYSQL_RES* m_res; MYSQL_ROW\tm_row; map\u0026lt;string, int\u0026gt;\tm_key_map; }; // 插入数据用 class CPrepareStatement { public: CPrepareStatement(); virtual ~CPrepareStatement(); bool Init(MYSQL* mysql, string\u0026amp; sql); void SetParam(uint32_t index, int\u0026amp; value); void SetParam(uint32_t index, uint32_t\u0026amp; value); void SetParam(uint32_t index, string\u0026amp; value); void SetParam(uint32_t index, const string\u0026amp; value); bool ExecuteUpdate(); uint32_t GetInsertId(); private: MYSQL_STMT*\tm_stmt; MYSQL_BIND*\tm_param_bind; uint32_t\tm_param_cnt; }; class CDBPool; class CDBConn { public: CDBConn(CDBPool* pDBPool); virtual ~CDBConn(); int Init(); // 创建表 \tbool ExecuteCreate(const char* sql_query); // 删除表 \tbool ExecuteDrop(const char* sql_query); // 查询 \tCResultSet* ExecuteQuery(const char* sql_query); /** * 执行DB更新，修改 * * @param sql_query sql * @param care_affected_rows 是否在意影响的行数，false:不在意；true:在意 * * @return 成功返回true 失败返回false */ bool ExecuteUpdate(const char* sql_query, bool care_affected_rows = true); uint32_t GetInsertId(); // 开启事务 \tbool StartTransaction(); // 提交事务 \tbool Commit(); // 回滚事务 \tbool Rollback(); // 获取连接池名 \tconst char* GetPoolName(); MYSQL* GetMysql() { return m_mysql; } private: CDBPool* m_pDBPool;\t// to get MySQL server information \tMYSQL* m_mysql;\t// 对应一个连接 \tchar\tm_escape_string[MAX_ESCAPE_STRING_LEN + 1]; }; class CDBPool {\t// 只是负责管理连接CDBConn，真正干活的是CDBConn public: CDBPool() {} CDBPool(const char* pool_name, const char* db_server_ip, uint16_t db_server_port, const char* username, const char* password, const char* db_name, int max_conn_cnt); virtual ~CDBPool(); int Init();\t// 连接数据库，创建连接 \tCDBConn* GetDBConn(const int timeout_ms = -1);\t// 获取连接资源 \tvoid RelDBConn(CDBConn* pConn);\t// 归还连接资源  const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetDBServerIP() { return m_db_server_ip.c_str(); } uint16_t GetDBServerPort() { return m_db_server_port; } const char* GetUsername() { return m_username.c_str(); } const char* GetPasswrod() { return m_password.c_str(); } const char* GetDBName() { return m_db_name.c_str(); } private: string m_pool_name;\t// 连接池名称 \tstring m_db_server_ip;\t// 数据库ip \tuint16_t\tm_db_server_port; // 数据库端口 \tstring m_username; // 用户名 \tstring m_password;\t// 用户密码 \tstring m_db_name;\t// db名称 \tint\tm_db_cur_conn_cnt;\t// 当前启用的连接数量 \tint m_db_max_conn_cnt;\t// 最大连接数量 \tlist\u0026lt;CDBConn*\u0026gt;\tm_free_list;\t// 空闲的连接  list\u0026lt;CDBConn*\u0026gt;\tm_used_list;\t// 记录已经被请求的连接 \tstd::mutex m_mutex; std::condition_variable m_cond_var; bool m_abort_request = false; // CThreadNotify\tm_free_notify;\t// 信号量 }; #endif /* DBPOOL_H_ */ }; 实现：  #include \u0026#34;DBPool.h\u0026#34;#include \u0026lt;string.h\u0026gt; #define log_error printf #define log_warn printf #define log_info printf #define MIN_DB_CONN_CNT 2 #define MAX_DB_CONN_FAIL_NUM 10  CResultSet::CResultSet(MYSQL_RES *res) { m_res = res; // map table field key to index in the result array \tint num_fields = mysql_num_fields(m_res); MYSQL_FIELD *fields = mysql_fetch_fields(m_res); for (int i = 0; i \u0026lt; num_fields; i++) { // 多行 \tm_key_map.insert(make_pair(fields[i].name, i)); } } CResultSet::~CResultSet() { if (m_res) { mysql_free_result(m_res); m_res = NULL; } } bool CResultSet::Next() { m_row = mysql_fetch_row(m_res); if (m_row) { return true; } else { return false; } } int CResultSet::_GetIndex(const char *key) { map\u0026lt;string, int\u0026gt;::iterator it = m_key_map.find(key); if (it == m_key_map.end()) { return -1; } else { return it-\u0026gt;second; } } int CResultSet::GetInt(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return 0; } else { return atoi(m_row[idx]); // 有索引 \t} } char *CResultSet::GetString(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return NULL; } else { return m_row[idx];\t// 列 \t} } ///////////////////////////////////////// CPrepareStatement::CPrepareStatement() { m_stmt = NULL; m_param_bind = NULL; m_param_cnt = 0; } CPrepareStatement::~CPrepareStatement() { if (m_stmt) { mysql_stmt_close(m_stmt); m_stmt = NULL; } if (m_param_bind) { delete[] m_param_bind; m_param_bind = NULL; } } bool CPrepareStatement::Init(MYSQL *mysql, string \u0026amp;sql) { mysql_ping(mysql);\t// 当mysql连接丢失的时候，使用mysql_ping能够自动重连数据库  //g_master_conn_fail_num ++; \tm_stmt = mysql_stmt_init(mysql); if (!m_stmt) { log_error(\u0026#34;mysql_stmt_init failed\\n\u0026#34;); return false; } if (mysql_stmt_prepare(m_stmt, sql.c_str(), sql.size())) { log_error(\u0026#34;mysql_stmt_prepare failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } m_param_cnt = mysql_stmt_param_count(m_stmt); if (m_param_cnt \u0026gt; 0) { m_param_bind = new MYSQL_BIND[m_param_cnt]; if (!m_param_bind) { log_error(\u0026#34;new failed\\n\u0026#34;); return false; } memset(m_param_bind, 0, sizeof(MYSQL_BIND) * m_param_cnt); } return true; } void CPrepareStatement::SetParam(uint32_t index, int \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, uint32_t \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } void CPrepareStatement::SetParam(uint32_t index, const string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } bool CPrepareStatement::ExecuteUpdate() { if (!m_stmt) { log_error(\u0026#34;no m_stmt\\n\u0026#34;); return false; } if (mysql_stmt_bind_param(m_stmt, m_param_bind)) { log_error(\u0026#34;mysql_stmt_bind_param failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_execute(m_stmt)) { log_error(\u0026#34;mysql_stmt_execute failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_affected_rows(m_stmt) == 0) { log_error(\u0026#34;ExecuteUpdate have no effect\\n\u0026#34;); return false; } return true; } uint32_t CPrepareStatement::GetInsertId() { return mysql_stmt_insert_id(m_stmt); } ///////////////////// CDBConn::CDBConn(CDBPool *pPool) { m_pDBPool = pPool; m_mysql = NULL; } CDBConn::~CDBConn() { if (m_mysql) { mysql_close(m_mysql); } } int CDBConn::Init() { m_mysql = mysql_init(NULL);\t// mysql_标准的mysql c client对应的api \tif (!m_mysql) { log_error(\u0026#34;mysql_init failed\\n\u0026#34;); return 1; } my_bool reconnect = true; mysql_options(m_mysql, MYSQL_OPT_RECONNECT, \u0026amp;reconnect);\t// 配合mysql_ping实现自动重连 \tmysql_options(m_mysql, MYSQL_SET_CHARSET_NAME, \u0026#34;utf8mb4\u0026#34;);\t// utf8mb4和utf8区别  // ip 端口 用户名 密码 数据库名 \tif (!mysql_real_connect(m_mysql, m_pDBPool-\u0026gt;GetDBServerIP(), m_pDBPool-\u0026gt;GetUsername(), m_pDBPool-\u0026gt;GetPasswrod(), m_pDBPool-\u0026gt;GetDBName(), m_pDBPool-\u0026gt;GetDBServerPort(), NULL, 0)) { log_error(\u0026#34;mysql_real_connect failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return 2; } return 0; } const char *CDBConn::GetPoolName() { return m_pDBPool-\u0026gt;GetPoolName(); } bool CDBConn::ExecuteCreate(const char *sql_query) { mysql_ping(m_mysql); // mysql_real_query 实际就是执行了SQL \tif (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::ExecuteDrop(const char *sql_query) { mysql_ping(m_mysql);\t// 如果端开了，能够自动重连  if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } CResultSet *CDBConn::ExecuteQuery(const char *sql_query) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); return NULL; } // 返回结果 \tMYSQL_RES *res = mysql_store_result(m_mysql);\t// 返回结果 \tif (!res) { log_error(\u0026#34;mysql_store_result failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return NULL; } CResultSet *result_set = new CResultSet(res);\t// 存储到CResultSet \treturn result_set; } /* 1.执行成功，则返回受影响的行的数目，如果最近一次查询失败的话，函数返回 -1 2.对于delete,将返回实际删除的行数. 3.对于update,如果更新的列值原值和新值一样,如update tables set col1=10 where id=1; id=1该条记录原值就是10的话,则返回0。 mysql_affected_rows返回的是实际更新的行数,而不是匹配到的行数。 */ bool CDBConn::ExecuteUpdate(const char *sql_query, bool care_affected_rows) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); //g_master_conn_fail_num ++; \treturn false; } if (mysql_affected_rows(m_mysql) \u0026gt; 0) { return true; } else { // 影响的行数为0时 \tif (care_affected_rows) { // 如果在意影响的行数时, 返回false, 否则返回true \tlog_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\\n\u0026#34;, mysql_error(m_mysql), sql_query); return false; } else { log_warn(\u0026#34;affected_rows=0, sql: %s\\n\\n\u0026#34;, sql_query); return true; } } } bool CDBConn::StartTransaction() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;start transaction\\n\u0026#34;, 17)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Rollback() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;rollback\\n\u0026#34;, 8)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: rollback\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Commit() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;commit\\n\u0026#34;, 6)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: commit\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } uint32_t CDBConn::GetInsertId() { return (uint32_t)mysql_insert_id(m_mysql); } //////////////// CDBPool::CDBPool(const char *pool_name, const char *db_server_ip, uint16_t db_server_port, const char *username, const char *password, const char *db_name, int max_conn_cnt) { m_pool_name = pool_name; m_db_server_ip = db_server_ip; m_db_server_port = db_server_port; m_username = username; m_password = password; m_db_name = db_name; m_db_max_conn_cnt = max_conn_cnt;\t// \tm_db_cur_conn_cnt = MIN_DB_CONN_CNT; // 最小连接数量 } // 释放连接池 CDBPool::~CDBPool() { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); m_abort_request = true; m_cond_var.notify_all();\t// 通知所有在等待的  for (list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CDBConn *pConn = *it; delete pConn; } m_free_list.clear(); } int CDBPool::Init() { // 创建固定最小的连接数量 \tfor (int i = 0; i \u0026lt; m_db_cur_conn_cnt; i++) { CDBConn *pDBConn = new CDBConn(this); int ret = pDBConn-\u0026gt;Init(); if (ret) { delete pDBConn; return ret; } m_free_list.push_back(pDBConn); } // log_error(\u0026#34;db pool: %s, size: %d\\n\u0026#34;, m_pool_name.c_str(), (int)m_free_list.size()); \treturn 0; } /* *TODO: 增加保护机制，把分配的连接加入另一个队列，这样获取连接时，如果没有空闲连接， *TODO: 检查已经分配的连接多久没有返回，如果超过一定时间，则自动收回连接，放在用户忘了调用释放连接的接口 * timeout_ms默认为-1死等 * timeout_ms \u0026gt;=0 则为等待的时间 */ int wait_cout = 0; CDBConn *CDBPool::GetDBConn(const int timeout_ms) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(m_mutex); if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } if (m_free_list.empty())\t// 当没有连接可以用时 \t{ // 第一步先检测 当前连接数量是否达到最大的连接数量 \tif (m_db_cur_conn_cnt \u0026gt;= m_db_max_conn_cnt) { // 如果已经到达了，看看是否需要超时等待 \tif(timeout_ms \u0026lt; 0)\t// 死等，直到有连接可以用 或者 连接池要退出 \t{ log_info(\u0026#34;wait ms:%d\\n\u0026#34;, timeout_ms); m_cond_var.wait(lock, [this] { // log_info(\u0026#34;wait:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \t// 当前连接数量小于最大连接数量 或者请求释放连接池时退出 \treturn (!m_free_list.empty()) | m_abort_request; }); } else { // return如果返回 false，继续wait(或者超时), 如果返回true退出wait \t// 1.m_free_list不为空 \t// 2.超时退出 \t// 3. m_abort_request被置为true，要释放整个连接池 \tm_cond_var.wait_for(lock, std::chrono::milliseconds(timeout_ms), [this] { // log_info(\u0026#34;wait_for:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \treturn (!m_free_list.empty()) | m_abort_request; }); // 带超时功能时还要判断是否为空 \tif(m_free_list.empty()) // 如果连接池还是没有空闲则退出 \t{ return NULL; } } if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } } else // 还没有到最大连接则创建连接 \t{ CDBConn *pDBConn = new CDBConn(this);\t//新建连接 \tint ret = pDBConn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init DBConnecton failed\\n\\n\u0026#34;); delete pDBConn; return NULL; } else { m_free_list.push_back(pDBConn); m_db_cur_conn_cnt++; log_info(\u0026#34;new db connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_db_cur_conn_cnt); } } } CDBConn *pConn = m_free_list.front();\t// 获取连接 \tm_free_list.pop_front();\t// STL 吐出连接，从空闲队列删除 \t// pConn-\u0026gt;setCurrentTime(); // 伪代码 \tm_used_list.push_back(pConn);\t//  return pConn; } void CDBPool::RelDBConn(CDBConn *pConn) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++)\t// 避免重复归还 \t{ if (*it == pConn)\t{ break; } } if (it == m_free_list.end()) { m_used_list.remove(pConn); m_free_list.push_back(pConn); m_cond_var.notify_one();\t// 通知取队列 \t} else { log_error(\u0026#34;RelDBConn failed\\n\u0026#34;); } } // 遍历检测是否超时未归还 // pConn-\u0026gt;isTimeout(); // 当前时间 - 被请求的时间 // 强制回收 从m_used_list 放回 m_free_list redis连接池 =============\n头文件  /* * @Author: your name * @Date: 2019-12-07 10:54:57 * @LastEditTime : 2020-01-10 16:35:13 * @LastEditors : Please set LastEditors * @Description: In User Settings Edit * @FilePath: \\src\\cache_pool\\CachePool.h */ #ifndef CACHEPOOL_H_ #define CACHEPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;list\u0026gt; #include \u0026#34;Thread.h\u0026#34; #include \u0026#34;hiredis.h\u0026#34; using std::string; using std::list; using std::map; using std::vector; class CachePool; class CacheConn { public: CacheConn(const char* server_ip, int server_port, int db_index, const char* password, const char *pool_name =\u0026#34;\u0026#34;); CacheConn(CachePool* pCachePool);\tvirtual ~CacheConn(); int Init(); void DeInit(); const char* GetPoolName(); // 通用操作  // 判断一个key是否存在  bool isExists(string \u0026amp;key); // 删除某个key  long del(string \u0026amp;key); // ------------------- 字符串相关 ------------------- \tstring get(string key); string set(string key, string\u0026amp; value); string setex(string key, int timeout, string value); // string mset(string key, map);  //批量获取  bool mget(const vector\u0026lt;string\u0026gt;\u0026amp; keys, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); //原子加减1  long incr(string key); long decr(string key); // ---------------- 哈希相关 ------------------------ \tlong hdel(string key, string field); string hget(string key, string field); bool hgetAll(string key, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); long hset(string key, string field, string value); long hincrBy(string key, string field, long value); long incrBy(string key, long value); string hmset(string key, map\u0026lt;string, string\u0026gt;\u0026amp; hash); bool hmget(string key, list\u0026lt;string\u0026gt;\u0026amp; fields, list\u0026lt;string\u0026gt;\u0026amp; ret_value); // ------------ 链表相关 ------------ \tlong lpush(string key, string value); long rpush(string key, string value); long llen(string key); bool lrange(string key, long start, long end, list\u0026lt;string\u0026gt;\u0026amp; ret_value); bool flushdb(); private: CachePool* m_pCachePool; redisContext* m_pContext; uint64_t\tm_last_connect_time; uint16_t m_server_port; string m_server_ip; string m_password; uint16_t m_db_index; string m_pool_name; }; class CachePool { public: // db_index和mysql不同的地方 \tCachePool(const char* pool_name, const char* server_ip, int server_port, int db_index, const char *password, int max_conn_cnt); virtual ~CachePool(); int Init(); // 获取空闲的连接资源 \tCacheConn* GetCacheConn(); // Pool回收连接资源 \tvoid RelCacheConn(CacheConn* pCacheConn); const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetServerIP() { return m_server_ip.c_str(); } const char* GetPassword() { return m_password.c_str(); } int GetServerPort() { return m_server_port; } int GetDBIndex() { return m_db_index; } private: string m_pool_name; string\tm_server_ip; string m_password; int\tm_server_port; int\tm_db_index;\t// mysql 数据库名字， redis db index  int\tm_cur_conn_cnt; int m_max_conn_cnt; list\u0026lt;CacheConn*\u0026gt;\tm_free_list; CThreadNotify\tm_free_notify; }; #endif /* CACHEPOOL_H_ */ 实现  #include \u0026#34;CachePool.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;Thread.h\u0026#34; #define log_error printf #define log_info printf  #define MIN_CACHE_CONN_CNT 2 #define MAX_CACHE_CONN_FAIL_NUM 10  CacheConn::CacheConn(const char *server_ip, int server_port, int db_index, const char *password, const char *pool_name) { m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_pool_name = pool_name; m_pContext = NULL; m_last_connect_time = 0; } CacheConn::CacheConn(CachePool *pCachePool) { m_pCachePool = pCachePool; if (pCachePool) { m_server_ip = pCachePool-\u0026gt;GetServerIP(); m_server_port = pCachePool-\u0026gt;GetServerPort(); m_db_index = pCachePool-\u0026gt;GetDBIndex(); m_password = pCachePool-\u0026gt;GetPassword(); m_pool_name = pCachePool-\u0026gt;GetPoolName(); } else { log_error(\u0026#34;pCachePool is NULL\\n\u0026#34;); } m_pContext = NULL; m_last_connect_time = 0; } CacheConn::~CacheConn() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } /* * redis初始化连接和重连操作，类似mysql_ping() */ int CacheConn::Init() { if (m_pContext)\t// 非空，连接是正常的 \t{ return 0; } // 1s 尝试重连一次 \tuint64_t cur_time = (uint64_t)time(NULL); if (cur_time \u0026lt; m_last_connect_time + 1) // 重连尝试 间隔1秒 \t{ printf(\u0026#34;cur_time:%lu, m_last_connect_time:%lu\\n\u0026#34;, cur_time, m_last_connect_time); return 1; } // printf(\u0026#34;m_last_connect_time = cur_time\\n\u0026#34;); \tm_last_connect_time = cur_time; // 1000ms超时 \tstruct timeval timeout = {0, 1000000}; // 建立连接后使用 redisContext 来保存连接状态。 \t// redisContext 在每次操作后会修改其中的 err 和 errstr 字段来表示发生的错误码（大于0）和对应的描述。 \tm_pContext = redisConnectWithTimeout(m_server_ip.c_str(), m_server_port, timeout); if (!m_pContext || m_pContext-\u0026gt;err) { if (m_pContext) { log_error(\u0026#34;redisConnect failed: %s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; } else { log_error(\u0026#34;redisConnect failed\\n\u0026#34;); } return 1; } redisReply *reply; // 验证 \tif (!m_password.empty()) { reply = (redisReply *)redisCommand(m_pContext, \u0026#34;AUTH %s\u0026#34;, m_password.c_str()); if (!reply || reply-\u0026gt;type == REDIS_REPLY_ERROR) { log_error(\u0026#34;Authentication failure:%p\\n\u0026#34;, reply); if (reply) freeReplyObject(reply); return -1; } else { // log_info(\u0026#34;Authentication success\\n\u0026#34;); \t} freeReplyObject(reply); } reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SELECT %d\u0026#34;, 0); if (reply \u0026amp;\u0026amp; (reply-\u0026gt;type == REDIS_REPLY_STATUS) \u0026amp;\u0026amp; (strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0)) { freeReplyObject(reply); return 0; } else { if (reply) log_error(\u0026#34;select cache db failed:%s\\n\u0026#34;, reply-\u0026gt;str); return 2; } } void CacheConn::DeInit() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } const char *CacheConn::GetPoolName() { return m_pool_name.c_str(); } string CacheConn::get(string key) { string value; if (Init()) { return value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;GET %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return value; } string CacheConn::set(string key, string \u0026amp;value) { string ret_value; if (Init()) { return ret_value; } // 返回的结果存放在redisReply \tredisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SET %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); // 释放资源 \treturn ret_value; } string CacheConn::setex(string key, int timeout, string value) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SETEX %s %d %s\u0026#34;, key.c_str(), timeout, value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); return ret_value; } bool CacheConn::mget(const vector\u0026lt;string\u0026gt; \u0026amp;keys, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } if (keys.empty()) { return false; } string strKey; bool bFirst = true; for (vector\u0026lt;string\u0026gt;::const_iterator it = keys.begin(); it != keys.end(); ++it) { if (bFirst) { bFirst = false; strKey = *it; } else { strKey += \u0026#34; \u0026#34; + *it; } } if (strKey.empty()) { return false; } strKey = \u0026#34;MGET \u0026#34; + strKey; redisReply *reply = (redisReply *)redisCommand(m_pContext, strKey.c_str()); if (!reply) { log_info(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; ++i) { redisReply *child_reply = reply-\u0026gt;element[i]; if (child_reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value[keys[i]] = child_reply-\u0026gt;str; } } } freeReplyObject(reply); return true; } bool CacheConn::isExists(string \u0026amp;key) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;EXISTS %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); if (0 == ret_value) { return false; } else { return true; } } long CacheConn::del(string \u0026amp;key) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DEL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hdel(string key, string field) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HDEL %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hget(string key, string field) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGET %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return ret_value; } bool CacheConn::hgetAll(string key, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGETALL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if ((reply-\u0026gt;type == REDIS_REPLY_ARRAY) \u0026amp;\u0026amp; (reply-\u0026gt;elements % 2 == 0)) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i += 2) { redisReply *field_reply = reply-\u0026gt;element[i]; redisReply *value_reply = reply-\u0026gt;element[i + 1]; string field(field_reply-\u0026gt;str, field_reply-\u0026gt;len); string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.insert(make_pair(field, value)); } } freeReplyObject(reply); return true; } long CacheConn::hset(string key, string field, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HSET %s %s %s\u0026#34;, key.c_str(), field.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hincrBy(string key, string field, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HINCRBY %s %s %ld\u0026#34;, key.c_str(), field.c_str(), value); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::incrBy(string key, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCRBY %s %ld\u0026#34;, key.c_str(), value); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hmset(string key, map\u0026lt;string, string\u0026gt; \u0026amp;hash) { string ret_value; if (Init()) { return ret_value; } int argc = hash.size() * 2 + 2; const char **argv = new const char *[argc]; if (!argv) { return ret_value; } argv[0] = \u0026#34;HMSET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (map\u0026lt;string, string\u0026gt;::iterator it = hash.begin(); it != hash.end(); it++) { argv[i++] = it-\u0026gt;first.c_str(); argv[i++] = it-\u0026gt;second.c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); delete[] argv; freeReplyObject(reply); return ret_value; } bool CacheConn::hmget(string key, list\u0026lt;string\u0026gt; \u0026amp;fields, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } int argc = fields.size() + 2; const char **argv = new const char *[argc]; if (!argv) { return false; } argv[0] = \u0026#34;HMGET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (list\u0026lt;string\u0026gt;::iterator it = fields.begin(); it != fields.end(); it++) { argv[i++] = it-\u0026gt;c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, (const char **)argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } delete[] argv; freeReplyObject(reply); return true; } long CacheConn::incr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::decr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DECR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::lpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::rpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;RPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::llen(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LLEN %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } bool CacheConn::lrange(string key, long start, long end, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LRANGE %s %d %d\u0026#34;, key.c_str(), start, end); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } freeReplyObject(reply); return true; } bool CacheConn::flushdb() { bool ret = false; if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;FLUSHDB\u0026#34;); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_STRING \u0026amp;\u0026amp; strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0) { ret = true; } freeReplyObject(reply); return ret; } /////////////// CachePool::CachePool(const char *pool_name, const char *server_ip, int server_port, int db_index, const char *password, int max_conn_cnt) { m_pool_name = pool_name; m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_max_conn_cnt = max_conn_cnt; m_cur_conn_cnt = MIN_CACHE_CONN_CNT; } CachePool::~CachePool() { m_free_notify.Lock(); for (list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CacheConn *pConn = *it; delete pConn; } m_free_list.clear(); m_cur_conn_cnt = 0; m_free_notify.Unlock(); } int CachePool::Init() { for (int i = 0; i \u0026lt; m_cur_conn_cnt; i++) { CacheConn *pConn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); if (pConn-\u0026gt;Init()) { delete pConn; return 1; } m_free_list.push_back(pConn); } log_info(\u0026#34;cache pool: %s, list size: %lu\\n\u0026#34;, m_pool_name.c_str(), m_free_list.size()); return 0; } CacheConn *CachePool::GetCacheConn() { m_free_notify.Lock(); while (m_free_list.empty()) { if (m_cur_conn_cnt \u0026gt;= m_max_conn_cnt) { m_free_notify.Wait(); } else { CacheConn *p_cache_conn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); int ret = p_cache_conn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init CacheConn failed\\n\u0026#34;); delete p_cache_conn; m_free_notify.Unlock(); return NULL; } else { m_free_list.push_back(p_cache_conn); m_cur_conn_cnt++; log_info(\u0026#34;new cache connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_cur_conn_cnt); } } } CacheConn *pConn = m_free_list.front(); m_free_list.pop_front(); m_free_notify.Unlock(); return pConn; } void CachePool::RelCacheConn(CacheConn *p_cache_conn) { m_free_notify.Lock(); list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++) { if (*it == p_cache_conn) { break; } } if (it == m_free_list.end()) { m_free_list.push_back(p_cache_conn); } m_free_notify.Signal(); m_free_notify.Unlock(); } ","date":"2020-03-21T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/245_hubfe96db144675e6281ae6d025e60e504_7615599_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"连接池"},{"content":"内存池实现（注释详细） #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define MP_ALIGNMENT 32 //对齐信息 #define MP_PAGE_SIZE\t4096 //单次分配大块大小 #define MP_MAX_ALLOC_FROM_POOL\t(MP_PAGE_SIZE-1)  #define mp_align(n, alignment) (((n)+(alignment-1)) \u0026amp; ~(alignment-1)) #define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) \u0026amp; ~(alignment-1))  struct mp_large_s { struct mp_large_s *next; void *alloc; }; // 当单次分配超过pagesize时就需要一次分配然后归入large的一个链表中保存  struct mp_node_s { unsigned char *last; unsigned char *end; struct mp_node_s *next; size_t failed; };// 页，用于小块的分配,last指向页内使用到的位置  struct mp_pool_s { size_t max; struct mp_node_s *current; struct mp_large_s *large; struct mp_node_s head[0]; }; //内存池  struct mp_pool_s *mp_create_pool(size_t size); void mp_destory_pool(struct mp_pool_s *pool); void *mp_alloc(struct mp_pool_s *pool, size_t size); void *mp_nalloc(struct mp_pool_s *pool, size_t size); void *mp_calloc(struct mp_pool_s *pool, size_t size); void mp_free(struct mp_pool_s *pool, void *p); //首先需要明确，在分配的时候需要将所有的数据结构都存在我们管理的内存池中 //比如struct mp_pool_s *pool这个内存池本身也需要受我们管理 struct mp_pool_s *mp_create_pool(size_t size) { struct mp_pool_s *p; int ret = posix_memalign((void **)\u0026amp;p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s)); //posix_memalign 分配足够的内存，size（page_size：4096） 加上内存池本身和小块结构本身 \tif (ret) { return NULL; } p-\u0026gt;max = (size \u0026lt; MP_MAX_ALLOC_FROM_POOL) ? size : MP_MAX_ALLOC_FROM_POOL; //内存池单块大小受我们定义的pagesize限制 \tp-\u0026gt;current = p-\u0026gt;head;// 初始化时还有没分配数内存，所以head就是current \tp-\u0026gt;large = NULL;// 还没有分配large块  p-\u0026gt;head-\u0026gt;last = (unsigned char *)p + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s); //最后一个分配的小块 \tp-\u0026gt;head-\u0026gt;end = p-\u0026gt;head-\u0026gt;last + size;//这次分配的页面的末尾  p-\u0026gt;head-\u0026gt;failed = 0; return p; } void mp_destory_pool(struct mp_pool_s *pool) { //释放内存池  struct mp_node_s *h, *n; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { //循环释放分配的large块 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } h = pool-\u0026gt;head-\u0026gt;next; while (h) { //循环释放分配的页面 \tn = h-\u0026gt;next; free(h); h = n; } free(pool);//释放内存池本身  } void mp_reset_pool(struct mp_pool_s *pool) { //重置内存池  struct mp_node_s *h; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) {//重置的时候需要释放大块内存 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } pool-\u0026gt;large = NULL; for (h = pool-\u0026gt;head; h; h = h-\u0026gt;next) {// 但针对页面只需要将页面内的小块内存指针退回起始位置就可以，不需要将已经分配的页面还给操作系统 \th-\u0026gt;last = (unsigned char *)h + sizeof(struct mp_node_s); } } static void *mp_alloc_block(struct mp_pool_s *pool, size_t size) { // 开辟新的页面并分配内存  unsigned char *m; struct mp_node_s *h = pool-\u0026gt;head; //先拿到当前页面的head指针 \tsize_t psize = (size_t)(h-\u0026gt;end - (unsigned char *)h); //拿到页面总大小 \tint ret = posix_memalign((void **)\u0026amp;m, MP_ALIGNMENT, psize);//分配新的页面 \tif (ret) return NULL; struct mp_node_s *p, *new_node, *current; new_node = (struct mp_node_s*)m; //初始化新的页面  new_node-\u0026gt;end = m + psize; new_node-\u0026gt;next = NULL; new_node-\u0026gt;failed = 0; m += sizeof(struct mp_node_s); m = mp_align_ptr(m, MP_ALIGNMENT); new_node-\u0026gt;last = m + size; //把需要的内存大小从新分配的页面上取走  current = pool-\u0026gt;current; //关键点：旨在减少页面末尾的内存碎片，nginx使用的方式  for (p = current; p-\u0026gt;next; p = p-\u0026gt;next) { //每一个页面都有一个自己的failed关键字，用于表明其页面末尾提供给新需求时失败的次数，失败次数大于4就将内存池的current指针换到下一个页面 \t// 失败次数少于4那么就不变内存池的current指针，这样在下一个需求到来时还是从当前分配失败的(页面末尾内存不够用)这一页面末尾开始查找，有利于减少末尾内存碎片，4这个值的得出应该是nginx的实验 \tif (p-\u0026gt;failed++ \u0026gt; 4) { current = p-\u0026gt;next; } } p-\u0026gt;next = new_node; pool-\u0026gt;current = current ? current : new_node; //这里可以看出，如果刚好所有之前创建的页面都失败大于4次，那么将当前内存池首选页面变为刚新建的页面即可  return m; } static void *mp_alloc_large(struct mp_pool_s *pool, size_t size) { //分配大块空间  void *p = malloc(size); if (p == NULL) return NULL; size_t n = 0; struct mp_large_s *large; for (large = pool-\u0026gt;large; large; large = large-\u0026gt;next) { if (large-\u0026gt;alloc == NULL) { large-\u0026gt;alloc = p; return p; } if (n ++ \u0026gt; 3) break; } large = mp_alloc(pool, sizeof(struct mp_large_s));// large这个数据结构本身也需要交由内存池来管理，分析一下在mp_alloc中因为这个数据结构很小会存储在页面上，故不会产生无限循环 \tif (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; //large链表的头插法，很简单  return p; } void *mp_memalign(struct mp_pool_s *pool, size_t size, size_t alignment) { void *p; int ret = posix_memalign(\u0026amp;p, alignment, size); if (ret) { return NULL; } struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s)); if (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; return p; } void *mp_alloc(struct mp_pool_s *pool, size_t size) { //内存分配的入口函数，分别处理大块内存和小块内存需求，小块内存需求在页面末尾空间不足时进入新建页面并分配函数中，否则直接分配在当前页面就可以  unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = mp_align_ptr(p-\u0026gt;last, MP_ALIGNMENT); if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m + size; return m; } p = p-\u0026gt;next; } while (p); //循环在current及其后的一个或多个页面上查找符合要求的末尾空间，存在的话就return  return mp_alloc_block(pool, size); //进到这里说明不存在符合要求空间，那就新建页面然后分配并对页面failed值计数和调整current页面 \t} return mp_alloc_large(pool, size); //大块内存情况 \t} void *mp_nalloc(struct mp_pool_s *pool, size_t size) { unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = p-\u0026gt;last; if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m+size; return m; } p = p-\u0026gt;next; } while (p); return mp_alloc_block(pool, size); } return mp_alloc_large(pool, size); } void *mp_calloc(struct mp_pool_s *pool, size_t size) { void *p = mp_alloc(pool, size); if (p) { memset(p, 0, size); } return p; } void mp_free(struct mp_pool_s *pool, void *p) { struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { if (p == l-\u0026gt;alloc) { free(l-\u0026gt;alloc); l-\u0026gt;alloc = NULL; return ; } } } int main(int argc, char *argv[]) { int size = 1 \u0026lt;\u0026lt; 12; struct mp_pool_s *p = mp_create_pool(size); int i = 0; for (i = 0;i \u0026lt; 10;i ++) { void *mp = mp_alloc(p, 512); //\tmp_free(mp); \t} //printf(\u0026#34;mp_create_pool: %ld\\n\u0026#34;, p-\u0026gt;max); \tprintf(\u0026#34;mp_align(123, 32): %d, mp_align(17, 32): %d\\n\u0026#34;, mp_align(24, 32), mp_align(17, 32)); //printf(\u0026#34;mp_align_ptr(p-\u0026gt;current, 32): %lx, p-\u0026gt;current: %lx, mp_align(p-\u0026gt;large, 32): %lx, p-\u0026gt;large: %lx\\n\u0026#34;, mp_align_ptr(p-\u0026gt;current, 32), p-\u0026gt;current, mp_align_ptr(p-\u0026gt;large, 32), p-\u0026gt;large);  int j = 0; for (i = 0;i \u0026lt; 5;i ++) { char *pp = mp_calloc(p, 32); for (j = 0;j \u0026lt; 32;j ++) { if (pp[j]) { printf(\u0026#34;calloc wrong\\n\u0026#34;); } printf(\u0026#34;calloc success\\n\u0026#34;); } } //printf(\u0026#34;mp_reset_pool\\n\u0026#34;);  for (i = 0;i \u0026lt; 5;i ++) { void *l = mp_alloc(p, 8192); mp_free(p, l); } mp_reset_pool(p); //printf(\u0026#34;mp_destory_pool\\n\u0026#34;); \tfor (i = 0;i \u0026lt; 58;i ++) { mp_alloc(p, 256); } mp_destory_pool(p); return 0; } ","date":"2020-03-19T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/244_hud068d333fa339d76c3f1e66a3f8bf604_8126422_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/","title":"内存池"},{"content":"请求池实现 同步阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  struct dns_header { unsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item { char *domain; char *ip; }; int dns_create_header(struct dns_header *header) { if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) { int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) { int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); return 0; } char *domain[] = { //\t\u0026#34;www.ntytcp.com\u0026#34;, \t\u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; typedef void (*async_result_cb)(struct dns_item *arg, int count); struct async_context { int epfd; pthread_t threadid; }; struct ep_arg { int sockfd; async_result_cb cb; }; #define ASYNC_EVENTS\t128  void *dns_async_callback(void *arg) { struct async_context* ctx = (struct async_context*)arg; while (1) { struct epoll_event events[ASYNC_EVENTS] = {0}; int nready = epoll_wait(ctx-\u0026gt;epfd, events, ASYNC_EVENTS, -1); if (nready \u0026lt; 0) { continue; } int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *ptr = events[i].data.ptr; int sockfd = ptr-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; int count = dns_parse_response(buffer, \u0026amp;domains); ptr-\u0026gt;cb(domains, count); // sockfd \tclose (sockfd); free(ptr); // epollout --\u0026gt; \t//epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_MOD, sockfd, NULL); \t} } } // 1 . context // 2 . return context; // struct async_context* dns_async_client_init(void) { int epfd = epoll_create(1); if (epfd \u0026lt; 0) return NULL; struct async_context* ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) return NULL; ctx-\u0026gt;epfd = epfd; int ret = pthread_create(\u0026amp;ctx-\u0026gt;threadid, NULL, dns_async_callback, ctx); if (ret) { close(epfd); free(ctx); return NULL; } return ctx; } int dns_async_client_destroy(struct async_context* ctx) { close(ctx-\u0026gt;epfd); pthread_cancel(ctx-\u0026gt;threadid); } // int dns_async_client_commit(struct async_context *ctx, async_result_cb cb) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *ptr = calloc(1, sizeof(struct ep_arg)); if (ptr == NULL) return -1; ptr-\u0026gt;sockfd = sockfd; ptr-\u0026gt;cb = cb; // \tstruct epoll_event ev; ev.data.ptr = ptr; ev.events = EPOLLIN; epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); return 0; } int main(int argc, char *argv[]) { int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_client_commit(domain[i]); } getchar(); } 异步非阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  #define ASYNC_CLIENT_NUM\t1024  struct dns_header { //dns 头 \tunsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question {//dns 请求 \tint length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item {//dns 基本信息 \tchar *domain; char *ip; }; typedef void (*async_result_cb)(struct dns_item *list, int count); //请求端针对返回数据的处理函数（以回调函数的方式存在，在recv后调用，通过epoll中的event内的data中ptr传递，这一点与reactor中很像）  struct async_context { //异步请求池中 epoll_wait单独占用一个线程 且epoll也需要在其他线程中出现，这一部分为不同线程共用部分，所以命名为async context也叫上下文 //上下文中一般来讲还需要保存进行epoll_wait的线程id，但这里保存也没啥用 \tint epfd; }; struct ep_arg { //每一个请求（fd）和其处理函数(cb)存在一个sturct中，与reactor 类似 \tint sockfd; async_result_cb cb; }; int dns_create_header(struct dns_header *header) { //dns 创建头  if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { //dns 创建请求  if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) {//dns 创建请求  int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static int set_block(int fd, int block) { //设置fd的阻塞类型 \tint flags = fcntl(fd, F_GETFL, 0); if (flags \u0026lt; 0) return flags; if (block) { flags \u0026amp;= ~O_NONBLOCK; } else { flags |= O_NONBLOCK; } if (fcntl(fd, F_SETFL, flags) \u0026lt; 0) return -1; return 0; } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { //dns 解析域名  int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) {//dns 解析应答  int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { //这个是同步请求中的提交，大概看一下就可以  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); while (1) { char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); if (n \u0026lt;= 0) continue; printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); break; } return 0; } void dns_async_client_free_domains(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { free(list[i].domain); free(list[i].ip); } free(list); } //dns_async_client_proc() //epoll_wait //result callback static void* dns_async_client_proc(void *arg) { //异步请求框架第三环节。 callback函数，这个callback函数不是用于处理接受数据的callback，而是initial中用于接受和处理数据的线程运行的函数，在线程创建时传入 \tstruct async_context *ctx = (struct async_context*)arg; int epfd = ctx-\u0026gt;epfd; while (1) { struct epoll_event events[ASYNC_CLIENT_NUM] = {0}; int nready = epoll_wait(epfd, events, ASYNC_CLIENT_NUM, -1); if (nready \u0026lt; 0) { if (errno == EINTR || errno == EAGAIN) { continue; } else { break; } } else if (nready == 0) { continue; } printf(\u0026#34;nready:%d\\n\u0026#34;, nready); int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *data = (struct ep_arg*)events[i].data.ptr; int sockfd = data-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); struct dns_item *domain_list = NULL; int count = dns_parse_response(buffer, \u0026amp;domain_list); data-\u0026gt;cb(domain_list, count); //call cb \tint ret = epoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL); //printf(\u0026#34;epoll_ctl DEL --\u0026gt; sockfd:%d\\n\u0026#34;, sockfd);  close(sockfd); ///// //异步请求框架第四环节。 关闭  dns_async_client_free_domains(domain_list, count); free(data); } } } //dns_async_client_init() //epoll init //thread init struct async_context *dns_async_client_init(void) { //异步请求框架第一环节。 initial 初始化上下文信息，主要是创建epoll、创建线程（接受数据的线程）  int epfd = epoll_create(1); // \tif (epfd \u0026lt; 0) return NULL; struct async_context *ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) { close(epfd); return NULL; } ctx-\u0026gt;epfd = epfd; pthread_t thread_id; int ret = pthread_create(\u0026amp;thread_id, NULL, dns_async_client_proc, ctx); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); return NULL; } usleep(1); //child go first  return ctx; } //dns_async_client_commit(ctx, domain) //socket init //dns_request //sendto dns send int dns_async_client_commit(struct async_context* ctx, const char *domain, async_result_cb cb) { //异步请求框架第二环节 commit，提交请求。 可以看到传参上下文，获取epoll，将新来的请求提交并加入epoll中在另一个线程中等待返回数据并使用传递的cb解析  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *eparg = (struct ep_arg*)calloc(1, sizeof(struct ep_arg)); //初始化这个新请求后续返回结果对应的处理函数，并通过下面ev.data.ptr = eparg保存并通过另一个线程检测到返回数据后调用 \tif (eparg == NULL) return -1; eparg-\u0026gt;sockfd = sockfd; eparg-\u0026gt;cb = cb; struct epoll_event ev; ev.data.ptr = eparg; ev.events = EPOLLIN; ret = epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); //printf(\u0026#34; epoll_ctl ADD: sockfd-\u0026gt;%d, ret:%d\\n\u0026#34;, sockfd, ret);  return ret; } char *domain[] = { \u0026#34;www.ntytcp.com\u0026#34;, \u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; static void dns_async_client_result_callback(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { printf(\u0026#34;name:%s, ip:%s\\n\u0026#34;, list[i].domain, list[i].ip); } } int main(int argc, char *argv[]) { #if 0dns_client_commit(argv[1]); #else  struct async_context *ctx = dns_async_client_init(); if (ctx == NULL) return -2; int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_async_client_commit(ctx, domain[i], dns_async_client_result_callback); //sleep(2); \t} getchar(); #endif \t} ","date":"2020-03-17T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/247_hu00377075f8d3c09f9b9bff67495c3ec1_5277773_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/","title":"请求池"},{"content":"锁 自旋锁 当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -\u0026gt; 等待的机制被称为自旋锁(spinlock)。\n自旋锁的原理比较简单，如果持有锁的线程能在短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。\n因为自旋锁避免了操作系统进程调度和线程切换，所以自旋锁通常适用在时间比较短的情况下。由于这个原因，操作系统的内核经常使用自旋锁。但是，如果长时间上锁的话，自旋锁会非常耗费性能，它阻止了其他线程的运行和调度。线程持有锁的时间越长，则持有该锁的线程将被 OS(Operating System) 调度程序中断的风险越大。如果发生中断情况，那么其他线程将保持旋转状态(反复尝试获取锁)，而持有该锁的线程并不打算释放锁，这样导致的是结果是无限期推迟，直到持有锁的线程可以完成并释放它为止。\n解决上面这种情况一个很好的方式是给自旋锁设定一个自旋时间，等时间一到立即释放自旋锁。自旋锁的目的是占着CPU资源不进行释放，等到获取锁立即进行处理。但是如何去选择自旋时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用 CPU 资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！\n在计算任务轻的情况下使用自旋锁可以显著提升速度，这是因为线程切换的开销大于等锁的开销，但是计算任务重的话自旋锁的等待时间就成为主要的开销了。\n互斥锁 互斥锁实际是一个互斥量，为获得互斥锁的线程会挂起，这就涉及到线程切换的开销，计算任务重的情况下会比较适合使用。\n读写锁 读写锁即只能由一人写但可以由多人读的锁，适用于读操作很多但写操作很少的情况下。\n原子操作 多线程下使用原子操作确保我们的操作不会被其他线程参与，一般内联汇编 memory 内存屏障，只允许这一缓存写回内存，确保多线程安全。\n多进程下对共享内存的操作使用内联汇编lock，锁住总线，同一时刻只允许一个进程通过总线操作内存。\n粒度大小排序\n互斥锁 \u0026gt; 自旋锁 \u0026gt; 读写锁 \u0026gt; 原子操作\n代码实现 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #define THREAD_SIZE 10  int count = 0; pthread_mutex_t mutex; pthread_spinlock_t spinlock; pthread_rwlock_t rwlock; // MOV dest, src; at\u0026amp;t // MOV src, dest; x86  int inc(int *value, int add) { int old; __asm__ volatile ( //lock 锁住总线 只允许一个cpu操作内存（通过总线）确保多进程安全  \u0026#34;lock; xaddl %2, %1;\u0026#34; // \u0026#34;lock; xchg %2, %1, %3;\u0026#34;  : \u0026#34;=a\u0026#34; (old) : \u0026#34;m\u0026#34; (*value), \u0026#34;a\u0026#34; (add) : \u0026#34;cc\u0026#34;, \u0026#34;memory\u0026#34; //memory 内存屏障，只允许这一缓存写回内存，确保多线程安全  ); return old; } // void *func(void *arg) { int *pcount = (int *)arg; int i = 0; while (i++ \u0026lt; 100000) { #if 0//无锁 (*pcount) ++; #elif 0// 互斥锁版本  pthread_mutex_lock(\u0026amp;mutex); (*pcount) ++; pthread_mutex_unlock(\u0026amp;mutex); #elif 0// 互斥锁非阻塞版本  if (0 != pthread_mutex_trylock(\u0026amp;mutex)) { i --; continue; } (*pcount) ++; pthread_mutex_unlock(\u0026amp;mutex); #elif 0 //自旋锁  pthread_spin_lock(\u0026amp;spinlock); (*pcount) ++; pthread_spin_unlock(\u0026amp;spinlock); #elif 0//读写锁  pthread_rwlock_wrlock(\u0026amp;rwlock); (*pcount) ++; pthread_rwlock_unlock(\u0026amp;rwlock); #else //原子操作  inc(pcount, 1); #endif  usleep(1); } } int main() { #if 0//多线程 pthread_t threadid[THREAD_SIZE] = {0}; pthread_mutex_init(\u0026amp;mutex, NULL); pthread_spin_init(\u0026amp;spinlock, PTHREAD_PROCESS_SHARED); pthread_rwlock_init(\u0026amp;rwlock, NULL); int i = 0; int count = 0; for (i = 0;i \u0026lt; THREAD_SIZE;i ++) { int ret = pthread_create(\u0026amp;threadid[i], NULL, func, \u0026amp;count); if (ret) { break; } } for (i = 0;i \u0026lt; 100;i ++) { pthread_rwlock_rdlock(\u0026amp;rwlock); printf(\u0026#34;count --\u0026gt; %d\\n\u0026#34;, count); pthread_rwlock_unlock(\u0026amp;rwlock); sleep(1); } #else //多进程  int *pcount = mmap(NULL, sizeof(int), PROT_READ | PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0); //将pcount设置为共享内存区域，这样多进程都可以访问  int i = 0; pid_t pid = 0; for (i = 0;i \u0026lt; THREAD_SIZE;i ++) { pid = fork(); if (pid \u0026lt;= 0) { //创建固定数量进程的巧妙操作，子进程被创建后会休眠1微秒后退出这个创建循环开始下面的加法运算，确保只有主进程在创建子进程。  usleep(1); break; } } if (pid \u0026gt; 0) { // 主进程不做加法，只打印信息  for (i = 0;i \u0026lt; 100;i ++) { printf(\u0026#34;count --\u0026gt; %d\\n\u0026#34;, (*pcount)); sleep(1); } } else {//子进程在这里做加法运算  int i = 0; while (i++ \u0026lt; 100000) { #if 0//总线不锁，进程读取到的共享内存更新存在延迟，最终结果小于一百万 (*pcount) ++; #else//锁总线，一次只有一个cpu能操作共享内存，也告诉我们临界资源就是很难发挥出多核性能  inc(pcount, 1); #endif  usleep(1); } } #endif } ","date":"2020-03-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E9%94%81/389_hu19a302d4968ef7ca02b91dd8f18c3f5f_3179286_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E9%94%81/","title":"锁"},{"content":"HTTP 实现http客户端程序\n基础 HTTP使用TCP连接\nHTTP报文：\n\r \r\n实现 域名到ip地址转换(dns) 直接调用api进行转换比较简单：\nchar * host_to_ip(const char* hostname) { struct hostent *host_entry = gethostbyname(hostname); if(host_entry) { return inet_ntoa(*(struct in_addr*)*host_entry -\u0026gt; h_addr_list); } return NULL; } host_entry存储了dns请求的接收，从中取出第一个ip地址并将点分十进制转换为字符串返回\n创建TCP套接字（建立连接） posix api创建\nint http_create_socket(char *ip) { int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in sin = {0}; sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr.s_addr = inet_addr(ip); if(0 != connect(sockfd, (struct sockaddr*)\u0026amp;sin, sizeof(struct sockaddr_in))) { return -1; } fcntl(sockfd, F_SETFL, O_NONBLOCK); return sockfd; } fcntl(sockfd, F_SETFL, O_NONBLOCK);这个函数用于设置该套接字io为非阻塞\n通过套接字向目标网站请求资源（select）\nchar * http_send_request(const char *hostname, const char *resource) { char *ip = host_to_ip(hostname); // \tint sockfd = http_create_socket(ip); char buffer[BUFFER_SIZE] = {0}; sprintf(buffer, \u0026#34;GET %s %s\\r\\n\\ Host: %s\\r\\n\\ %s\\r\\n\\ \\r\\n\u0026#34;, resource, HTTP_VERSION, hostname, CONNECTION_TYPE ); send(sockfd, buffer, strlen(buffer), 0); //select  fd_set fdread; FD_ZERO(\u0026amp;fdread); FD_SET(sockfd, \u0026amp;fdread); struct timeval tv; tv.tv_sec = 5; tv.tv_usec = 0; char *result = malloc(sizeof(int)); memset(result, 0, sizeof(int)); while (1) { int selection = select(sockfd+1, \u0026amp;fdread, NULL, NULL, \u0026amp;tv); if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread)) { break; } else { memset(buffer, 0, BUFFER_SIZE); int len = recv(sockfd, buffer, BUFFER_SIZE, 0); if (len == 0) { // disconnect \tbreak; } result = realloc(result, (strlen(result) + len + 1) * sizeof(char)); strncat(result, buffer, len); } } return result; } select部分： 首先根据套接字初始化fread来监听io，如果有消息到来就置为1，调用select函数： select(sockfd, \u0026amp;rset, \u0026amp;wset, *eset, *tv); \u0026amp;rset位置表示读监听io \u0026amp;wset位置表示写监听io \u0026amp;eset位置表示错误监听io（断开或者其他） tv为轮询间隔时间 select函数内部轮询监听这几个io，有置1就说明有信息需要处理，就返回然后处理信息 断开连接的话返回0，所以if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread))可以有效控制连接断开的break 正常时返回收到的结果result\n附main函数\nint main(char argc, char*argv[]) { if(argc \u0026lt;3) { return -1; } char *response = http_send_request(argv[1], argv[2]); printf(\u0026#34;response: %s\\n\u0026#34;, response); free(response); return 1; } ","date":"2020-03-01T00:00:00Z","image":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/florian-klauer-nptLmg6jqDo-unsplash_hu595aaf3b3dbbb41af5aed8d3958cc9f9_13854_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/","title":"简易http客户端(C posix API)"},{"content":"DNS Domain Name System 域名系统，是一个分布式数据库，用于映射IP与域名\n每级域名长度限制为63，总长度限制为253，使用TCP UDP端口53\nDNS分层 顶级域：com org等 第二级域：baidu google等 第三级域：www edu等\n域名解析 静态映射：在本机上配置域名和ip映射并直接使用\n动态映射：使用DNS域名解析系统，在DNS服务器上配置ip到域名的映射\n域名服务器 根域名服务器： 共a-m十三台（十三个ip）但拥有很多镜像服务器，镜像与本体使用同一个ip，存有顶级域名服务器的ip 顶级域名服务器：管理在该顶级域名服务器下注册的二级域名 权限域名服务器：一个区域的域名解析 本地域名服务器：处理本地的请求，保存本地的映射\n域名解析方式 迭代查询：本机请求本地域名服务器，本地域名服务器开始迭代的查询各个层级服务器，先查询根获得顶级的ip然后根据获得的ip查询顶级在获得区域的ip依次迭代查到请求的映射\n递归查询：递归查询时只发出一次请求然后等待接收到最终结果，在上面的步骤中本机使用的就是递归查询\n协议报文格式 \rdns_dp\r \rdns_dp\r \rdns_dp\r \rdns_dp\r\n具体查看文档\nDNS client UDP编程 首先需要自己定义数据结构用于存储dns报文\nstruct dns_header{ unsigned short id; unsigned short flags; unsigned short questions; unsigned short answer; unsigned short authority; unsigned short additional; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; unsigned char *name; }; 这里只需要question和header是因为我们作为client只实现发送A请求也就是获取域名的ipv4地址，在实现中header的授权码和附加码都不需要使用只需要使用questions id和flags即可\n先建立UDP套接字用于发送UDP报文：\n#define DNS_SERVER_PORT 53 #define DNS_SERVER_IP \u0026#34;114.114.114.114\u0026#34;  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if(sockfd \u0026lt; 0) { return -1; } struct sockaddr_in servaddr = {0}; servaddr.sin_family = AF_INET; servaddr.sin_port = htons(DNS_SERVER_PORT); servaddr.sin_addr.s_addr = inet_addr(DNS_SERVER_IP); 然后使用connect检测开辟本机到DNS服务器的通路：\nint ret = connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); ret返回值可以用于检测通路是否存在\n然后开始初始化要发送的DNS报文：\n1.初始化dns header：\nstruct dns_header header = {0}; dns_create_header(\u0026amp;header); 使用的dns_create_header：\nint dns_create_header(struct dns_header *header) { if(header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); //random  srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags = htons(0x0100); header-\u0026gt;questions = htons(1); return 0; } 这里的id为随机生成 questions为1代表一个问题 flags的0x0100查表后代表：期望递归 传地址来初始化header\n然后就是初始化正文部分的question：\nstruct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); dns_create_question的实现：\nint dns_create_question(struct dns_question* question, const char *hostname) { if(question == NULL || hostname == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;name = (char*)malloc(strlen(hostname) + 2); if(question-\u0026gt;name == NULL) { return -1; } question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); //name  const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname = question-\u0026gt;name; while(token != NULL) { size_t len = strlen(token); *qname = len; qname++; strncpy(qname, token, len+1); // +1 copy \\0  qname += len; token = strtok(NULL, delim); } free(hostname_dup); } 这里正文的初始化使用循环和strtok分别取出每一级域名然后形成正文部分，www.baidu.com \u0026ndash;\u0026gt; 3www5baidu3com0\n接下来就可以将已经初始化好的header和question组装成dns报文request：\nchar request[1024] = {0}; int length = dns_build_request(\u0026amp;header, \u0026amp;question, request, 1024); dns_build_request的实现：\nint dns_build_request(struct dns_header* header, struct dns_question* question, char *request, int rlen ) { if(header == NULL || question == NULL || request == NULL) return -1; memset(request, 0 , rlen); //header --\u0026gt; request  int offset = 0; memcpy(request, header, sizeof(struct dns_header)); offset = sizeof(struct dns_header); //question --\u0026gt; request  memcpy(request+offset, question-\u0026gt;name, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); offset += sizeof(question-\u0026gt;qclass); return offset; } 可以看到就是按照协议进行拼接\n接下来就可以调用unix 网络编程capi来通过udp套接字发送正文：\n//request int slen = sendto(sockfd, request, length, 0, (struct sockaddr*)\u0026amp;servaddr, sizeof(struct sockaddr)); 至此发送就结束了\n我们发送了dns查询报文dns服务器会返回结果，所以可以通过套接字来接收这个结果，并打印出来验证一下：\n//recvfrom() char response[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n =recvfrom(sockfd, response, sizeof(response), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom : %d, %s\\n\u0026#34;, n, response); 也可以写代码解析返回的结果，具体查看代码就可以\n至此就实现了一个dns客户端用于给dns服务器发送查询报文来查询一个域名的ipv4地址\n","date":"2020-02-23T00:00:00Z","image":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/081_hu7f5bc23efa6e31f2e831fa0dcdad6471_5540510_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/","title":"DNS协议解析"},{"content":"Tcp服务器 一请求一线程 首先说明问题： 已请求一线程能承载的请求数量极少，posix标准线程8M，请求数量多时极其占用内存\n简单实现 实现一请求一线程很简单：\n#define BUFFER_SIZE 1024 void *client_routine(void *arg) { int clientfd = *(int *) arg; while(1) { char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); break; } else if(len ==0 ) { close(clientfd); break; } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } while(1) { struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); pthread_t thread_id; pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd); } return 0; } main函数中首先在指定端口处打开一个迎宾套接字sockfd用于对到来的请求分配线程（新建客户端套接字）来处理\nint clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); 就是迎宾套接字不断检测是否有新请求到来，如果到来就返回clientfd 后面新建立的线程将clientfd作为参数传递给线程执行函数即可： pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd);\nEpoll实现Tcp服务端 使用epoll来管理多个io到来的请求然后依次处理这些请求\n首先理清逻辑\n还是通过迎宾套接字来捕获请求，只是迎宾套接字应该首先加入epoll中 外层循环需要对epoll中有输入的套接字（io）遍历，如果有输入（有请求），就处理，这里需要判断套接字的类型：如果是迎宾套接字说明有新的请求，需要通过accpet来创建clientfd并交给epoll管理，如果不是就处理请求，处理完毕后从epoll中删除套接字（io），所以这个逻辑下可以知道，到来的请求是在下一轮中才处理的，并不是一到来就立即处理\nC实现\n#define EPOLL_SIZE 1024  #define BUFFER_SIZE 1024  int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } int epfd = epoll_create(1); struct epoll_event events[EPOLL_SIZE] = {0}; struct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); while(1) { int nready = epoll_wait(epfd, events, EPOLL_SIZE, 5); //-1 events里没东西就不去  if(nready == -1) continue; int i=0; for(i = 0;i \u0026lt;nready;++i) { if (events[i].data.fd == sockfd) { //迎宾的sock那就新产生一个clientfd然后交给epoll  struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); ev.events = EPOLLIN | EPOLLET; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, \u0026amp;ev); } else { int clientfd = events[i].data.fd; char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else if(len ==0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } } return 0; } 关键的函数是epoll_ctl，epoll_create，epoll_wait\nepoll_create的参数是一个int size这个参数在新版本linux内核中无意义，原来也只是告诉内核epoll大致的大小 epoll_ctl配合EPOLL_CTL_xxx的宏来对epoll进行操作（添加删除修改） epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 用来等待事件发生，其他参数比较简单，只说timeout参数：在没有检测到事件发生时最多等待的时间（单位为毫秒）\nET、LT工作模式 水平触发模式：\nev.events = EPOLLIN; 边缘触发模式：\nev.events = EPOLLIN | EPOLLET; 设置好触发模式后可以把event加入epoll中：\nstruct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); ","date":"2020-02-20T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/029_hu5f7dae7e78ac97b0b045f4b1159a4d54_14023957_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"简易Tcp服务器"},{"content":"MYSQL mysql安装与配置 在虚拟机上安装mysql,使用apt-get install就可以 这里我只检索到了mysql-server-5.7就安装了5.7\n在本地win10上安装mysqlbench用于连接虚拟机的mysql服务器 这里使用网络连接，可能是因为mysql版本的原因，本来应该在/etc/mysql中的my.cnf文件中显式的配置有基本信息，我只需要修改部分，但5.7在/etc/mysql/mysql.conf.d/mysqld.cnf,在它的基础上修改对应的bind-address为0.0.0.0保证回环地址可访问：\n# # The MySQL database server configuration file. # # You can copy this to one of: # - \u0026#34;/etc/mysql/my.cnf\u0026#34; to set global options, # - \u0026#34;~/.my.cnf\u0026#34; to set user-specific options. # # One can use all long options that the program supports. # Run program with --help to get a list of available options and with # --print-defaults to see which it would actually understand and use. # # For explanations see # http://dev.mysql.com/doc/mysql/en/server-system-variables.html # This will be passed to all mysql clients # It has been reported that passwords should be enclosed with ticks/quotes # escpecially if they contain \u0026#34;#\u0026#34; chars... # Remember to edit /etc/mysql/debian.cnf when changing the socket location. # Here is entries for some specific programs # The following values assume you have at least 32M ram [mysqld_safe] socket = /var/run/mysqld/mysqld.sock nice = 0 [mysqld] # # * Basic Settings # user = mysql pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock port = 3306 basedir = /usr datadir = /var/lib/mysql tmpdir = /tmp lc-messages-dir = /usr/share/mysql skip-external-locking # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address = 0.0.0.0 # # * Fine Tuning # key_buffer_size = 16M max_allowed_packet = 16M thread_stack = 192K thread_cache_size = 8 # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched myisam-recover-options = BACKUP #max_connections = 100 #table_cache = 64 #thread_concurrency = 10 # # * Query Cache Configuration # query_cache_limit = 1M query_cache_size = 16M # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error log - should be very few entries. # log_error = /var/log/mysql/error.log # # Here you can see queries with especially long duration #log_slow_queries = /var/log/mysql/mysql-slow.log #long_query_time = 2 #log-queries-not-using-indexes # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id = 1 #log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M #binlog_do_db = include_database_name #binlog_ignore_db = include_database_name # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI \u0026#34;tinyca\u0026#34;. # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem 这样保证win10的mysqlbench可以连接到虚拟机的mysql服务器\n但是还需要在mysql中设置对应用户并使之具有外部访问的权限和操作数据库的权限，我这里直接新建gao用户并赋予外部访问权限和操作权限：\nCREATE USER 'gao'@'%' IDENTIFIED BY 'password';\r%号表示可以被任意位置访问也就允许了远程ip访问 然后给gao用户授权，使其能随意操作数据库：\nGRANT all privileges ON * TO 'gao'@'%';\r可以在mysql这个数据库内的user表内找到自己添加的用户信息\nmysql 建表添加数据等操作 建立使用的数据库USR_DB\ncreatedatabasesUSR_DB;使用USR_DB\nuseUSR_DB;建表TBL_USR\ncreatetableTBL_USR(U_IDintprimarykeyauto_increment,U_NAMEchar(10),U_GENGDERchar(10));插入表项\ninsertTBL_USR(U_NAME,U_GENGDER)values(\u0026#39;gao\u0026#39;,\u0026#39;man\u0026#39;);选取表中所有数据显示\nselect*fromTBL_USR;删除与修改表项就涉及到安全模式，mysql默认运行在安全模式所以不能进行修改和删除表项的操作，所以需要取消安全模式然后操作，操作结束后需要再设置回安全模式\nsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;但这样操作是有问题的，如果这三部操作是原子的，是没有问题的，但不是原子的就引入不安全的因素，其他进程可能趁这个时候错误的篡改数据，所以将这三条合成一个过程来确保操作的安全性\nDELIMITER##createprocedureproc_delete_usr(inUNAMEchar(10))beginsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##上面这个DELIMITER ##的意思就是这一段使用##作为限制符号，也就是##框住的区域视为一个整体区域，用于指示过程的区域\n定义了过程之后就可以使用call调用过程达到安全的操作：\ncallproc_delete_usr(\u0026#39;gao\u0026#39;);同样的，修改也可以这样：\nDELIMITER##createprocedureset_img(inUNAMEchar(10),UIMGBLOB)beginsetSQL_SAFE_UPDATES=0;updateTBL_USRsetU_IMG=UIMGwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##callset_img(\u0026#39;gao\u0026#39;,IMG);这里的call set_img里的IMG其实在后面用于c的API调用,绑定statement之后传入的是一个char*的buffer接收的图像数据,然后设置到数据库里\n上面用到了U_IMG的column,这个列在建表时没有建立，需要使用添加column操作：\nALTERTABLETBL_USRcreatecolumnU_IMG;当然也可以使用以下操作删除：\nALTERTABLETBL_USRdropcolumnU_IMG;C api远程调用mysql 编写C程序来做到控制mysql数据库\n安装库： 先安装相关依赖和库才可以调用c api： 直接在虚拟机上\nsudo apt-get install libmysqlclient-dev;\r就安装成功了相关的c开发套件\n使用时需要在程序中包含头文件：\n#include\u0026lt;mysql.h\u0026gt;在编译相关程序时:\ngcc -o xxx xxx.c -I /usr/include/mysql -lmysqlclient 这里基本准备就完成\n基本操作: 首先需要连接mysql数据库，可以想到的就是建立一个mysql的handler，所以很自然的这里就需要一个特殊的struct，库为我们提供了MYSQL的数据类型：\nMYSQL mysql; 这样就建立了mysql这样一个handle，之后的所有操作都基于这个handle进行\n连接操作：\nif(NULL == mysql_init(\u0026amp;mysql)) { printf(\u0026#34;mysql init %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); return -1; } if(!mysql_real_connect(\u0026amp;mysql, king_db_server_ip, king_db_username, king_db_password, king_db_default_db, king_db_server_port, NULL, 0)) { printf(\u0026#34;mysql_real_connect: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } 可读性很高，这里不解释\n然后发送自己预定义好的sql语句：\n#define sql_insert \u0026#34;insert TBL_USR(U_NAME, U_GENGDER) values(\u0026#39;qiuxiang\u0026#39;, \u0026#39;woman\u0026#39;);\u0026#34;  #if 1 if(mysql_real_query(\u0026amp;mysql, sql_insert, strlen(sql_insert))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } #endif 一般情况下不进行其他操作了的话需要关闭mysql连接：\nmysql_close(\u0026amp;mysql); 以上就是简单的基于c api的mysql操作了\n其他操作 select基础\n如果需要从mysql服务器接收数据，比如select一些数据， 那么就需要一个容器来接受数据，这里使用MYSQL_RES来保存mysql的返回的结果:\n同样需要先query：\nif(mysql_real_query(mysql, sql_select, strlen(sql_select))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -1; } 然后接收数据\nMYSQL_RES *res = mysql_store_result(mysql); if(res == NULL) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -2; } 然后处理数据（打印出来） 想打印的话首先需要知道行列数然后再选取需要的数据来打印：\nint rows = mysql_num_rows(res); printf(\u0026#34;rows: %d\\n\u0026#34;, rows); int fields = mysql_num_fields(res); printf(\u0026#34;fields: %d\\n\u0026#34;, fields); 再根据获取的行列数循环fetch数据行然后打印特定行列的数据\nMYSQL_ROW row; while(row = mysql_fetch_row(res)) { int i=0; for(i=0; i\u0026lt;fields;++i) { printf(\u0026#34;%s\\t\u0026#34;, row[i]); } printf(\u0026#34;\\n\u0026#34;); } 这里就可以看到，数据转存到了MYSQL_ROW这个结构中\n最后释放接收的结果\nmysql_free_result(res); statement\n使用statement来存储或发送数据到mysql服务器\n整个流程： 1、初始化stmt，使用MYSQL* handle 2、 准备statement类似于query但是不执行 3、初始化绑定参数MYSQL_BIND param，因为要insert所以要初始化buffer用于指示insert数据 4、将参数绑定到stmt上 5、将buffer中的数据通过statement发送到mysql服务器？（不太清楚是否真的发送了） 6、执行statement 7、执行完毕关闭statement\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_insert_img, strlen(sql_insert_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND param = {0}; param.buffer_type = MYSQL_TYPE_LONG_BLOB; param.buffer = NULL; param.is_null = 0; param.length = NULL; ret = mysql_stmt_bind_param(stmt, \u0026amp;param); if(ret) { printf(\u0026#34;mysql_stmt_bind_param error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_send_long_data(stmt, 0, buffer, length); if(ret) { printf(\u0026#34;mysql_stmt_send_long_data error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } ret = mysql_stmt_close(stmt); if(ret) { printf(\u0026#34;mysql_stmt_close error: %s\\n\u0026#34;, mysql_error(handle)); return -6; } 以下是一个read的statement：\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_select_img, strlen(sql_select_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND result = {0}; result.buffer_type = MYSQL_TYPE_LONG_BLOB; unsigned long total_length = 0; result.length = \u0026amp;total_length; ret = mysql_stmt_bind_result(stmt, \u0026amp;result); if(ret) { printf(\u0026#34;mysql_stmt_bind_result error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_store_result(stmt); if(ret) { printf(\u0026#34;mysql_stmt_store_result error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } while(1) { ret = mysql_stmt_fetch(stmt); if(ret !=0 \u0026amp;\u0026amp; ret != MYSQL_DATA_TRUNCATED) { break; } int start = 0; while(start \u0026lt; (int)total_length) { result.buffer = buffer + start; result.buffer_length = 1; mysql_stmt_fetch_column(stmt, \u0026amp;result, 0, start); start += result.buffer_length; } } mysql_stmt_close(stmt); 可以看到多了fetch操作将result的buffer成员指向外部接受用的buffer的最新的接受位置，mysql_stmt_fetch_column进行了接收工作，MYSQL_BIND result也有了新的定义方式\n","date":"2020-02-12T00:00:00Z","image":"https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/","title":"Mysql基本知识"},{"content":"ubuntu-server虚拟机配置 Sh配置 安装完ubuntu后先配置sh这样可以通过xshell连接 只需要：\nsudo apt-get install openssh-server ssh即可 Samba配置 然后安装samba\nsudo apt-get install samba 创建share文件夹\ncd /home sudo mkdir share sudo chmod 777 share 然后在/etc/samba里配置smb.conf 文件 在文件尾部添加：\n[share] comment = My Samba path = /home/gao/share browseable = yes writeable = yes 然后设置密码\nsudo smbpasswd -a gao 然后去主机上映射盘符就可以方便的访问 在文件框内输入\\192.168.134.xxx 也就是虚拟机ip就可以 把share映射为盘符\ngcc配置 换apt阿里源：\ncd /etc/apt sudo mv source.list source.list.back sudo vim source.list 改为：\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial universe deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties deb http://archive.canonical.com/ubuntu xenial partner deb-src http://archive.canonical.com/ubuntu xenial partner deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 安装gcc：\nsudo apt-get install build-essential ","date":"2020-02-01T00:00:00Z","image":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/","title":"ubuntu-server虚拟机配置"}]
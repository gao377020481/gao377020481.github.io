[{"content":"DNS Domain Name System 域名系统，是一个分布式数据库，用于映射IP与域名\n每级域名长度限制为63，总长度限制为253，使用TCP UDP端口53\nDNS分层 顶级域：com org等 第二级域：baidu google等 第三级域：www edu等\n域名解析 静态映射：在本机上配置域名和ip映射并直接使用\n动态映射：使用DNS域名解析系统，在DNS服务器上配置ip到域名的映射\n域名服务器 根域名服务器： 共a-m十三台（十三个ip）但拥有很多镜像服务器，镜像与本体使用同一个ip，存有顶级域名服务器的ip 顶级域名服务器：管理在该顶级域名服务器下注册的二级域名 权限域名服务器：一个区域的域名解析 本地域名服务器：处理本地的请求，保存本地的映射\n域名解析方式 迭代查询：本机请求本地域名服务器，本地域名服务器开始迭代的查询各个层级服务器，先查询根获得顶级的ip然后根据获得的ip查询顶级在获得区域的ip依次迭代查到请求的映射\n递归查询：递归查询时只发出一次请求然后等待接收到最终结果，在上面的步骤中本机使用的就是递归查询\n协议报文格式 \rdns_dp\r \rdns_dp\r \rdns_dp\r \rdns_dp\r\n具体查看文档\nDNS client UDP编程 首先需要自己定义数据结构用于存储dns报文\nstruct dns_header{ unsigned short id; unsigned short flags; unsigned short questions; unsigned short answer; unsigned short authority; unsigned short additional; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; unsigned char *name; }; 这里只需要question和header是因为我们作为client只实现发送A请求也就是获取域名的ipv4地址，在实现中header的授权码和附加码都不需要使用只需要使用questions id和flags即可\n先建立UDP套接字用于发送UDP报文：\n#define DNS_SERVER_PORT 53 #define DNS_SERVER_IP \u0026#34;114.114.114.114\u0026#34;  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if(sockfd \u0026lt; 0) { return -1; } struct sockaddr_in servaddr = {0}; servaddr.sin_family = AF_INET; servaddr.sin_port = htons(DNS_SERVER_PORT); servaddr.sin_addr.s_addr = inet_addr(DNS_SERVER_IP); 然后使用connect检测开辟本机到DNS服务器的通路：\nint ret = connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); ret返回值可以用于检测通路是否存在\n然后开始初始化要发送的DNS报文：\n1.初始化dns header：\nstruct dns_header header = {0}; dns_create_header(\u0026amp;header); 使用的dns_create_header：\nint dns_create_header(struct dns_header *header) { if(header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); //random  srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags = htons(0x0100); header-\u0026gt;questions = htons(1); return 0; } 这里的id为随机生成 questions为1代表一个问题 flags的0x0100查表后代表：期望递归 传地址来初始化header\n然后就是初始化正文部分的question：\nstruct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); dns_create_question的实现：\nint dns_create_question(struct dns_question* question, const char *hostname) { if(question == NULL || hostname == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;name = (char*)malloc(strlen(hostname) + 2); if(question-\u0026gt;name == NULL) { return -1; } question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); //name  const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname = question-\u0026gt;name; while(token != NULL) { size_t len = strlen(token); *qname = len; qname++; strncpy(qname, token, len+1); // +1 copy \\0  qname += len; token = strtok(NULL, delim); } free(hostname_dup); } 这里正文的初始化使用循环和strtok分别取出每一级域名然后形成正文部分，www.baidu.com \u0026ndash;\u0026gt; 3www5baidu3com0\n接下来就可以将已经初始化好的header和question组装成dns报文request：\nchar request[1024] = {0}; int length = dns_build_request(\u0026amp;header, \u0026amp;question, request, 1024); dns_build_request的实现：\nint dns_build_request(struct dns_header* header, struct dns_question* question, char *request, int rlen ) { if(header == NULL || question == NULL || request == NULL) return -1; memset(request, 0 , rlen); //header --\u0026gt; request  int offset = 0; memcpy(request, header, sizeof(struct dns_header)); offset = sizeof(struct dns_header); //question --\u0026gt; request  memcpy(request+offset, question-\u0026gt;name, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); offset += sizeof(question-\u0026gt;qclass); return offset; } 可以看到就是按照协议进行拼接\n接下来就可以调用unix 网络编程capi来通过udp套接字发送正文：\n//request int slen = sendto(sockfd, request, length, 0, (struct sockaddr*)\u0026amp;servaddr, sizeof(struct sockaddr)); 至此发送就结束了\n我们发送了dns查询报文dns服务器会返回结果，所以可以通过套接字来接收这个结果，并打印出来验证一下：\n//recvfrom() char response[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n =recvfrom(sockfd, response, sizeof(response), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom : %d, %s\\n\u0026#34;, n, response); 也可以写代码解析返回的结果，具体查看代码就可以\n至此就实现了一个dns客户端用于给dns服务器发送查询报文来查询一个域名的ipv4地址\n","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/081_hu7f5bc23efa6e31f2e831fa0dcdad6471_5540510_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/","title":"DNS协议解析"},{"content":"ubuntu-server虚拟机配置 Sh配置 安装完ubuntu后先配置sh这样可以通过xshell连接 只需要：\nsudo apt-get install openssh-server ssh即可 Samba配置 然后安装samba\nsudo apt-get install samba 创建share文件夹\ncd /home sudo mkdir share sudo chmod 777 share 然后在/etc/samba里配置smb.conf 文件 在文件尾部添加：\n[share] comment = My Samba path = /home/gao/share browseable = yes writeable = yes 然后设置密码\nsudo smbpasswd -a gao 然后去主机上映射盘符就可以方便的访问 在文件框内输入\\192.168.134.xxx 也就是虚拟机ip就可以 把share映射为盘符\ngcc配置 换apt阿里源：\ncd /etc/apt sudo mv source.list source.list.back sudo vim source.list 改为：\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial universe deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties deb http://archive.canonical.com/ubuntu xenial partner deb-src http://archive.canonical.com/ubuntu xenial partner deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 安装gcc：\nsudo apt-get install build-essential ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/","title":"ubuntu-server虚拟机配置"},{"content":"内存池实现（注释详细） #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define MP_ALIGNMENT 32 //对齐信息 #define MP_PAGE_SIZE\t4096 //单次分配大块大小 #define MP_MAX_ALLOC_FROM_POOL\t(MP_PAGE_SIZE-1)  #define mp_align(n, alignment) (((n)+(alignment-1)) \u0026amp; ~(alignment-1)) #define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) \u0026amp; ~(alignment-1))  struct mp_large_s { struct mp_large_s *next; void *alloc; }; // 当单次分配超过pagesize时就需要一次分配然后归入large的一个链表中保存  struct mp_node_s { unsigned char *last; unsigned char *end; struct mp_node_s *next; size_t failed; };// 页内的小区域，用于小块的分配  struct mp_pool_s { size_t max; struct mp_node_s *current; struct mp_large_s *large; struct mp_node_s head[0]; }; //内存池  struct mp_pool_s *mp_create_pool(size_t size); void mp_destory_pool(struct mp_pool_s *pool); void *mp_alloc(struct mp_pool_s *pool, size_t size); void *mp_nalloc(struct mp_pool_s *pool, size_t size); void *mp_calloc(struct mp_pool_s *pool, size_t size); void mp_free(struct mp_pool_s *pool, void *p); //首先需要明确，在分配的时候需要将所有的数据结构都存在我们管理的内存池中 //比如struct mp_pool_s *pool这个内存池本身也需要受我们管理 struct mp_pool_s *mp_create_pool(size_t size) { struct mp_pool_s *p; int ret = posix_memalign((void **)\u0026amp;p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s)); //posix_memalign 分配足够的内存，size（page_size：4096） 加上内存池本身和小块结构本身 \tif (ret) { return NULL; } p-\u0026gt;max = (size \u0026lt; MP_MAX_ALLOC_FROM_POOL) ? size : MP_MAX_ALLOC_FROM_POOL; //内存池单块大小受我们定义的pagesize限制 \tp-\u0026gt;current = p-\u0026gt;head;// 初始化时head无值且struct中head在最末尾，c++的struct内存布局决定了current就是head \tp-\u0026gt;large = NULL;// 还没有分配large块  p-\u0026gt;head-\u0026gt;last = (unsigned char *)p + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s); //最后一个分配的小块 \tp-\u0026gt;head-\u0026gt;end = p-\u0026gt;head-\u0026gt;last + size;//这次分配的页面的末尾  p-\u0026gt;head-\u0026gt;failed = 0; return p; } void mp_destory_pool(struct mp_pool_s *pool) { //释放内存池  struct mp_node_s *h, *n; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { //循环释放分配的large块 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } h = pool-\u0026gt;head-\u0026gt;next; while (h) { //循环释放分配的页面 \tn = h-\u0026gt;next; free(h); h = n; } free(pool);//释放内存池本身  } void mp_reset_pool(struct mp_pool_s *pool) { //重置内存池  struct mp_node_s *h; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) {//重置的时候需要释放大块内存 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } pool-\u0026gt;large = NULL; for (h = pool-\u0026gt;head; h; h = h-\u0026gt;next) {// 但针对页面只需要将页面内的小块内存指针退回起始位置就可以，不需要将已经分配的页面还给操作系统 \th-\u0026gt;last = (unsigned char *)h + sizeof(struct mp_node_s); } } static void *mp_alloc_block(struct mp_pool_s *pool, size_t size) { // 开辟新的页面并分配内存  unsigned char *m; struct mp_node_s *h = pool-\u0026gt;head; //先拿到当前页面的head指针 \tsize_t psize = (size_t)(h-\u0026gt;end - (unsigned char *)h); //拿到页面总大小 \tint ret = posix_memalign((void **)\u0026amp;m, MP_ALIGNMENT, psize);//分配新的页面 \tif (ret) return NULL; struct mp_node_s *p, *new_node, *current; new_node = (struct mp_node_s*)m; //初始化新的页面  new_node-\u0026gt;end = m + psize; new_node-\u0026gt;next = NULL; new_node-\u0026gt;failed = 0; m += sizeof(struct mp_node_s); m = mp_align_ptr(m, MP_ALIGNMENT); new_node-\u0026gt;last = m + size; //把需要的内存大小从新分配的页面上取走  current = pool-\u0026gt;current; //关键点：旨在减少页面末尾的内存碎片，nginx使用的方式  for (p = current; p-\u0026gt;next; p = p-\u0026gt;next) { //每一个页面都有一个自己的failed关键字，用于表明其页面末尾提供给新需求时失败的次数，失败次数大于4就将内存池的current指针换到下一个页面 \t// 失败次数少于4那么就不变内存池的current指针，这样在下一个需求到来时还是从当前分配失败的(页面末尾内存不够用)这一页面末尾开始查找，有利于减少末尾内存碎片，4这个值的得出应该是nginx的实验 \tif (p-\u0026gt;failed++ \u0026gt; 4) { current = p-\u0026gt;next; } } p-\u0026gt;next = new_node; pool-\u0026gt;current = current ? current : new_node; //这里可以看出，如果刚好所有之前创建的页面都失败大于4次，那么将当前内存池首选页面变为刚新建的页面即可  return m; } static void *mp_alloc_large(struct mp_pool_s *pool, size_t size) { //分配大块空间  void *p = malloc(size); if (p == NULL) return NULL; size_t n = 0; struct mp_large_s *large; for (large = pool-\u0026gt;large; large; large = large-\u0026gt;next) { if (large-\u0026gt;alloc == NULL) { large-\u0026gt;alloc = p; return p; } if (n ++ \u0026gt; 3) break; } large = mp_alloc(pool, sizeof(struct mp_large_s));// large这个数据结构本身也需要交由内存池来管理，分析一下在mp_alloc中因为这个数据结构很小会存储在页面上，故不会产生无限循环 \tif (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; //large链表的头插法，很简单  return p; } void *mp_memalign(struct mp_pool_s *pool, size_t size, size_t alignment) { void *p; int ret = posix_memalign(\u0026amp;p, alignment, size); if (ret) { return NULL; } struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s)); if (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; return p; } void *mp_alloc(struct mp_pool_s *pool, size_t size) { //内存分配的入口函数，分别处理大块内存和小块内存需求，小块内存需求在页面末尾空间不足时进入新建页面并分配函数中，否则直接分配在当前页面就可以  unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = mp_align_ptr(p-\u0026gt;last, MP_ALIGNMENT); if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m + size; return m; } p = p-\u0026gt;next; } while (p); //循环在current及其后的一个或多个页面上查找符合要求的末尾空间，存在的话就return  return mp_alloc_block(pool, size); //进到这里说明不存在符合要求空间，那就新建页面然后分配并对页面failed值计数和调整current页面 \t} return mp_alloc_large(pool, size); //大块内存情况 \t} void *mp_nalloc(struct mp_pool_s *pool, size_t size) { unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = p-\u0026gt;last; if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m+size; return m; } p = p-\u0026gt;next; } while (p); return mp_alloc_block(pool, size); } return mp_alloc_large(pool, size); } void *mp_calloc(struct mp_pool_s *pool, size_t size) { void *p = mp_alloc(pool, size); if (p) { memset(p, 0, size); } return p; } void mp_free(struct mp_pool_s *pool, void *p) { struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { if (p == l-\u0026gt;alloc) { free(l-\u0026gt;alloc); l-\u0026gt;alloc = NULL; return ; } } } int main(int argc, char *argv[]) { int size = 1 \u0026lt;\u0026lt; 12; struct mp_pool_s *p = mp_create_pool(size); int i = 0; for (i = 0;i \u0026lt; 10;i ++) { void *mp = mp_alloc(p, 512); //\tmp_free(mp); \t} //printf(\u0026#34;mp_create_pool: %ld\\n\u0026#34;, p-\u0026gt;max); \tprintf(\u0026#34;mp_align(123, 32): %d, mp_align(17, 32): %d\\n\u0026#34;, mp_align(24, 32), mp_align(17, 32)); //printf(\u0026#34;mp_align_ptr(p-\u0026gt;current, 32): %lx, p-\u0026gt;current: %lx, mp_align(p-\u0026gt;large, 32): %lx, p-\u0026gt;large: %lx\\n\u0026#34;, mp_align_ptr(p-\u0026gt;current, 32), p-\u0026gt;current, mp_align_ptr(p-\u0026gt;large, 32), p-\u0026gt;large);  int j = 0; for (i = 0;i \u0026lt; 5;i ++) { char *pp = mp_calloc(p, 32); for (j = 0;j \u0026lt; 32;j ++) { if (pp[j]) { printf(\u0026#34;calloc wrong\\n\u0026#34;); } printf(\u0026#34;calloc success\\n\u0026#34;); } } //printf(\u0026#34;mp_reset_pool\\n\u0026#34;);  for (i = 0;i \u0026lt; 5;i ++) { void *l = mp_alloc(p, 8192); mp_free(p, l); } mp_reset_pool(p); //printf(\u0026#34;mp_destory_pool\\n\u0026#34;); \tfor (i = 0;i \u0026lt; 58;i ++) { mp_alloc(p, 256); } mp_destory_pool(p); return 0; } ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/244_hud068d333fa339d76c3f1e66a3f8bf604_8126422_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/","title":"内存池"},{"content":"定时器 定时器作用很多，常见于心跳检测，冷却等等 实现时区别主要在于定时器队列的管控（排序）\n基本方案： 红黑树： 使用红黑树对定时器排序，排序依据为定时器过期时间，每隔单位时间检查红黑树中最小时间是否小于等于当前时间，如果小于等于就删除节点并触发节点的callback。时间复杂度增删O(logn)，Nginx使用红黑树。删除添加操作自旋。\n最小堆： 最小堆根节点最小，直接拿出根节点与当前时间比较即可，删除操作将根节点与末尾节点对换并删除末尾节点然后将新的根节点下沉，添加时加入末尾节点并上升。\n时间轮：  时间轮可以分为单层级与多层级。简单的单层级时间轮使用初始化好的链表数组来存储对应的事件节点链表，时间数据结构中一般包含引用计数，该数据结构只有在引用计数置零后销毁，一般也代表着事件对应的资源可以释放。单层时间轮的大小至少需要大于最长定时时间/单位时间，举例：每5秒发送一个心跳包，连接收到心跳包时需要开启一个10秒的定时器并将事件引用计数加一（事件数据结构插入链表数组中10秒后的链表中），也就是最长定时10秒，10秒后检查该连接对应的事件并将引用计数减一，如果减一后为0就说明连接超时，释放所有资源，关闭事件。在该例子中，初始化的链表数组大小至少为11，因为假如在第0秒来一个心跳包，我们就需要在第10号位置将该连接对应的事件节点加入事件链表中，如果小于11，比如为8，那从0开始往后10个的位置就是在2号位置，那2秒后就得触发了，这与我们设置的10秒定时时间不一致。\n\r\n代码实现： 红黑树： 红黑树数据结构直接使用nginx自带的rbtree头文件，就不自己写了\n红黑树定时器头文件： #ifndef _MARK_RBT_ #define _MARK_RBT_  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdint.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stddef.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026#34;rbtree.h\u0026#34; ngx_rbtree_t timer; static ngx_rbtree_node_t sentinel; typedef struct timer_entry_s timer_entry_t; typedef void (*timer_handler_pt)(timer_entry_t *ev); struct timer_entry_s { ngx_rbtree_node_t timer; timer_handler_pt handler; }; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int init_timer() { ngx_rbtree_init(\u0026amp;timer, \u0026amp;sentinel, ngx_rbtree_insert_timer_value); return 0; } void add_timer(timer_entry_t *te, uint32_t msec) { msec += current_time(); printf(\u0026#34;add_timer expire at msec = %u\\n\u0026#34;, msec); te-\u0026gt;timer.key = msec; ngx_rbtree_insert(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void del_timer(timer_entry_t *te) { ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void expire_timer() { timer_entry_t *te; ngx_rbtree_node_t *sentinel, *root, *node; sentinel = timer.sentinel; uint32_t now = current_time(); for (;;) { root = timer.root; if (root == sentinel) break; node = ngx_rbtree_min(root, sentinel); if (node-\u0026gt;key \u0026gt; now) break; printf(\u0026#34;touch timer expire time=%u, now = %u\\n\u0026#34;, node-\u0026gt;key, now); te = (timer_entry_t *) ((char *) node - offsetof(timer_entry_t, timer)); te-\u0026gt;handler(te); ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); free(te); } } #endif  expire_timer() 检查红黑树中是否有过期定时器，清理所有过期定时器并触发对应的回调函数\nadd_timer 向红黑树中插入事件，使用nginx定时器专用的红黑树插入函数，这里红黑树使用黑色哨兵sentinel，其含义如下：\n红黑树有一个性质是：\r叶结点的左右两边必须为黑色。也就是本来叶结点如果没有左右孩子直接初始化为NULL就是了，但它居然要黑色，意味着我需要新分配两块内存空间啥数据也不保存，tm就是为了给它涂上黑色然后挂到叶结点的left、right。\r当叶结点多起来的时候你说多浪费内存空间？理想的二叉树结构是为了让我们保存数据（key），而不是为了保存颜色吧？\r所以哨兵这个外援就来了，我们申请一块内存命名为哨兵，然后把这块内存涂上黑色，之后所有没有孩子的叶结点left、right都指向这个已涂上黑色的哨兵。以上是红黑树哨兵的作用。\r其他函数较简单\n红黑树定时器主文件： #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;rbt-timer.h\u0026#34; void print_hello(timer_entry_t *te) { printf(\u0026#34;hello world time = %u\\n\u0026#34;, te-\u0026gt;timer.key); } int main() { init_timer(); timer_entry_t *te = malloc(sizeof(timer_entry_t)); memset(te, 0, sizeof(timer_entry_t)); te-\u0026gt;handler = print_hello; add_timer(te, 3000); for (;;) { expire_timer(); usleep(10000); } return 0; } 主函数中主事件循环使用单位10秒usleep(10000),usleep中参数单位为ms\n最小堆： 最小堆定时器头文件： #pragma once  #include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;using namespace std; typedef void (*TimerHandler) (struct TimerNode *node); struct TimerNode { int idx = 0; int id = 0; unsigned int expire = 0; TimerHandler cb = NULL; }; class MinHeapTimer { public: MinHeapTimer() { _heap.clear(); _map.clear(); } static inline int Count() { return ++_count; } int AddTimer(uint32_t expire, TimerHandler cb); bool DelTimer(int id); void ExpireTimer(); private: inline bool _lessThan(int lhs, int rhs) { return _heap[lhs]-\u0026gt;expire \u0026lt; _heap[rhs]-\u0026gt;expire; } bool _shiftDown(int pos); void _shiftUp(int pos); void _delNode(TimerNode *node); private: vector\u0026lt;TimerNode*\u0026gt; _heap; map\u0026lt;int, TimerNode*\u0026gt; _map; static int _count; }; int MinHeapTimer::_count = 0; 需要分析的是struct TimerNode这个数据结构，idx指示在最小堆数组中的位置，用于在定时器过期时确认位置，id用于与_map协作以快速定位并删除特定id的节点，id的值自增长，expire为定时长度，cd是回调函数,具体可以看代码。\n最小堆定时器主文件： #include \u0026lt;unistd.h\u0026gt;#if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026lt;iostream\u0026gt; #include \u0026#34;minheap.h\u0026#34; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int MinHeapTimer::AddTimer(uint32_t expire, TimerHandler cb) { int64_t timeout = current_time() + expire; TimerNode* node = new TimerNode; int id = Count(); node-\u0026gt;id = id; node-\u0026gt;expire = timeout; node-\u0026gt;cb = cb; node-\u0026gt;idx = (int)_heap.size(); _heap.push_back(node); _shiftUp((int)_heap.size() - 1); _map.insert(make_pair(id, node)); return id; } bool MinHeapTimer::DelTimer(int id) { auto iter = _map.find(id); if (iter == _map.end()) return false; _delNode(iter-\u0026gt;second); return true; } void MinHeapTimer::_delNode(TimerNode *node) { int last = (int)_heap.size() - 1; int idx = node-\u0026gt;idx; if (idx != last) { std::swap(_heap[idx], _heap[last]); _heap[idx]-\u0026gt;idx = idx; if (!_shiftDown(idx)) { _shiftUp(idx); } } _heap.pop_back(); _map.erase(node-\u0026gt;id); delete node; } void MinHeapTimer::ExpireTimer() { if (_heap.empty()) return; uint32_t now = current_time(); do { TimerNode* node = _heap.front(); if (now \u0026lt; node-\u0026gt;expire) break; for (int i = 0; i \u0026lt; _heap.size(); i++) std::cout \u0026lt;\u0026lt; \u0026#34;touch idx: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34; id: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;id \u0026lt;\u0026lt; \u0026#34; expire: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;expire \u0026lt;\u0026lt; std::endl; if (node-\u0026gt;cb) { node-\u0026gt;cb(node); } _delNode(node); } while(!_heap.empty()); } bool MinHeapTimer::_shiftDown(int pos){ int last = (int)_heap.size()-1; int idx = pos; for (;;) { int left = 2 * idx + 1; if ((left \u0026gt;= last) || (left \u0026lt; 0)) { break; } int min = left; // left child  int right = left + 1; if (right \u0026lt; last \u0026amp;\u0026amp; !_lessThan(left, right)) { min = right; // right child  } if (!_lessThan(min, idx)) { break; } std::swap(_heap[idx], _heap[min]); _heap[idx]-\u0026gt;idx = idx; _heap[min]-\u0026gt;idx = min; idx = min; } return idx \u0026gt; pos; } void MinHeapTimer::_shiftUp(int pos) { for (;;) { int parent = (pos - 1) / 2; // parent node  if (parent == pos || !_lessThan(pos, parent)) { break; } std::swap(_heap[parent], _heap[pos]); _heap[parent]-\u0026gt;idx = parent; _heap[pos]-\u0026gt;idx = pos; pos = parent; } } void print_hello(TimerNode *te) { std::cout \u0026lt;\u0026lt; \u0026#34;hello world time = \u0026#34; \u0026lt;\u0026lt; te-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34;\\t\u0026#34; \u0026lt;\u0026lt; te-\u0026gt;id \u0026lt;\u0026lt; std::endl; } int main() { MinHeapTimer mht; mht.AddTimer(0, print_hello); mht.AddTimer(1000, print_hello); mht.AddTimer(7000, print_hello); mht.AddTimer(2000, print_hello); mht.AddTimer(9000, print_hello); mht.AddTimer(10000, print_hello); mht.AddTimer(6000, print_hello); mht.AddTimer(3000, print_hello); for (;;) { mht.ExpireTimer(); usleep(10000); } return 0; } 思路与红黑树的大致相同，不做赘述\n单层时间轮： 与最小堆不同，我们这里需要使用静态数组，宏定义数组大小即可，最长定时时间10秒，单位时间1秒，所以使用大于10的数组大小。\n#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdint.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #define MAX_TIMER ((1\u0026lt;\u0026lt;17)-1) #define MAX_CONN ((1\u0026lt;\u0026lt;16)-1)  typedef struct conn_node { //也可以考虑添加callback  uint8_t used; int id; } conn_node_t; typedef struct timer_node { //定时器节点 \tstruct timer_node *next; struct conn_node *node; uint32_t idx; } timer_node_t; static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储 static conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储  static uint32_t t_iter = 0; //创建过的定时器节点数目 static uint32_t c_iter = 0;//创建过的连接节点数目  timer_node_t * get_timer_node() { // 注意：没有检测定时任务数超过 MAX_TIMER 的情况！！！  t_iter++; while (timer_nodes[t_iter \u0026amp; MAX_TIMER].idx \u0026gt; 0) { t_iter++; //这个定时器节点正在被使用，换下一个试试  } timer_nodes[t_iter].idx = t_iter; return \u0026amp;timer_nodes[t_iter]; } conn_node_t * get_conn_node() { // 注意：没有检测连接数超过 MAX_CONN 的情况  c_iter++; while (conn_nodes[c_iter \u0026amp; MAX_CONN].used \u0026gt; 0) { c_iter++;//这个连接节点正在被使用，换下一个试试  } return \u0026amp;conn_nodes[c_iter]; } #define TW_SIZE 16 #define EXPIRE 10 #define TW_MASK (TW_SIZE - 1) static uint32_t tick = 0; typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; void add_conn(link_list_t *tw, conn_node_t *cnode, int delay) { link_list_t *list = \u0026amp;tw[(tick+EXPIRE+delay) \u0026amp; TW_MASK]; timer_node_t * tnode = get_timer_node(); cnode-\u0026gt;used++; tnode-\u0026gt;node = cnode; list-\u0026gt;tail-\u0026gt;next = tnode; list-\u0026gt;tail = tnode; tnode-\u0026gt;next = NULL; } //尾插，因为先到的定时任务需要先访问，比较严谨  void link_clear(link_list_t *list) { list-\u0026gt;head.next = NULL; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); } void check_conn(link_list_t *tw) { int32_t itick = tick; //获取当前时间戳  tick++;//全局时间戳加一  link_list_t *list = \u0026amp;tw[itick \u0026amp; TW_MASK];// 获取当前时间戳对应的定时器链表，对其进行检查  timer_node_t *current = list-\u0026gt;head.next;//定时器链表的头并不是一个实际的定时器节点，所以获取head的next，这是第一个实际的定时器节点  while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; //下一个  conn_node_t *cn = temp-\u0026gt;node; //从定时器节点中拿到对应的连接  cn-\u0026gt;used--; //连接的引用计数减一  temp-\u0026gt;idx = 0;//该定时器可以回收留待下次使用  if (cn-\u0026gt;used == 0) { //引用计数为0，说明连接失活，可以做相应处理  printf(\u0026#34;fd:%d kill down\\n\u0026#34;, cn-\u0026gt;id); temp-\u0026gt;next = NULL; continue; } printf(\u0026#34;fd:%d used:%d\\n\u0026#34;, cn-\u0026gt;id, cn-\u0026gt;used); } link_clear(list);//检查完这一时间戳的事件就可以删除了，等待下一次继续添加 } static time_t current_time() { time_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (time_t)ti.tv_sec; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (time_t)tv.tv_sec; #endif \treturn t; } int main() { memset(timer_nodes, 0, MAX_TIMER * sizeof(timer_node_t)); memset(conn_nodes, 0, MAX_CONN * sizeof(conn_node_t)); // init link list  link_list_t tw[TW_SIZE]; memset(tw, 0, TW_SIZE * sizeof(link_list_t)); for (int i = 0; i \u0026lt; TW_SIZE; i++) { link_clear(\u0026amp;tw[i]); } // 该测试起始时间为0秒，所以 delay 不能添加超过10的数。  { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10001; add_conn(tw, node, 0); add_conn(tw, node, 5); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10002; add_conn(tw, node, 0); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10003; add_conn(tw, node, 3); } time_t start = current_time(); for (;;) { time_t now = current_time(); if (now - start \u0026gt; 0) { for (int i=0; i\u0026lt;now-start; i++) //循环保证单位时间移动（1秒走一格）  check_conn(tw); start = now; printf(\u0026#34;check conn tick:%d\\n\u0026#34;, tick); } usleep(20000); } return 0; } static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储\nstatic conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储\n创建节点存储数组可以做到节点的回收利用，减少节点的创建和删除，静态数组连续存储，访问时间快\n#define TW_SIZE 16 时间轮大小\n#define EXPIRE 10 定时时间\n#define TW_MASK (TW_SIZE - 1) 用于轮转的掩码\nstatic uint32_t tick = 0; 时间戳,指示当前位置 add_conn(link_list_t *tw, conn_node_t *cnode, int delay)\n可以设置delay，根据提供的conn_node_t初始化定时任务，并放入tw中正确位置：(tick+EXPIRE+delay) \u0026amp; TW_MASK\nlink_clear(link_list_t *list)清空一个时间戳上的整个定时器链表\ncheck_conn(link_list_t *tw) 检查当前时间戳对应定时器链表的连接，并对齐引用计数减一，引用计数为0可以做相应处理（回调函数中可以关闭conn，或者给一个信号到管理连接的线程等等）\n多层时间轮： 多层时间轮就是在单层的基础上添加几个粒度更大的层，我们只在粒度最小的层上运行定时器（粒度最小的层运行与单层时间轮一样）即可，用钟表举例：最小层运行一周代表秒针转一圈也就是需要转分针了，这时就从第二层上映射下一分钟的所有定时任务到最小层上，第二层每一分钟的任务同时也需要前移（只要移动指针就可以，不用移动所有任务链表），前移操作很容易理解，但映射需要说一下。举例：\n我们有一个定时任务是在1分27秒后触发，模拟时钟，第一层的数组大小为60，代表60秒，第二层也是60代表60分钟，第三层24。假设此时tick（时间戳）为0，我们需要将这个定时任务放置于第二层的第0号位置（这里先不区分粒度），并在任务内保存准确的expire时间（1分27），当第一层60秒转完后，我们就需要开始映射，将第二层0号位置上的一串定时任务遍历，一个一个根据其保存的expire时间对60秒取余获得其应该在第一层的位置（27秒对应26号位置），映射到第一层上的26号位置（串到26号下面的链表中）。在下一轮的第一层运转中就会触发定时任务啦。同理，第三层（时针）到第二层（分针）的映射也是相同，使用mask取出定时任务的分位（比如1小时32分15秒就是取出32分，对应第二层的31号位置），找到对应位置放到对应链表中去。\n写的代码的时间轮结构如图：\n\r\n这里所有层级的位数加起来刚好是32位，最低层级使用2的8次方，后续四层都为2的四次方，这样的设计正好用完一个unsigned INT数，在使用mask的时候十分方便。最低层级粒度为10ms。\n这里直接附上代码，自行阅读即可：\n头文件\n#ifndef _MARK_TIMEWHEEL_ #define _MARK_TIMEWHEEL_  #include \u0026lt;stdint.h\u0026gt; #define TIME_NEAR_SHIFT 8 #define TIME_NEAR (1 \u0026lt;\u0026lt; TIME_NEAR_SHIFT) #define TIME_LEVEL_SHIFT 6 #define TIME_LEVEL (1 \u0026lt;\u0026lt; TIME_LEVEL_SHIFT) #define TIME_NEAR_MASK (TIME_NEAR-1) #define TIME_LEVEL_MASK (TIME_LEVEL-1)  typedef struct timer_node timer_node_t; typedef void (*handler_pt) (struct timer_node *node); struct timer_node { struct timer_node *next; uint32_t expire;//过期时间（非定时时间）  handler_pt callback; uint8_t cancel; //由于idx很多且不断变化很难找到特定节点，通过设计cancel成员来在触发时取消触发操作 \tint id; // 此时携带参数 }; timer_node_t* add_timer(int time, handler_pt func, int threadid); void expire_timer(void); void del_timer(timer_node_t* node); void init_timer(void); void clear_timer(); #endif  主文件\n#include \u0026#34;spinlock.h\u0026#34;#include \u0026#34;timewheel.h\u0026#34;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stddef.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; typedef struct timer { link_list_t near[TIME_NEAR]; link_list_t t[4][TIME_LEVEL]; struct spinlock lock; uint32_t time; uint64_t current; uint64_t current_point; }s_timer_t; static s_timer_t * TI = NULL; timer_node_t * link_clear(link_list_t *list) { timer_node_t * ret = list-\u0026gt;head.next; list-\u0026gt;head.next = 0; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); return ret; } void link(link_list_t *list, timer_node_t *node) { list-\u0026gt;tail-\u0026gt;next = node; list-\u0026gt;tail = node; node-\u0026gt;next=0; } void add_node(s_timer_t *T, timer_node_t *node) { uint32_t time=node-\u0026gt;expire; uint32_t current_time=T-\u0026gt;time; uint32_t msec = time - current_time; if (msec \u0026lt; TIME_NEAR) { //[0, 0x100) \tlink(\u0026amp;T-\u0026gt;near[time\u0026amp;TIME_NEAR_MASK],node); } else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+TIME_LEVEL_SHIFT))) {//[0x100, 0x4000) \tlink(\u0026amp;T-\u0026gt;t[0][((time\u0026gt;\u0026gt;TIME_NEAR_SHIFT) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+2*TIME_LEVEL_SHIFT))) {//[0x4000, 0x100000) \tlink(\u0026amp;T-\u0026gt;t[1][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+3*TIME_LEVEL_SHIFT))) {//[0x100000, 0x4000000) \tlink(\u0026amp;T-\u0026gt;t[2][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 2*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else {//[0x4000000, 0xffffffff] \tlink(\u0026amp;T-\u0026gt;t[3][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 3*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} } timer_node_t* add_timer(int time, handler_pt func, int threadid) { timer_node_t *node = (timer_node_t *)malloc(sizeof(*node)); spinlock_lock(\u0026amp;TI-\u0026gt;lock); node-\u0026gt;expire = time+TI-\u0026gt;time;// 每10ms加1 0 \tnode-\u0026gt;callback = func; node-\u0026gt;id = threadid; if (time \u0026lt;= 0) { node-\u0026gt;callback(node); free(node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return NULL; } add_node(TI, node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return node; } void move_list(s_timer_t *T, int level, int idx) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;t[level][idx]); while (current) { timer_node_t *temp=current-\u0026gt;next; add_node(T,current); current=temp; } } void timer_shift(s_timer_t *T) { int mask = TIME_NEAR; uint32_t ct = ++T-\u0026gt;time; if (ct == 0) { move_list(T, 3, 0); } else { // ct / 256 \tuint32_t time = ct \u0026gt;\u0026gt; TIME_NEAR_SHIFT; int i=0; // ct % 256 == 0 \twhile ((ct \u0026amp; (mask-1))==0) { int idx=time \u0026amp; TIME_LEVEL_MASK; if (idx!=0) { move_list(T, i, idx); break;\t} mask \u0026lt;\u0026lt;= TIME_LEVEL_SHIFT; time \u0026gt;\u0026gt;= TIME_LEVEL_SHIFT; ++i; } } } void dispatch_list(timer_node_t *current) { do { timer_node_t * temp = current; current=current-\u0026gt;next; if (temp-\u0026gt;cancel == 0) temp-\u0026gt;callback(temp); free(temp); } while (current); } void timer_execute(s_timer_t *T) { int idx = T-\u0026gt;time \u0026amp; TIME_NEAR_MASK; while (T-\u0026gt;near[idx].head.next) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;near[idx]); spinlock_unlock(\u0026amp;T-\u0026gt;lock); dispatch_list(current); spinlock_lock(\u0026amp;T-\u0026gt;lock); } } void timer_update(s_timer_t *T) { spinlock_lock(\u0026amp;T-\u0026gt;lock); timer_execute(T); timer_shift(T); timer_execute(T); spinlock_unlock(\u0026amp;T-\u0026gt;lock); } void del_timer(timer_node_t *node) { node-\u0026gt;cancel = 1; } s_timer_t * timer_create_timer() { s_timer_t *r=(s_timer_t *)malloc(sizeof(s_timer_t)); memset(r,0,sizeof(*r)); int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_clear(\u0026amp;r-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_clear(\u0026amp;r-\u0026gt;t[i][j]); } } spinlock_init(\u0026amp;r-\u0026gt;lock); r-\u0026gt;current = 0; return r; } uint64_t gettime() { uint64_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint64_t)ti.tv_sec * 100; t += ti.tv_nsec / 10000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint64_t)tv.tv_sec * 100; t += tv.tv_usec / 10000; #endif \treturn t; } void expire_timer(void) { uint64_t cp = gettime(); if (cp != TI-\u0026gt;current_point) { uint32_t diff = (uint32_t)(cp - TI-\u0026gt;current_point); TI-\u0026gt;current_point = cp; int i; for (i=0; i\u0026lt;diff; i++) { timer_update(TI); } } } void init_timer(void) { TI = timer_create_timer(); TI-\u0026gt;current_point = gettime(); } void clear_timer() { int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_list_t * list = \u0026amp;TI-\u0026gt;near[i]; timer_node_t* current = list-\u0026gt;head.next; while(current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_list_t * list = \u0026amp;TI-\u0026gt;t[i][j]; timer_node_t* current = list-\u0026gt;head.next; while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;t[i][j]); } } } ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/427_hud2c5f23b59d3c6c6e1b3f4674af3ff5b_6078631_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/","title":"定时器"},{"content":"Tcp服务器 一请求一线程 首先说明问题： 已请求一线程能承载的请求数量极少，posix标准线程8M，请求数量多时极其占用内存\n简单实现 实现一请求一线程很简单：\n#define BUFFER_SIZE 1024 void *client_routine(void *arg) { int clientfd = *(int *) arg; while(1) { char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); break; } else if(len ==0 ) { close(clientfd); break; } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } while(1) { struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); pthread_t thread_id; pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd); } return 0; } main函数中首先在指定端口处打开一个迎宾套接字sockfd用于对到来的请求分配线程（新建客户端套接字）来处理\nint clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); 就是迎宾套接字不断检测是否有新请求到来，如果到来就返回clientfd 后面新建立的线程将clientfd作为参数传递给线程执行函数即可： pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd);\nEpoll实现Tcp服务端 使用epoll来管理多个io到来的请求然后依次处理这些请求\n首先理清逻辑\n还是通过迎宾套接字来捕获请求，只是迎宾套接字应该首先加入epoll中 外层循环需要对epoll中有输入的套接字（io）遍历，如果有输入（有请求），就处理，这里需要判断套接字的类型：如果是迎宾套接字说明有新的请求，需要通过accpet来创建clientfd并交给epoll管理，如果不是就处理请求，处理完毕后从epoll中删除套接字（io），所以这个逻辑下可以知道，到来的请求是在下一轮中才处理的，并不是一到来就立即处理\nC实现\n#define EPOLL_SIZE 1024  #define BUFFER_SIZE 1024  int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } int epfd = epoll_create(1); struct epoll_event events[EPOLL_SIZE] = {0}; struct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); while(1) { int nready = epoll_wait(epfd, events, EPOLL_SIZE, 5); //-1 events里没东西就不去  if(nready == -1) continue; int i=0; for(i = 0;i \u0026lt;nready;++i) { if (events[i].data.fd == sockfd) { //迎宾的sock那就新产生一个clientfd然后交给epoll  struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); ev.events = EPOLLIN | EPOLLET; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, \u0026amp;ev); } else { int clientfd = events[i].data.fd; char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else if(len ==0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } } return 0; } 关键的函数是epoll_ctl，epoll_create，epoll_wait\nepoll_create的参数是一个int size这个参数在新版本linux内核中无意义，原来也只是告诉内核epoll大致的大小 epoll_ctl配合EPOLL_CTL_xxx的宏来对epoll进行操作（添加删除修改） epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 用来等待事件发生，其他参数比较简单，只说timeout参数：在没有检测到事件发生时最多等待的时间（单位为毫秒）\nET、LT工作模式 水平触发模式：\nev.events = EPOLLIN; 边缘触发模式：\nev.events = EPOLLIN | EPOLLET; 设置好触发模式后可以把event加入epoll中：\nstruct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/029_hu5f7dae7e78ac97b0b045f4b1159a4d54_14023957_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"简易Tcp服务器"},{"content":"线程池 基本功能模块：  线程池创建函数 线程池删除函数 线程池回调函数 线程池添加函数 线程池数据结构 线程任务数据结构 线程本身数据结构（由pid唯一确认）  首先实现数据结构： 线程任务数据结构：\nstruct nTask { void (*task_func)(struct nTask *task); void *user_data; struct nTask *prev; struct nTask *next; }; 这是任务中的一个个体，任务队列头存储在线程池数据结构中 void (*task_func)(struct nTask *task)函数指针表明函数为task_func且参数为struct nTask， 参数若为void是否更好\n线程本身数据结构：\nstruct nWorker { pthread_t threadid; int terminate; struct nManager *manager; struct nWorker *prev; struct nWorker *next; }; pid唯一标识线程，terminate用于标识该线程应被删除，存储manager（也就是所属线程池）是为了通过manager找到task队列以获取task\n线程池数据结构：\ntypedef struct nManager { struct nTask *tasks; struct nWorker *workers; pthread_mutex_t mutex; pthread_cond_t cond; } ThreadPool; 可以看到线程池其实只是一个管理者，使用mutex控制各个线程对进程内公共资源的访问，保证同时只有一个线程在访问公共资源，cond来控制各个线程的状态（处于等待队列（阻塞）或可以运行（运行、就绪态））细节在回调函数中\n然后实现API： 线程池创建函数：\nint nThreadPoolCreate(ThreadPool *pool, int numWorkers) { if (pool == NULL) return -1; if (numWorkers \u0026lt; 1) numWorkers = 1; memset(pool, 0, sizeof(ThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;cond, \u0026amp;blank_cond, sizeof(pthread_cond_t)); //pthread_mutex_init(\u0026amp;pool-\u0026gt;mutex, NULL); \tpthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;mutex, \u0026amp;blank_mutex, sizeof(pthread_mutex_t)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { struct nWorker *worker = (struct nWorker*)malloc(sizeof(struct nWorker)); if (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return -2; } memset(worker, 0, sizeof(struct nWorker)); worker-\u0026gt;manager = pool; //  int ret = pthread_create(\u0026amp;worker-\u0026gt;threadid, NULL, nThreadPoolCallback, worker); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return -3; } LIST_INSERT(worker, pool-\u0026gt;workers); } // success \treturn 0; } 根据传入线程数量参数，创建含有指定数量线程的线程池，初始化条件变量和互斥量，初始化线程本身然后放入队列\n线程池回调函数：\nstatic void *nThreadPoolCallback(void *arg) { struct nWorker *worker = (struct nWorker*)arg; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); while (worker-\u0026gt;manager-\u0026gt;tasks == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;manager-\u0026gt;cond, \u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); } if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); break; } struct nTask *task = worker-\u0026gt;manager-\u0026gt;tasks; LIST_REMOVE(task, worker-\u0026gt;manager-\u0026gt;tasks); pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); task-\u0026gt;task_func(task); //go task \t} free(worker); } 线程在创建并将其pid放入队列中后就运行回调函数，通过回调函数可以看到线程会阻塞在mutex上也可能阻塞在cond上 pthread_cond_wait函数使用两个参数： cond和mutex 这个函数等待在cond上并在收到signal或broadcast后返回，使主函数继续运行 在函数等待的时候，首先将该线程放到等待队列上然后释放mutex，这样可以保证其他线程对公共资源的访问 在收到cond的single或broadcast后线程会争夺mutex锁住临界区资源，然后自己消费，消费完后释放互斥锁 使用while循环可以保证在有资源到来的时候也就是signal cond的时候，速度慢的线程（没有抢到互斥锁的线程）可以发现资源已经被消耗完并重新通过pthread_cond_wait进入等待区\n可以看到只有在对临界区资源的访问中才加锁：访问任务队列并从中获取任务\n线程池添加函数：\nint nThreadPoolPushTask(ThreadPool *pool, struct nTask *task) { pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); LIST_INSERT(task, pool-\u0026gt;tasks); pthread_cond_signal(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); } 添加后通知整个线程队列，让他们消费\n线程池删除函数：\nint nThreadPoolDestory(ThreadPool *pool, int nWorker) { struct nWorker *worker = NULL; for (worker = pool-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate; } pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); pthread_cond_broadcast(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); pool-\u0026gt;workers = NULL; pool-\u0026gt;tasks = NULL; return 0; } 设置删除位terminate之后唤醒所有等待队列中的线程叫他们检查自己的删除位terminate 如果要删除该线程就退出while循环然后释放worker再退出\n附一个使用代码：\n#if 1  #define THREADPOOL_INIT_COUNT\t20 #define TASK_INIT_SIZE\t1000  void task_entry(struct nTask *task) { //type  //struct nTask *task = (struct nTask*)task; \tint idx = *(int *)task-\u0026gt;user_data; printf(\u0026#34;idx: %d\\n\u0026#34;, idx); free(task-\u0026gt;user_data); free(task); } int main(void) { ThreadPool pool = {0}; nThreadPoolCreate(\u0026amp;pool, THREADPOOL_INIT_COUNT); // pool --\u0026gt; memset(); \tint i = 0; for (i = 0;i \u0026lt; TASK_INIT_SIZE;i ++) { struct nTask *task = (struct nTask *)malloc(sizeof(struct nTask)); if (task == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } memset(task, 0, sizeof(struct nTask)); task-\u0026gt;task_func = task_entry; task-\u0026gt;user_data = malloc(sizeof(int)); *(int*)task-\u0026gt;user_data = i; nThreadPoolPushTask(\u0026amp;pool, task); } getchar(); } #endif ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/240_huc6553c3d943592fc8492e4a8a4385797_5666304_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","title":"线程池"},{"content":"#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define LL_ADD(item, list) do { \\ item-\u0026gt;prev = NULL;\t\\ item-\u0026gt;next = list;\t\\ list = item;\t\\ } while(0)  #define LL_REMOVE(item, list) do {\t\\ if (item-\u0026gt;prev != NULL) item-\u0026gt;prev-\u0026gt;next = item-\u0026gt;next;\t\\ if (item-\u0026gt;next != NULL) item-\u0026gt;next-\u0026gt;prev = item-\u0026gt;prev;\t\\ if (list == item) list = item-\u0026gt;next;\t\\ item-\u0026gt;prev = item-\u0026gt;next = NULL;\t\\ } while(0)  typedef struct NWORKER {//工作线程信息 \tpthread_t thread; //线程id \tint terminate; //是否要终止 \tstruct NWORKQUEUE *workqueue; //线程池，用于找到工作队列 \tstruct NWORKER *prev; struct NWORKER *next; } nWorker; typedef struct NJOB { //工作个体 \tvoid (*job_function)(struct NJOB *job); void *user_data; struct NJOB *prev; struct NJOB *next; } nJob; typedef struct NWORKQUEUE { struct NWORKER *workers; //所有工作线程的链表 \tstruct NJOB *waiting_jobs; //工作队列 \tpthread_mutex_t jobs_mtx; pthread_cond_t jobs_cond; } nWorkQueue; typedef nWorkQueue nThreadPool; static void *ntyWorkerThread(void *ptr) { //工作线程取用工作 \tnWorker *worker = (nWorker*)ptr; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //先获取工作队列的操作互斥锁  while (worker-\u0026gt;workqueue-\u0026gt;waiting_jobs == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_cond, \u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果工作队列为空，这个线程就阻塞在条件变量上等待事件发生 \t} if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果检测到工作线程被终止，那么这个线程就需要结束工作，但在结束工作前需要将对工作队列的取用权限放开，所以这里在break前需要解锁这个互斥锁 \tbreak; } nJob *job = worker-\u0026gt;workqueue-\u0026gt;waiting_jobs; //从工作队列中获取一个工作 \tif (job != NULL) { LL_REMOVE(job, worker-\u0026gt;workqueue-\u0026gt;waiting_jobs); //从工作队列中移除掉获取的这个工作 \tif (job != NULL) { } pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //已经取到工作了，就可以放开对工作队列的占有了  if (job == NULL) continue; job-\u0026gt;job_function(job); //针对工作调用他的回调函数处理，处理结束后继续循环去工作队列中取 \t} free(worker); //工作线程被终止那当然需要释放其堆上内存 \tpthread_exit(NULL); } int ntyThreadPoolCreate(nThreadPool *workqueue, int numWorkers) { //创建线程池  if (numWorkers \u0026lt; 1) numWorkers = 1; memset(workqueue, 0, sizeof(nThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; //条件变量用于通知所有的工作线程事件发生 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_cond, \u0026amp;blank_cond, sizeof(workqueue-\u0026gt;jobs_cond)); pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; // 互斥锁用于锁住工作队列确保同时只有一个工作线程在从工作队列中取工作 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_mtx, \u0026amp;blank_mutex, sizeof(workqueue-\u0026gt;jobs_mtx)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { nWorker *worker = (nWorker*)malloc(sizeof(nWorker)); //这里在堆上创建线程，那就需要在线程终止时释放 \tif (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return 1; } memset(worker, 0, sizeof(nWorker)); worker-\u0026gt;workqueue = workqueue; //初始化工作线程信息，线程池用于找到工作队列  int ret = pthread_create(\u0026amp;worker-\u0026gt;thread, NULL, ntyWorkerThread, (void *)worker); //创建线程，传入该线程信息，达到信息和线程的绑定关系 \tif (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return 1; } LL_ADD(worker, worker-\u0026gt;workqueue-\u0026gt;workers); //头插法在所有工作线程的链表中插入新建的工作线程 \t} return 0; } void ntyThreadPoolShutdown(nThreadPool *workqueue) { //关闭线程池 \tnWorker *worker = NULL; for (worker = workqueue-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate = 1; //所有工作线程的terminate关键字置为1 \t} pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); workqueue-\u0026gt;workers = NULL; //清空工作线程链表 \tworkqueue-\u0026gt;waiting_jobs = NULL; // 清空工作队列  pthread_cond_broadcast(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉所有工作线程有事件发生(shutdown，下一步检查terminate关键字)  pthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } void ntyThreadPoolQueue(nThreadPool *workqueue, nJob *job) { //向工作队列中添加工作  pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); LL_ADD(job, workqueue-\u0026gt;waiting_jobs); pthread_cond_signal(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉任意一个工作线程有事件发生(目前有新的工作出现在工作队列里了，下一步get互斥锁并取工作) \tpthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } /************************** debug thread pool **************************/ //sdk --\u0026gt; software develop kit // 提供SDK给其他开发者使用  #if 1  #define KING_MAX_THREAD\t80 #define KING_COUNTER_SIZE\t1000  void king_counter(nJob *job) { int index = *(int*)job-\u0026gt;user_data; printf(\u0026#34;index : %d, selfid : %lu\\n\u0026#34;, index, pthread_self()); free(job-\u0026gt;user_data); free(job); } int main(int argc, char *argv[]) { nThreadPool pool; ntyThreadPoolCreate(\u0026amp;pool, KING_MAX_THREAD); int i = 0; for (i = 0;i \u0026lt; KING_COUNTER_SIZE;i ++) { nJob *job = (nJob*)malloc(sizeof(nJob)); if (job == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } job-\u0026gt;job_function = king_counter; job-\u0026gt;user_data = malloc(sizeof(int)); *(int*)job-\u0026gt;user_data = i; ntyThreadPoolQueue(\u0026amp;pool, job); } getchar(); printf(\u0026#34;\\n\u0026#34;); } #endif  ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/241_hudd714e7c3305b4c239dc02d266d283b9_7211866_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/","title":"线程池进阶版"},{"content":"请求池实现 同步阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  struct dns_header { unsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item { char *domain; char *ip; }; int dns_create_header(struct dns_header *header) { if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) { int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) { int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); return 0; } char *domain[] = { //\t\u0026#34;www.ntytcp.com\u0026#34;, \t\u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; typedef void (*async_result_cb)(struct dns_item *arg, int count); struct async_context { int epfd; pthread_t threadid; }; struct ep_arg { int sockfd; async_result_cb cb; }; #define ASYNC_EVENTS\t128  void *dns_async_callback(void *arg) { struct async_context* ctx = (struct async_context*)arg; while (1) { struct epoll_event events[ASYNC_EVENTS] = {0}; int nready = epoll_wait(ctx-\u0026gt;epfd, events, ASYNC_EVENTS, -1); if (nready \u0026lt; 0) { continue; } int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *ptr = events[i].data.ptr; int sockfd = ptr-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; int count = dns_parse_response(buffer, \u0026amp;domains); ptr-\u0026gt;cb(domains, count); // sockfd \tclose (sockfd); free(ptr); // epollout --\u0026gt; \t//epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_MOD, sockfd, NULL); \t} } } // 1 . context // 2 . return context; // struct async_context* dns_async_client_init(void) { int epfd = epoll_create(1); if (epfd \u0026lt; 0) return NULL; struct async_context* ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) return NULL; ctx-\u0026gt;epfd = epfd; int ret = pthread_create(\u0026amp;ctx-\u0026gt;threadid, NULL, dns_async_callback, ctx); if (ret) { close(epfd); free(ctx); return NULL; } return ctx; } int dns_async_client_destroy(struct async_context* ctx) { close(ctx-\u0026gt;epfd); pthread_cancel(ctx-\u0026gt;threadid); } // int dns_async_client_commit(struct async_context *ctx, async_result_cb cb) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *ptr = calloc(1, sizeof(struct ep_arg)); if (ptr == NULL) return -1; ptr-\u0026gt;sockfd = sockfd; ptr-\u0026gt;cb = cb; // \tstruct epoll_event ev; ev.data.ptr = ptr; ev.events = EPOLLIN; epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); return 0; } int main(int argc, char *argv[]) { int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_client_commit(domain[i]); } getchar(); } 异步非阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  #define ASYNC_CLIENT_NUM\t1024  struct dns_header { //dns 头 \tunsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question {//dns 请求 \tint length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item {//dns 基本信息 \tchar *domain; char *ip; }; typedef void (*async_result_cb)(struct dns_item *list, int count); //请求端针对返回数据的处理函数（以回调函数的方式存在，在recv后调用，通过epoll中的event内的data中ptr传递，这一点与reactor中很像）  struct async_context { //异步请求池中 epoll_wait单独占用一个线程 且epoll也需要在其他线程中出现，这一部分为不同线程共用部分，所以命名为async context也叫上下文 //上下文中一般来讲还需要保存进行epoll_wait的线程id，但这里保存也没啥用 \tint epfd; }; struct ep_arg { //每一个请求（fd）和其处理函数(cb)存在一个sturct中，与reactor 类似 \tint sockfd; async_result_cb cb; }; int dns_create_header(struct dns_header *header) { //dns 创建头  if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { //dns 创建请求  if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) {//dns 创建请求  int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static int set_block(int fd, int block) { //设置fd的阻塞类型 \tint flags = fcntl(fd, F_GETFL, 0); if (flags \u0026lt; 0) return flags; if (block) { flags \u0026amp;= ~O_NONBLOCK; } else { flags |= O_NONBLOCK; } if (fcntl(fd, F_SETFL, flags) \u0026lt; 0) return -1; return 0; } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { //dns 解析域名  int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) {//dns 解析应答  int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { //这个是同步请求中的提交，大概看一下就可以  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); while (1) { char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); if (n \u0026lt;= 0) continue; printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); break; } return 0; } void dns_async_client_free_domains(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { free(list[i].domain); free(list[i].ip); } free(list); } //dns_async_client_proc() //epoll_wait //result callback static void* dns_async_client_proc(void *arg) { //异步请求框架第三环节。 callback函数，这个callback函数不是用于处理接受数据的callback，而是initial中用于接受和处理数据的线程运行的函数，在线程创建时传入 \tstruct async_context *ctx = (struct async_context*)arg; int epfd = ctx-\u0026gt;epfd; while (1) { struct epoll_event events[ASYNC_CLIENT_NUM] = {0}; int nready = epoll_wait(epfd, events, ASYNC_CLIENT_NUM, -1); if (nready \u0026lt; 0) { if (errno == EINTR || errno == EAGAIN) { continue; } else { break; } } else if (nready == 0) { continue; } printf(\u0026#34;nready:%d\\n\u0026#34;, nready); int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *data = (struct ep_arg*)events[i].data.ptr; int sockfd = data-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); struct dns_item *domain_list = NULL; int count = dns_parse_response(buffer, \u0026amp;domain_list); data-\u0026gt;cb(domain_list, count); //call cb \tint ret = epoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL); //printf(\u0026#34;epoll_ctl DEL --\u0026gt; sockfd:%d\\n\u0026#34;, sockfd);  close(sockfd); ///// //异步请求框架第四环节。 关闭  dns_async_client_free_domains(domain_list, count); free(data); } } } //dns_async_client_init() //epoll init //thread init struct async_context *dns_async_client_init(void) { //异步请求框架第一环节。 initial 初始化上下文信息，主要是创建epoll、创建线程（接受数据的线程）  int epfd = epoll_create(1); // \tif (epfd \u0026lt; 0) return NULL; struct async_context *ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) { close(epfd); return NULL; } ctx-\u0026gt;epfd = epfd; pthread_t thread_id; int ret = pthread_create(\u0026amp;thread_id, NULL, dns_async_client_proc, ctx); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); return NULL; } usleep(1); //child go first  return ctx; } //dns_async_client_commit(ctx, domain) //socket init //dns_request //sendto dns send int dns_async_client_commit(struct async_context* ctx, const char *domain, async_result_cb cb) { //异步请求框架第二环节 commit，提交请求。 可以看到传参上下文，获取epoll，将新来的请求提交并加入epoll中在另一个线程中等待返回数据并使用传递的cb解析  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *eparg = (struct ep_arg*)calloc(1, sizeof(struct ep_arg)); //初始化这个新请求后续返回结果对应的处理函数，并通过下面ev.data.ptr = eparg保存并通过另一个线程检测到返回数据后调用 \tif (eparg == NULL) return -1; eparg-\u0026gt;sockfd = sockfd; eparg-\u0026gt;cb = cb; struct epoll_event ev; ev.data.ptr = eparg; ev.events = EPOLLIN; ret = epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); //printf(\u0026#34; epoll_ctl ADD: sockfd-\u0026gt;%d, ret:%d\\n\u0026#34;, sockfd, ret);  return ret; } char *domain[] = { \u0026#34;www.ntytcp.com\u0026#34;, \u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; static void dns_async_client_result_callback(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { printf(\u0026#34;name:%s, ip:%s\\n\u0026#34;, list[i].domain, list[i].ip); } } int main(int argc, char *argv[]) { #if 0dns_client_commit(argv[1]); #else  struct async_context *ctx = dns_async_client_init(); if (ctx == NULL) return -2; int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_async_client_commit(ctx, domain[i], dns_async_client_result_callback); //sleep(2); \t} getchar(); #endif \t} ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/247_hu00377075f8d3c09f9b9bff67495c3ec1_5277773_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/","title":"请求池"},{"content":"连接池实现 mysql连接池 头文件： #ifndef DBPOOL_H_ #define DBPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;list\u0026gt;#include \u0026lt;mutex\u0026gt;#include \u0026lt;condition_variable\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;stdint.h\u0026gt; #include \u0026lt;mysql.h\u0026gt; #define MAX_ESCAPE_STRING_LEN\t10240  using namespace std; // 返回结果 select的时候用 class CResultSet { public: CResultSet(MYSQL_RES* res); virtual ~CResultSet(); bool Next(); int GetInt(const char* key); char* GetString(const char* key); private: int _GetIndex(const char* key); MYSQL_RES* m_res; MYSQL_ROW\tm_row; map\u0026lt;string, int\u0026gt;\tm_key_map; }; // 插入数据用 class CPrepareStatement { public: CPrepareStatement(); virtual ~CPrepareStatement(); bool Init(MYSQL* mysql, string\u0026amp; sql); void SetParam(uint32_t index, int\u0026amp; value); void SetParam(uint32_t index, uint32_t\u0026amp; value); void SetParam(uint32_t index, string\u0026amp; value); void SetParam(uint32_t index, const string\u0026amp; value); bool ExecuteUpdate(); uint32_t GetInsertId(); private: MYSQL_STMT*\tm_stmt; MYSQL_BIND*\tm_param_bind; uint32_t\tm_param_cnt; }; class CDBPool; class CDBConn { public: CDBConn(CDBPool* pDBPool); virtual ~CDBConn(); int Init(); // 创建表 \tbool ExecuteCreate(const char* sql_query); // 删除表 \tbool ExecuteDrop(const char* sql_query); // 查询 \tCResultSet* ExecuteQuery(const char* sql_query); /** * 执行DB更新，修改 * * @param sql_query sql * @param care_affected_rows 是否在意影响的行数，false:不在意；true:在意 * * @return 成功返回true 失败返回false */ bool ExecuteUpdate(const char* sql_query, bool care_affected_rows = true); uint32_t GetInsertId(); // 开启事务 \tbool StartTransaction(); // 提交事务 \tbool Commit(); // 回滚事务 \tbool Rollback(); // 获取连接池名 \tconst char* GetPoolName(); MYSQL* GetMysql() { return m_mysql; } private: CDBPool* m_pDBPool;\t// to get MySQL server information \tMYSQL* m_mysql;\t// 对应一个连接 \tchar\tm_escape_string[MAX_ESCAPE_STRING_LEN + 1]; }; class CDBPool {\t// 只是负责管理连接CDBConn，真正干活的是CDBConn public: CDBPool() {} CDBPool(const char* pool_name, const char* db_server_ip, uint16_t db_server_port, const char* username, const char* password, const char* db_name, int max_conn_cnt); virtual ~CDBPool(); int Init();\t// 连接数据库，创建连接 \tCDBConn* GetDBConn(const int timeout_ms = -1);\t// 获取连接资源 \tvoid RelDBConn(CDBConn* pConn);\t// 归还连接资源  const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetDBServerIP() { return m_db_server_ip.c_str(); } uint16_t GetDBServerPort() { return m_db_server_port; } const char* GetUsername() { return m_username.c_str(); } const char* GetPasswrod() { return m_password.c_str(); } const char* GetDBName() { return m_db_name.c_str(); } private: string m_pool_name;\t// 连接池名称 \tstring m_db_server_ip;\t// 数据库ip \tuint16_t\tm_db_server_port; // 数据库端口 \tstring m_username; // 用户名 \tstring m_password;\t// 用户密码 \tstring m_db_name;\t// db名称 \tint\tm_db_cur_conn_cnt;\t// 当前启用的连接数量 \tint m_db_max_conn_cnt;\t// 最大连接数量 \tlist\u0026lt;CDBConn*\u0026gt;\tm_free_list;\t// 空闲的连接  list\u0026lt;CDBConn*\u0026gt;\tm_used_list;\t// 记录已经被请求的连接 \tstd::mutex m_mutex; std::condition_variable m_cond_var; bool m_abort_request = false; // CThreadNotify\tm_free_notify;\t// 信号量 }; #endif /* DBPOOL_H_ */ }; 实现：  #include \u0026#34;DBPool.h\u0026#34;#include \u0026lt;string.h\u0026gt; #define log_error printf #define log_warn printf #define log_info printf #define MIN_DB_CONN_CNT 2 #define MAX_DB_CONN_FAIL_NUM 10  CResultSet::CResultSet(MYSQL_RES *res) { m_res = res; // map table field key to index in the result array \tint num_fields = mysql_num_fields(m_res); MYSQL_FIELD *fields = mysql_fetch_fields(m_res); for (int i = 0; i \u0026lt; num_fields; i++) { // 多行 \tm_key_map.insert(make_pair(fields[i].name, i)); } } CResultSet::~CResultSet() { if (m_res) { mysql_free_result(m_res); m_res = NULL; } } bool CResultSet::Next() { m_row = mysql_fetch_row(m_res); if (m_row) { return true; } else { return false; } } int CResultSet::_GetIndex(const char *key) { map\u0026lt;string, int\u0026gt;::iterator it = m_key_map.find(key); if (it == m_key_map.end()) { return -1; } else { return it-\u0026gt;second; } } int CResultSet::GetInt(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return 0; } else { return atoi(m_row[idx]); // 有索引 \t} } char *CResultSet::GetString(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return NULL; } else { return m_row[idx];\t// 列 \t} } ///////////////////////////////////////// CPrepareStatement::CPrepareStatement() { m_stmt = NULL; m_param_bind = NULL; m_param_cnt = 0; } CPrepareStatement::~CPrepareStatement() { if (m_stmt) { mysql_stmt_close(m_stmt); m_stmt = NULL; } if (m_param_bind) { delete[] m_param_bind; m_param_bind = NULL; } } bool CPrepareStatement::Init(MYSQL *mysql, string \u0026amp;sql) { mysql_ping(mysql);\t// 当mysql连接丢失的时候，使用mysql_ping能够自动重连数据库  //g_master_conn_fail_num ++; \tm_stmt = mysql_stmt_init(mysql); if (!m_stmt) { log_error(\u0026#34;mysql_stmt_init failed\\n\u0026#34;); return false; } if (mysql_stmt_prepare(m_stmt, sql.c_str(), sql.size())) { log_error(\u0026#34;mysql_stmt_prepare failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } m_param_cnt = mysql_stmt_param_count(m_stmt); if (m_param_cnt \u0026gt; 0) { m_param_bind = new MYSQL_BIND[m_param_cnt]; if (!m_param_bind) { log_error(\u0026#34;new failed\\n\u0026#34;); return false; } memset(m_param_bind, 0, sizeof(MYSQL_BIND) * m_param_cnt); } return true; } void CPrepareStatement::SetParam(uint32_t index, int \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, uint32_t \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } void CPrepareStatement::SetParam(uint32_t index, const string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } bool CPrepareStatement::ExecuteUpdate() { if (!m_stmt) { log_error(\u0026#34;no m_stmt\\n\u0026#34;); return false; } if (mysql_stmt_bind_param(m_stmt, m_param_bind)) { log_error(\u0026#34;mysql_stmt_bind_param failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_execute(m_stmt)) { log_error(\u0026#34;mysql_stmt_execute failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_affected_rows(m_stmt) == 0) { log_error(\u0026#34;ExecuteUpdate have no effect\\n\u0026#34;); return false; } return true; } uint32_t CPrepareStatement::GetInsertId() { return mysql_stmt_insert_id(m_stmt); } ///////////////////// CDBConn::CDBConn(CDBPool *pPool) { m_pDBPool = pPool; m_mysql = NULL; } CDBConn::~CDBConn() { if (m_mysql) { mysql_close(m_mysql); } } int CDBConn::Init() { m_mysql = mysql_init(NULL);\t// mysql_标准的mysql c client对应的api \tif (!m_mysql) { log_error(\u0026#34;mysql_init failed\\n\u0026#34;); return 1; } my_bool reconnect = true; mysql_options(m_mysql, MYSQL_OPT_RECONNECT, \u0026amp;reconnect);\t// 配合mysql_ping实现自动重连 \tmysql_options(m_mysql, MYSQL_SET_CHARSET_NAME, \u0026#34;utf8mb4\u0026#34;);\t// utf8mb4和utf8区别  // ip 端口 用户名 密码 数据库名 \tif (!mysql_real_connect(m_mysql, m_pDBPool-\u0026gt;GetDBServerIP(), m_pDBPool-\u0026gt;GetUsername(), m_pDBPool-\u0026gt;GetPasswrod(), m_pDBPool-\u0026gt;GetDBName(), m_pDBPool-\u0026gt;GetDBServerPort(), NULL, 0)) { log_error(\u0026#34;mysql_real_connect failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return 2; } return 0; } const char *CDBConn::GetPoolName() { return m_pDBPool-\u0026gt;GetPoolName(); } bool CDBConn::ExecuteCreate(const char *sql_query) { mysql_ping(m_mysql); // mysql_real_query 实际就是执行了SQL \tif (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::ExecuteDrop(const char *sql_query) { mysql_ping(m_mysql);\t// 如果端开了，能够自动重连  if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } CResultSet *CDBConn::ExecuteQuery(const char *sql_query) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); return NULL; } // 返回结果 \tMYSQL_RES *res = mysql_store_result(m_mysql);\t// 返回结果 \tif (!res) { log_error(\u0026#34;mysql_store_result failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return NULL; } CResultSet *result_set = new CResultSet(res);\t// 存储到CResultSet \treturn result_set; } /* 1.执行成功，则返回受影响的行的数目，如果最近一次查询失败的话，函数返回 -1 2.对于delete,将返回实际删除的行数. 3.对于update,如果更新的列值原值和新值一样,如update tables set col1=10 where id=1; id=1该条记录原值就是10的话,则返回0。 mysql_affected_rows返回的是实际更新的行数,而不是匹配到的行数。 */ bool CDBConn::ExecuteUpdate(const char *sql_query, bool care_affected_rows) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); //g_master_conn_fail_num ++; \treturn false; } if (mysql_affected_rows(m_mysql) \u0026gt; 0) { return true; } else { // 影响的行数为0时 \tif (care_affected_rows) { // 如果在意影响的行数时, 返回false, 否则返回true \tlog_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\\n\u0026#34;, mysql_error(m_mysql), sql_query); return false; } else { log_warn(\u0026#34;affected_rows=0, sql: %s\\n\\n\u0026#34;, sql_query); return true; } } } bool CDBConn::StartTransaction() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;start transaction\\n\u0026#34;, 17)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Rollback() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;rollback\\n\u0026#34;, 8)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: rollback\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Commit() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;commit\\n\u0026#34;, 6)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: commit\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } uint32_t CDBConn::GetInsertId() { return (uint32_t)mysql_insert_id(m_mysql); } //////////////// CDBPool::CDBPool(const char *pool_name, const char *db_server_ip, uint16_t db_server_port, const char *username, const char *password, const char *db_name, int max_conn_cnt) { m_pool_name = pool_name; m_db_server_ip = db_server_ip; m_db_server_port = db_server_port; m_username = username; m_password = password; m_db_name = db_name; m_db_max_conn_cnt = max_conn_cnt;\t// \tm_db_cur_conn_cnt = MIN_DB_CONN_CNT; // 最小连接数量 } // 释放连接池 CDBPool::~CDBPool() { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); m_abort_request = true; m_cond_var.notify_all();\t// 通知所有在等待的  for (list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CDBConn *pConn = *it; delete pConn; } m_free_list.clear(); } int CDBPool::Init() { // 创建固定最小的连接数量 \tfor (int i = 0; i \u0026lt; m_db_cur_conn_cnt; i++) { CDBConn *pDBConn = new CDBConn(this); int ret = pDBConn-\u0026gt;Init(); if (ret) { delete pDBConn; return ret; } m_free_list.push_back(pDBConn); } // log_error(\u0026#34;db pool: %s, size: %d\\n\u0026#34;, m_pool_name.c_str(), (int)m_free_list.size()); \treturn 0; } /* *TODO: 增加保护机制，把分配的连接加入另一个队列，这样获取连接时，如果没有空闲连接， *TODO: 检查已经分配的连接多久没有返回，如果超过一定时间，则自动收回连接，放在用户忘了调用释放连接的接口 * timeout_ms默认为-1死等 * timeout_ms \u0026gt;=0 则为等待的时间 */ int wait_cout = 0; CDBConn *CDBPool::GetDBConn(const int timeout_ms) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(m_mutex); if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } if (m_free_list.empty())\t// 当没有连接可以用时 \t{ // 第一步先检测 当前连接数量是否达到最大的连接数量 \tif (m_db_cur_conn_cnt \u0026gt;= m_db_max_conn_cnt) { // 如果已经到达了，看看是否需要超时等待 \tif(timeout_ms \u0026lt; 0)\t// 死等，直到有连接可以用 或者 连接池要退出 \t{ log_info(\u0026#34;wait ms:%d\\n\u0026#34;, timeout_ms); m_cond_var.wait(lock, [this] { // log_info(\u0026#34;wait:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \t// 当前连接数量小于最大连接数量 或者请求释放连接池时退出 \treturn (!m_free_list.empty()) | m_abort_request; }); } else { // return如果返回 false，继续wait(或者超时), 如果返回true退出wait \t// 1.m_free_list不为空 \t// 2.超时退出 \t// 3. m_abort_request被置为true，要释放整个连接池 \tm_cond_var.wait_for(lock, std::chrono::milliseconds(timeout_ms), [this] { // log_info(\u0026#34;wait_for:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \treturn (!m_free_list.empty()) | m_abort_request; }); // 带超时功能时还要判断是否为空 \tif(m_free_list.empty()) // 如果连接池还是没有空闲则退出 \t{ return NULL; } } if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } } else // 还没有到最大连接则创建连接 \t{ CDBConn *pDBConn = new CDBConn(this);\t//新建连接 \tint ret = pDBConn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init DBConnecton failed\\n\\n\u0026#34;); delete pDBConn; return NULL; } else { m_free_list.push_back(pDBConn); m_db_cur_conn_cnt++; log_info(\u0026#34;new db connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_db_cur_conn_cnt); } } } CDBConn *pConn = m_free_list.front();\t// 获取连接 \tm_free_list.pop_front();\t// STL 吐出连接，从空闲队列删除 \t// pConn-\u0026gt;setCurrentTime(); // 伪代码 \tm_used_list.push_back(pConn);\t//  return pConn; } void CDBPool::RelDBConn(CDBConn *pConn) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++)\t// 避免重复归还 \t{ if (*it == pConn)\t{ break; } } if (it == m_free_list.end()) { m_used_list.remove(pConn); m_free_list.push_back(pConn); m_cond_var.notify_one();\t// 通知取队列 \t} else { log_error(\u0026#34;RelDBConn failed\\n\u0026#34;); } } // 遍历检测是否超时未归还 // pConn-\u0026gt;isTimeout(); // 当前时间 - 被请求的时间 // 强制回收 从m_used_list 放回 m_free_list redis连接池 =============\n头文件  /* * @Author: your name * @Date: 2019-12-07 10:54:57 * @LastEditTime : 2020-01-10 16:35:13 * @LastEditors : Please set LastEditors * @Description: In User Settings Edit * @FilePath: \\src\\cache_pool\\CachePool.h */ #ifndef CACHEPOOL_H_ #define CACHEPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;list\u0026gt; #include \u0026#34;Thread.h\u0026#34; #include \u0026#34;hiredis.h\u0026#34; using std::string; using std::list; using std::map; using std::vector; class CachePool; class CacheConn { public: CacheConn(const char* server_ip, int server_port, int db_index, const char* password, const char *pool_name =\u0026#34;\u0026#34;); CacheConn(CachePool* pCachePool);\tvirtual ~CacheConn(); int Init(); void DeInit(); const char* GetPoolName(); // 通用操作  // 判断一个key是否存在  bool isExists(string \u0026amp;key); // 删除某个key  long del(string \u0026amp;key); // ------------------- 字符串相关 ------------------- \tstring get(string key); string set(string key, string\u0026amp; value); string setex(string key, int timeout, string value); // string mset(string key, map);  //批量获取  bool mget(const vector\u0026lt;string\u0026gt;\u0026amp; keys, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); //原子加减1  long incr(string key); long decr(string key); // ---------------- 哈希相关 ------------------------ \tlong hdel(string key, string field); string hget(string key, string field); bool hgetAll(string key, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); long hset(string key, string field, string value); long hincrBy(string key, string field, long value); long incrBy(string key, long value); string hmset(string key, map\u0026lt;string, string\u0026gt;\u0026amp; hash); bool hmget(string key, list\u0026lt;string\u0026gt;\u0026amp; fields, list\u0026lt;string\u0026gt;\u0026amp; ret_value); // ------------ 链表相关 ------------ \tlong lpush(string key, string value); long rpush(string key, string value); long llen(string key); bool lrange(string key, long start, long end, list\u0026lt;string\u0026gt;\u0026amp; ret_value); bool flushdb(); private: CachePool* m_pCachePool; redisContext* m_pContext; uint64_t\tm_last_connect_time; uint16_t m_server_port; string m_server_ip; string m_password; uint16_t m_db_index; string m_pool_name; }; class CachePool { public: // db_index和mysql不同的地方 \tCachePool(const char* pool_name, const char* server_ip, int server_port, int db_index, const char *password, int max_conn_cnt); virtual ~CachePool(); int Init(); // 获取空闲的连接资源 \tCacheConn* GetCacheConn(); // Pool回收连接资源 \tvoid RelCacheConn(CacheConn* pCacheConn); const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetServerIP() { return m_server_ip.c_str(); } const char* GetPassword() { return m_password.c_str(); } int GetServerPort() { return m_server_port; } int GetDBIndex() { return m_db_index; } private: string m_pool_name; string\tm_server_ip; string m_password; int\tm_server_port; int\tm_db_index;\t// mysql 数据库名字， redis db index  int\tm_cur_conn_cnt; int m_max_conn_cnt; list\u0026lt;CacheConn*\u0026gt;\tm_free_list; CThreadNotify\tm_free_notify; }; #endif /* CACHEPOOL_H_ */ 实现  #include \u0026#34;CachePool.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;Thread.h\u0026#34; #define log_error printf #define log_info printf  #define MIN_CACHE_CONN_CNT 2 #define MAX_CACHE_CONN_FAIL_NUM 10  CacheConn::CacheConn(const char *server_ip, int server_port, int db_index, const char *password, const char *pool_name) { m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_pool_name = pool_name; m_pContext = NULL; m_last_connect_time = 0; } CacheConn::CacheConn(CachePool *pCachePool) { m_pCachePool = pCachePool; if (pCachePool) { m_server_ip = pCachePool-\u0026gt;GetServerIP(); m_server_port = pCachePool-\u0026gt;GetServerPort(); m_db_index = pCachePool-\u0026gt;GetDBIndex(); m_password = pCachePool-\u0026gt;GetPassword(); m_pool_name = pCachePool-\u0026gt;GetPoolName(); } else { log_error(\u0026#34;pCachePool is NULL\\n\u0026#34;); } m_pContext = NULL; m_last_connect_time = 0; } CacheConn::~CacheConn() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } /* * redis初始化连接和重连操作，类似mysql_ping() */ int CacheConn::Init() { if (m_pContext)\t// 非空，连接是正常的 \t{ return 0; } // 1s 尝试重连一次 \tuint64_t cur_time = (uint64_t)time(NULL); if (cur_time \u0026lt; m_last_connect_time + 1) // 重连尝试 间隔1秒 \t{ printf(\u0026#34;cur_time:%lu, m_last_connect_time:%lu\\n\u0026#34;, cur_time, m_last_connect_time); return 1; } // printf(\u0026#34;m_last_connect_time = cur_time\\n\u0026#34;); \tm_last_connect_time = cur_time; // 1000ms超时 \tstruct timeval timeout = {0, 1000000}; // 建立连接后使用 redisContext 来保存连接状态。 \t// redisContext 在每次操作后会修改其中的 err 和 errstr 字段来表示发生的错误码（大于0）和对应的描述。 \tm_pContext = redisConnectWithTimeout(m_server_ip.c_str(), m_server_port, timeout); if (!m_pContext || m_pContext-\u0026gt;err) { if (m_pContext) { log_error(\u0026#34;redisConnect failed: %s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; } else { log_error(\u0026#34;redisConnect failed\\n\u0026#34;); } return 1; } redisReply *reply; // 验证 \tif (!m_password.empty()) { reply = (redisReply *)redisCommand(m_pContext, \u0026#34;AUTH %s\u0026#34;, m_password.c_str()); if (!reply || reply-\u0026gt;type == REDIS_REPLY_ERROR) { log_error(\u0026#34;Authentication failure:%p\\n\u0026#34;, reply); if (reply) freeReplyObject(reply); return -1; } else { // log_info(\u0026#34;Authentication success\\n\u0026#34;); \t} freeReplyObject(reply); } reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SELECT %d\u0026#34;, 0); if (reply \u0026amp;\u0026amp; (reply-\u0026gt;type == REDIS_REPLY_STATUS) \u0026amp;\u0026amp; (strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0)) { freeReplyObject(reply); return 0; } else { if (reply) log_error(\u0026#34;select cache db failed:%s\\n\u0026#34;, reply-\u0026gt;str); return 2; } } void CacheConn::DeInit() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } const char *CacheConn::GetPoolName() { return m_pool_name.c_str(); } string CacheConn::get(string key) { string value; if (Init()) { return value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;GET %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return value; } string CacheConn::set(string key, string \u0026amp;value) { string ret_value; if (Init()) { return ret_value; } // 返回的结果存放在redisReply \tredisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SET %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); // 释放资源 \treturn ret_value; } string CacheConn::setex(string key, int timeout, string value) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SETEX %s %d %s\u0026#34;, key.c_str(), timeout, value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); return ret_value; } bool CacheConn::mget(const vector\u0026lt;string\u0026gt; \u0026amp;keys, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } if (keys.empty()) { return false; } string strKey; bool bFirst = true; for (vector\u0026lt;string\u0026gt;::const_iterator it = keys.begin(); it != keys.end(); ++it) { if (bFirst) { bFirst = false; strKey = *it; } else { strKey += \u0026#34; \u0026#34; + *it; } } if (strKey.empty()) { return false; } strKey = \u0026#34;MGET \u0026#34; + strKey; redisReply *reply = (redisReply *)redisCommand(m_pContext, strKey.c_str()); if (!reply) { log_info(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; ++i) { redisReply *child_reply = reply-\u0026gt;element[i]; if (child_reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value[keys[i]] = child_reply-\u0026gt;str; } } } freeReplyObject(reply); return true; } bool CacheConn::isExists(string \u0026amp;key) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;EXISTS %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); if (0 == ret_value) { return false; } else { return true; } } long CacheConn::del(string \u0026amp;key) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DEL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hdel(string key, string field) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HDEL %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hget(string key, string field) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGET %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return ret_value; } bool CacheConn::hgetAll(string key, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGETALL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if ((reply-\u0026gt;type == REDIS_REPLY_ARRAY) \u0026amp;\u0026amp; (reply-\u0026gt;elements % 2 == 0)) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i += 2) { redisReply *field_reply = reply-\u0026gt;element[i]; redisReply *value_reply = reply-\u0026gt;element[i + 1]; string field(field_reply-\u0026gt;str, field_reply-\u0026gt;len); string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.insert(make_pair(field, value)); } } freeReplyObject(reply); return true; } long CacheConn::hset(string key, string field, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HSET %s %s %s\u0026#34;, key.c_str(), field.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hincrBy(string key, string field, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HINCRBY %s %s %ld\u0026#34;, key.c_str(), field.c_str(), value); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::incrBy(string key, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCRBY %s %ld\u0026#34;, key.c_str(), value); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hmset(string key, map\u0026lt;string, string\u0026gt; \u0026amp;hash) { string ret_value; if (Init()) { return ret_value; } int argc = hash.size() * 2 + 2; const char **argv = new const char *[argc]; if (!argv) { return ret_value; } argv[0] = \u0026#34;HMSET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (map\u0026lt;string, string\u0026gt;::iterator it = hash.begin(); it != hash.end(); it++) { argv[i++] = it-\u0026gt;first.c_str(); argv[i++] = it-\u0026gt;second.c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); delete[] argv; freeReplyObject(reply); return ret_value; } bool CacheConn::hmget(string key, list\u0026lt;string\u0026gt; \u0026amp;fields, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } int argc = fields.size() + 2; const char **argv = new const char *[argc]; if (!argv) { return false; } argv[0] = \u0026#34;HMGET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (list\u0026lt;string\u0026gt;::iterator it = fields.begin(); it != fields.end(); it++) { argv[i++] = it-\u0026gt;c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, (const char **)argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } delete[] argv; freeReplyObject(reply); return true; } long CacheConn::incr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::decr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DECR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::lpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::rpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;RPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::llen(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LLEN %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } bool CacheConn::lrange(string key, long start, long end, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LRANGE %s %d %d\u0026#34;, key.c_str(), start, end); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } freeReplyObject(reply); return true; } bool CacheConn::flushdb() { bool ret = false; if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;FLUSHDB\u0026#34;); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_STRING \u0026amp;\u0026amp; strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0) { ret = true; } freeReplyObject(reply); return ret; } /////////////// CachePool::CachePool(const char *pool_name, const char *server_ip, int server_port, int db_index, const char *password, int max_conn_cnt) { m_pool_name = pool_name; m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_max_conn_cnt = max_conn_cnt; m_cur_conn_cnt = MIN_CACHE_CONN_CNT; } CachePool::~CachePool() { m_free_notify.Lock(); for (list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CacheConn *pConn = *it; delete pConn; } m_free_list.clear(); m_cur_conn_cnt = 0; m_free_notify.Unlock(); } int CachePool::Init() { for (int i = 0; i \u0026lt; m_cur_conn_cnt; i++) { CacheConn *pConn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); if (pConn-\u0026gt;Init()) { delete pConn; return 1; } m_free_list.push_back(pConn); } log_info(\u0026#34;cache pool: %s, list size: %lu\\n\u0026#34;, m_pool_name.c_str(), m_free_list.size()); return 0; } CacheConn *CachePool::GetCacheConn() { m_free_notify.Lock(); while (m_free_list.empty()) { if (m_cur_conn_cnt \u0026gt;= m_max_conn_cnt) { m_free_notify.Wait(); } else { CacheConn *p_cache_conn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); int ret = p_cache_conn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init CacheConn failed\\n\u0026#34;); delete p_cache_conn; m_free_notify.Unlock(); return NULL; } else { m_free_list.push_back(p_cache_conn); m_cur_conn_cnt++; log_info(\u0026#34;new cache connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_cur_conn_cnt); } } } CacheConn *pConn = m_free_list.front(); m_free_list.pop_front(); m_free_notify.Unlock(); return pConn; } void CachePool::RelCacheConn(CacheConn *p_cache_conn) { m_free_notify.Lock(); list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++) { if (*it == p_cache_conn) { break; } } if (it == m_free_list.end()) { m_free_list.push_back(p_cache_conn); } m_free_notify.Signal(); m_free_notify.Unlock(); } ","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/245_hubfe96db144675e6281ae6d025e60e504_7615599_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"连接池"},{"content":"锁 posix API 自旋锁 互斥锁 读写锁 原子操作 共享内存 try-catch\n","date":"2020-09-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E9%94%81/389_hu19a302d4968ef7ca02b91dd8f18c3f5f_3179286_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E9%94%81/","title":"锁"},{"content":"MYSQL mysql安装与配置 在虚拟机上安装mysql,使用apt-get install就可以 这里我只检索到了mysql-server-5.7就安装了5.7\n在本地win10上安装mysqlbench用于连接虚拟机的mysql服务器 这里使用网络连接，可能是因为mysql版本的原因，本来应该在/etc/mysql中的my.cnf文件中显式的配置有基本信息，我只需要修改部分，但5.7在/etc/mysql/mysql.conf.d/mysqld.cnf,在它的基础上修改对应的bind-address为0.0.0.0保证回环地址可访问：\n# # The MySQL database server configuration file. # # You can copy this to one of: # - \u0026#34;/etc/mysql/my.cnf\u0026#34; to set global options, # - \u0026#34;~/.my.cnf\u0026#34; to set user-specific options. # # One can use all long options that the program supports. # Run program with --help to get a list of available options and with # --print-defaults to see which it would actually understand and use. # # For explanations see # http://dev.mysql.com/doc/mysql/en/server-system-variables.html # This will be passed to all mysql clients # It has been reported that passwords should be enclosed with ticks/quotes # escpecially if they contain \u0026#34;#\u0026#34; chars... # Remember to edit /etc/mysql/debian.cnf when changing the socket location. # Here is entries for some specific programs # The following values assume you have at least 32M ram [mysqld_safe] socket = /var/run/mysqld/mysqld.sock nice = 0 [mysqld] # # * Basic Settings # user = mysql pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock port = 3306 basedir = /usr datadir = /var/lib/mysql tmpdir = /tmp lc-messages-dir = /usr/share/mysql skip-external-locking # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address = 0.0.0.0 # # * Fine Tuning # key_buffer_size = 16M max_allowed_packet = 16M thread_stack = 192K thread_cache_size = 8 # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched myisam-recover-options = BACKUP #max_connections = 100 #table_cache = 64 #thread_concurrency = 10 # # * Query Cache Configuration # query_cache_limit = 1M query_cache_size = 16M # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error log - should be very few entries. # log_error = /var/log/mysql/error.log # # Here you can see queries with especially long duration #log_slow_queries = /var/log/mysql/mysql-slow.log #long_query_time = 2 #log-queries-not-using-indexes # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id = 1 #log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M #binlog_do_db = include_database_name #binlog_ignore_db = include_database_name # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI \u0026#34;tinyca\u0026#34;. # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem 这样保证win10的mysqlbench可以连接到虚拟机的mysql服务器\n但是还需要在mysql中设置对应用户并使之具有外部访问的权限和操作数据库的权限，我这里直接新建gao用户并赋予外部访问权限和操作权限：\nCREATE USER 'gao'@'%' IDENTIFIED BY 'password';\r%号表示可以被任意位置访问也就允许了远程ip访问 然后给gao用户授权，使其能随意操作数据库：\nGRANT all privileges ON * TO 'gao'@'%';\r可以在mysql这个数据库内的user表内找到自己添加的用户信息\nmysql 建表添加数据等操作 建立使用的数据库USR_DB\ncreatedatabasesUSR_DB;使用USR_DB\nuseUSR_DB;建表TBL_USR\ncreatetableTBL_USR(U_IDintprimarykeyauto_increment,U_NAMEchar(10),U_GENGDERchar(10));插入表项\ninsertTBL_USR(U_NAME,U_GENGDER)values(\u0026#39;gao\u0026#39;,\u0026#39;man\u0026#39;);选取表中所有数据显示\nselect*fromTBL_USR;删除与修改表项就涉及到安全模式，mysql默认运行在安全模式所以不能进行修改和删除表项的操作，所以需要取消安全模式然后操作，操作结束后需要再设置回安全模式\nsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;但这样操作是有问题的，如果这三部操作是原子的，是没有问题的，但不是原子的就引入不安全的因素，其他进程可能趁这个时候错误的篡改数据，所以将这三条合成一个过程来确保操作的安全性\nDELIMITER##createprocedureproc_delete_usr(inUNAMEchar(10))beginsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##上面这个DELIMITER ##的意思就是这一段使用##作为限制符号，也就是##框住的区域视为一个整体区域，用于指示过程的区域\n定义了过程之后就可以使用call调用过程达到安全的操作：\ncallproc_delete_usr(\u0026#39;gao\u0026#39;);同样的，修改也可以这样：\nDELIMITER##createprocedureset_img(inUNAMEchar(10),UIMGBLOB)beginsetSQL_SAFE_UPDATES=0;updateTBL_USRsetU_IMG=UIMGwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##callset_img(\u0026#39;gao\u0026#39;,IMG);这里的call set_img里的IMG其实在后面用于c的API调用,绑定statement之后传入的是一个char*的buffer接收的图像数据,然后设置到数据库里\n上面用到了U_IMG的column,这个列在建表时没有建立，需要使用添加column操作：\nALTERTABLETBL_USRcreatecolumnU_IMG;当然也可以使用以下操作删除：\nALTERTABLETBL_USRdropcolumnU_IMG;C api远程调用mysql 编写C程序来做到控制mysql数据库\n安装库： 先安装相关依赖和库才可以调用c api： 直接在虚拟机上\nsudo apt-get install libmysqlclient-dev;\r就安装成功了相关的c开发套件\n使用时需要在程序中包含头文件：\n#include\u0026lt;mysql.h\u0026gt;在编译相关程序时:\ngcc -o xxx xxx.c -I /usr/include/mysql -lmysqlclient 这里基本准备就完成\n基本操作: 首先需要连接mysql数据库，可以想到的就是建立一个mysql的handler，所以很自然的这里就需要一个特殊的struct，库为我们提供了MYSQL的数据类型：\nMYSQL mysql; 这样就建立了mysql这样一个handle，之后的所有操作都基于这个handle进行\n连接操作：\nif(NULL == mysql_init(\u0026amp;mysql)) { printf(\u0026#34;mysql init %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); return -1; } if(!mysql_real_connect(\u0026amp;mysql, king_db_server_ip, king_db_username, king_db_password, king_db_default_db, king_db_server_port, NULL, 0)) { printf(\u0026#34;mysql_real_connect: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } 可读性很高，这里不解释\n然后发送自己预定义好的sql语句：\n#define sql_insert \u0026#34;insert TBL_USR(U_NAME, U_GENGDER) values(\u0026#39;qiuxiang\u0026#39;, \u0026#39;woman\u0026#39;);\u0026#34;  #if 1 if(mysql_real_query(\u0026amp;mysql, sql_insert, strlen(sql_insert))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } #endif 一般情况下不进行其他操作了的话需要关闭mysql连接：\nmysql_close(\u0026amp;mysql); 以上就是简单的基于c api的mysql操作了\n其他操作 select基础\n如果需要从mysql服务器接收数据，比如select一些数据， 那么就需要一个容器来接受数据，这里使用MYSQL_RES来保存mysql的返回的结果:\n同样需要先query：\nif(mysql_real_query(mysql, sql_select, strlen(sql_select))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -1; } 然后接收数据\nMYSQL_RES *res = mysql_store_result(mysql); if(res == NULL) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -2; } 然后处理数据（打印出来） 想打印的话首先需要知道行列数然后再选取需要的数据来打印：\nint rows = mysql_num_rows(res); printf(\u0026#34;rows: %d\\n\u0026#34;, rows); int fields = mysql_num_fields(res); printf(\u0026#34;fields: %d\\n\u0026#34;, fields); 再根据获取的行列数循环fetch数据行然后打印特定行列的数据\nMYSQL_ROW row; while(row = mysql_fetch_row(res)) { int i=0; for(i=0; i\u0026lt;fields;++i) { printf(\u0026#34;%s\\t\u0026#34;, row[i]); } printf(\u0026#34;\\n\u0026#34;); } 这里就可以看到，数据转存到了MYSQL_ROW这个结构中\n最后释放接收的结果\nmysql_free_result(res); statement\n使用statement来存储或发送数据到mysql服务器\n整个流程： 1、初始化stmt，使用MYSQL* handle 2、 准备statement类似于query但是不执行 3、初始化绑定参数MYSQL_BIND param，因为要insert所以要初始化buffer用于指示insert数据 4、将参数绑定到stmt上 5、将buffer中的数据通过statement发送到mysql服务器？（不太清楚是否真的发送了） 6、执行statement 7、执行完毕关闭statement\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_insert_img, strlen(sql_insert_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND param = {0}; param.buffer_type = MYSQL_TYPE_LONG_BLOB; param.buffer = NULL; param.is_null = 0; param.length = NULL; ret = mysql_stmt_bind_param(stmt, \u0026amp;param); if(ret) { printf(\u0026#34;mysql_stmt_bind_param error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_send_long_data(stmt, 0, buffer, length); if(ret) { printf(\u0026#34;mysql_stmt_send_long_data error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } ret = mysql_stmt_close(stmt); if(ret) { printf(\u0026#34;mysql_stmt_close error: %s\\n\u0026#34;, mysql_error(handle)); return -6; } 以下是一个read的statement：\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_select_img, strlen(sql_select_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND result = {0}; result.buffer_type = MYSQL_TYPE_LONG_BLOB; unsigned long total_length = 0; result.length = \u0026amp;total_length; ret = mysql_stmt_bind_result(stmt, \u0026amp;result); if(ret) { printf(\u0026#34;mysql_stmt_bind_result error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_store_result(stmt); if(ret) { printf(\u0026#34;mysql_stmt_store_result error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } while(1) { ret = mysql_stmt_fetch(stmt); if(ret !=0 \u0026amp;\u0026amp; ret != MYSQL_DATA_TRUNCATED) { break; } int start = 0; while(start \u0026lt; (int)total_length) { result.buffer = buffer + start; result.buffer_length = 1; mysql_stmt_fetch_column(stmt, \u0026amp;result, 0, start); start += result.buffer_length; } } mysql_stmt_close(stmt); 可以看到多了fetch操作将result的buffer成员指向外部接受用的buffer的最新的接受位置，mysql_stmt_fetch_column进行了接收工作，MYSQL_BIND result也有了新的定义方式\n","date":"2020-09-09T00:00:00Z","image":"https://gao377020481.github.io/p/mysql/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mysql/","title":"Mysql基本知识"},{"content":"HTTP 实现http客户端程序\n基础 HTTP使用TCP连接\nHTTP报文：\n\r \r\n实现 域名到ip地址转换(dns) 直接调用api进行转换比较简单：\nchar * host_to_ip(const char* hostname) { struct hostent *host_entry = gethostbyname(hostname); if(host_entry) { return inet_ntoa(*(struct in_addr*)*host_entry -\u0026gt; h_addr_list); } return NULL; } host_entry存储了dns请求的接收，从中取出第一个ip地址并将点分十进制转换为字符串返回\n创建TCP套接字（建立连接） posix api创建\nint http_create_socket(char *ip) { int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in sin = {0}; sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr.s_addr = inet_addr(ip); if(0 != connect(sockfd, (struct sockaddr*)\u0026amp;sin, sizeof(struct sockaddr_in))) { return -1; } fcntl(sockfd, F_SETFL, O_NONBLOCK); return sockfd; } fcntl(sockfd, F_SETFL, O_NONBLOCK);这个函数用于设置该套接字io为非阻塞\n通过套接字向目标网站请求资源（select）\nchar * http_send_request(const char *hostname, const char *resource) { char *ip = host_to_ip(hostname); // \tint sockfd = http_create_socket(ip); char buffer[BUFFER_SIZE] = {0}; sprintf(buffer, \u0026#34;GET %s %s\\r\\n\\ Host: %s\\r\\n\\ %s\\r\\n\\ \\r\\n\u0026#34;, resource, HTTP_VERSION, hostname, CONNECTION_TYPE ); send(sockfd, buffer, strlen(buffer), 0); //select  fd_set fdread; FD_ZERO(\u0026amp;fdread); FD_SET(sockfd, \u0026amp;fdread); struct timeval tv; tv.tv_sec = 5; tv.tv_usec = 0; char *result = malloc(sizeof(int)); memset(result, 0, sizeof(int)); while (1) { int selection = select(sockfd+1, \u0026amp;fdread, NULL, NULL, \u0026amp;tv); if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread)) { break; } else { memset(buffer, 0, BUFFER_SIZE); int len = recv(sockfd, buffer, BUFFER_SIZE, 0); if (len == 0) { // disconnect \tbreak; } result = realloc(result, (strlen(result) + len + 1) * sizeof(char)); strncat(result, buffer, len); } } return result; } select部分： 首先根据套接字初始化fread来监听io，如果有消息到来就置为1，调用select函数： select(sockfd, \u0026amp;rset, \u0026amp;wset, *eset, *tv); \u0026amp;rset位置表示读监听io \u0026amp;wset位置表示写监听io \u0026amp;eset位置表示错误监听io（断开或者其他） tv为轮询间隔时间 select函数内部轮询监听这几个io，有置1就说明有信息需要处理，就返回然后处理信息 断开连接的话返回0，所以if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread))可以有效控制连接断开的break 正常时返回收到的结果result\n附main函数\nint main(char argc, char*argv[]) { if(argc \u0026lt;3) { return -1; } char *response = http_send_request(argv[1], argv[2]); printf(\u0026#34;response: %s\\n\u0026#34;, response); free(response); return 1; } ","date":"2020-09-09T00:00:00Z","image":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/florian-klauer-nptLmg6jqDo-unsplash_hu595aaf3b3dbbb41af5aed8d3958cc9f9_13854_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/","title":"简易http客户端(C posix API)"}]
[{"content":"面向对象带来的性能损耗 关于对象的构成可以参考之前的一篇博客，C++对象\n 缓存开销  数据被组织在对象内，然后对象又有一些额外的内存开销，比如虚表等。\n方法开销  面向对象有很多的小函数，函数调用耗时，小的区块也限制了编译器优化的空间，虚函数机制多次跳转也有损耗。\n构造开销  构造函数开销大，有可能要处理虚继承虚函数的情况。\nJAVA中：\n 在堆上分配空间，调用类的构造函数，初始化类的字段 对象状态被垃圾收集器跟踪，可能发生垃圾回收 在使用调试器创建对象时，你可以在一定程度上看到这一点。  **C++**中：\n 拷贝构造经常发生（传参赋值返回），JAVA中都是引用就不会潜在发生。 构造与解构在继承体系中都是链状的，call的开销也很大  异常  这个可以参考之前写的异常处理，讲到try catch的实现，里面其实就是很多的跳转，还是非局部的，也就是很像function call。消耗当然大了。\n优化 Method In-lining 编译器很难自己去in-lining，c++的in-line关键字也只是鼓励编译器尝试in-line函数。但还是多用inline关键字。\n一般对于带循环的小函数编译器不会inline它，所以建议用重复的几行来替代小循环。\n私有，final的方法很可能被inline但是虚函数从不会被inline因为需要在运行时间接调用\nnested template：\ntemplate \u0026lt;template \u0026lt;typename, typename...\u0026gt; class V, typename... Args\u0026gt; void print_container(V\u0026lt;Args...\u0026gt; \u0026amp;con) 这需要C++11支持，template \u0026lt;typename, typename\u0026hellip;\u0026gt; class V是第一个模板参数，他是一个有typename, typename\u0026hellip;为模板参数的类，它的名字叫V，typename\u0026hellip; Args指后续的模板参数，数量任意多，Args就是V里的模板参数，需要在这里指示出来。这是C++11的写法，可以让编译器自动推导传入类型模板参数的数量和类型。\n举个例子，\nvector\u0026lt;vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026gt;的参数传进去，推导出来就是这个： void print_container\u0026lt;std::vector, std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;, std::allocator\u0026lt;std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026gt;\u0026gt;(std::vector\u0026lt;std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026gt; \u0026amp;con) Encapsulation 意思就是把性能差的部分拿出来单独放到一个区域中或类中，在辅助上硬件比如FPGA,GPGPU，可以达到不错的加速效果。\n","date":"2022-03-08T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/283_hu5ca3888a09b3a06cbac826e342e11bf1_4506406_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%8C%96/","title":"对象优化"},{"content":"最近在看HPC领域分布式文件系统，发现代码里用到mercury来提供RPC功能，决定还是循序渐进先把这个框架学习一下。 \u0026lt;主要是报的一些关于mercury的错我看不懂*-*\u0026gt;\nArchitecture Network Abstraction Layer 这一层是中间层，向上给Bluk layer和RPC layer提供一致的interface，向下采用插件机制适配不同的网络协议。\n可提供的功能：target address lookup, point-to-point messaging with both unexpected and expected messaging（就是阻塞和非阻塞）, remote memory access (RMA), progress and cancelation.\n初始化 首先需要初始化接口并选择底层的插件，之前的ofi就是一种网络插件，大意是指与网卡交互所使用的协议方法的集合或者说一层。\n初始化 初始化 初始化 初始化 Mercury RPC Layer Mercury Bulk Layer ","date":"2022-02-12T00:00:00Z","image":"https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/430_hu5d968a7e44909eeacdbd0844a8dbe990_11030818_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mercury-rpc-framework-for-hpc/","title":"Mercury初探"},{"content":"最近在看GekkoFS，打算从理论到实现来试试能不能记下来这个分布式文件系统的实现。\n他的亮点主要在于使用了系统调用拦截来直接在mount的目录上拦截相应的系统调用。\n使用时还用到了一个比较少见的指令: LD_PRELOAD=\u0026lt;install_path\u0026gt;/lib64/libgkfs_intercept.so cp ~/some_input_data \u0026lt;pseudo_gkfs_mount_dir_path\u0026gt;/some_input_data 这个指令的意思是提前加载某个动态库（libgkfs_intercept.so），使用的时候将拦截的动态库提前加载进来，就可以在进入glibc的文件操作之前转向别的路径。\n概述 node-local burst buffer file system.\n 客户端GekkoFS client library  客户端是以库的形式提供的给HPC应用程序使用的。GekkoFS既没有实现基于fuse用户态文件系统的客户端，也没有实现VFS kernel的内核态的客户端。而是通过可以库的形式，截获所有文件系统的系统调用：如果是GekkoFS文件系统的文件，就通过网络转发给Gekkofs 服务端。如果不是GekkoFS文件系统，就调用原始的glibc库的系统调用。\nServer端：GekkoFS daemon  Sever端通过本地的rocksdb来保存文件系统的元数据。 通过本地文件系统来保存数据。数据被切成同样大小的chunk（测试用512k）。\n在客户端访问文件时，通过hash的方法映射到多个Server节点上。\n总的来说这个架构还是很清晰的，附上论文里的图：\n\r\n重点在于其工程实现，希望能从对它的代码分析中学到些有用的知识。\n","date":"2022-02-11T00:00:00Z","image":"https://gao377020481.github.io/p/gekkofs/309_hu80f756db431420bf1003157b0f57b409_4015254_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/gekkofs/","title":"Gekkofs"},{"content":"Requirements  loop得有可以（ determinable (at run time)）确定的循环次数 loop里不能包含（inline不算）过程调用 loop里不能包含分支 迭代次数也得多 迭代之间最好也没有依赖性 理论上就算有依赖性也可以向量化但是不实用  现代向量处理器  依赖于大量的硬件单元而非流水线 有SIMD指令 大量依赖缓存，所以对齐很重要  几个x86 vec指令  MMX instructions  适用于 32,16 or 8-bit 整数类型的计算指令，有8 x 64-bit 的寄存器： MM0 … MM7\nSSE instructions  支持：64 \u0026amp; 32-bit floating point，64, 32,16 or 8-bit integer types， 有8 x 128-bit 寄存器： xmm0 … xmm7 (xmm8 .. xmm16 in 64-bit mode)\nAVX instructions  ymm0 … ymm15 寄存器相比于SSE拓展到256bits\nAVX2 AVX-512  AVX-512只在 Intel KNL and Skylake上有，更强力512-bits寄存器，zmm0 … zmm15，他分四部分：Foundation（扩展32，64指令），Conflict Detection Instructions（检测要vec的loop的冲突，尽可能让更多的loop能vec），Exponential and Reciprocal Instructions（KNL里的，能支持超越运算，咱也不懂。。）， Prefetch Instructions （KNL里的预取指支持）\nAMD和ARM的 AMD：\n256bits的AVX2已经支持，但是512的在下一代\nARM：\n大多数没有vec支持，通过 NEON instruction set有一些现代HPC处理器支持128bits，但是有SVE: Scalable Vector Extensions这个东西能给芯片制造商提供支持不同长度vec的指令（A64FX 512-bit SVE ）\n对齐 AVX在32字节对齐的地址上工作的最好\n所以一般需要告诉编译器对齐，并且连续的去访问内存\n这里说的都是intel上的，两步走：\n分配 动态内存 _mm_malloc, _mm_free\rfloat *a = _mm_malloc(1024*sizeof(float),64);\r静态内存 float a[1024] __attribute__((aligned(64)));\r多维数据对齐 使用_mm_malloc分配多维数组的话，可能出现问题：\n\rfloat* a = _mm_malloc(16*15*sizeof(float), 64);\rfor(i=0;i\u0026lt;16;i++){\r#pragma vector aligned\rfor(j=0;j\u0026lt;15;j++){\ra[i*15+j]++;\r}\r}\r比如这里实际上intel这个函数做了一些其他操作，尤其是在对齐和向量化的前提下，内层循环是15层，编译器知道内层要vectorize，对齐是64字节也就是8个float，内层的15次循环要做512bit向量化的话就会出问题的，因为512bit对应64字节，内层16次循环的话这个就比较完美。抛开这种当作二维用的情况，一维情况下从内存分配器那里就特殊处理了末尾，比如这个16*15的刚好是8的倍数不存在末尾，但是你这样用循环就存在了末尾：7个作为一个末尾，可是编译器又不知道你这样搞，他还是当你单层循环，向量化的程序就会出现问题，可能产生错误的结果\n通知编译器   #pragma vector aligned 放在loop前，告诉编译器里面data是对齐的\n  __assume_aligned(a, 64); 放在循环内都可以，告诉编译器a这个array是对齐的\n  __assume(n1%16==0); 可能还需要告诉编译器这个loop scalars的属性，来个例子就知道了：\n  __declspec(align(64)) float X[1000], X2[1000]; void foo(float * restrict a, int n, int n1, int n2) { __assume_aligned(a, 64); __assume(n1%16==0); __assume(n2%16==0); for(int i=0;i\u0026lt;n;i++) { // Compiler vectorizes loop with all aligned accesses  X[i] += a[i] + a[i+n1] + a[i-n1]+ a[i+n2] + a[i-n2]; } for(int i=0;i\u0026lt;n;i++) { // Compiler vectorizes loop with all aligned accesses  X2[i] += X[i]*a[i]; } } openmp的指令： #pragma omp simd aligned(a:64)  这样每个线程就可以去均分并操作整齐的数据\n__declspec(align(64)) float X[1000], X2[1000]; void foo(float * restrict a, int n, int n1, int n2) { int i; __assume(n1%16==0); __assume(n2%16==0); #pragma omp simd aligned(X:64,a:64)  for(i=0;i\u0026lt;n;i++) { X[i] += a[i] + a[i+n1] + a[i-n1]+ a[i+n2] + a[i-n2]; } #pragma omp simd aligned(a:64)  for(i=0;i\u0026lt;n;i++) { X2[i] += X[i]*a[i]; } } 检查编译器是否向量化程序 CCE: -hlist=a\nGNU: -fdump-tree-vect-all=Intel: -opt-report3\nAMD/Clang:-Rpass-analysis=.*\n使用perf来找线索\n充分配合编译器向量化   尽量消除迭代之间的依赖性（尽量多的使用循环的index而非一个外部变量来在loop中运算）：\n  将分支移出循环，if\n  #pragma ivdep 告诉编译器循环是独立的无依赖的\n  restrict 告诉编译器这个变量或者数组是唯一的\n  注意对齐\n  确保循环足够大，并确定循环大小\n  Gathers and Scatters 数据的访问模式不是均步的，他也有可能可以向量化。\n首先说明，这功能是基于处理器指令的，处理器没实现的话，就没法用。KNL有专门的Gathers and Scatters指令，但是还是比对齐的数据损耗更大。\n以网上找的一个人写的SIMD的库的教程来说明，我觉得有实际例子会比较好：\nUME::SIMD\n三种情况；\n 跨步访问，一次跨3个元素访问下一个 索引访问，以新计算的偏移量访问数组元素 改组访问，以不同的顺序访问元素  跨步访问 首先明确一点，gather是load的超集。\n但是load效率高，速度快，所以还是多使用load来将连续内存搞到寄存器里。如果可以通过修改数据结构和代码的方式做到那就尽量去做，少使用gather。\nfloat a[LARGE_DATA_SIZE]; uint32_t STRIDE = 8; ... for(int i = 0; i \u0026lt; PROBLEM_SIZE; i+=8) { SIMDVec\u0026lt;float, 8\u0026gt; vec; // Note that we have to scale the loop index.  int offset = i*STRIDE; // \u0026#39;load\u0026#39; the data to vec.  vec.gather(\u0026amp;a[offset], STRIDE); // do something useful  vec += 3.14; // store the result at original locations  vec.scatter(\u0026amp;a[offset], STRIDE); } 索引访问 float a[LARGE_DATA_SIZE]; int indices[PROBLEM_SIZE]; uint32_t STRIDE = 4; ... for(int i = 0; i \u0026lt; PROBLEM_SIZE; i+=8) { SIMDVec\u0026lt;float, 8\u0026gt; vec; // Here we are using precomputed indices,  // but they can be computed on-the-fly if necessary.  SIMDVec\u0026lt;uint32_t, 8\u0026gt; indices_vec(\u0026amp;indices[i]; // \u0026#39;load\u0026#39; the data to vec.  vec.gather(\u0026amp;a[0], indices_vec); // do something useful  vec += 3.14; // store the result at original locations  vec.scatter(\u0026amp;a[0], indices_vec); } Masking and Blending Mask 可以被认为是是一个描述分支的掩码数组，通过它和特殊的指令：blend， 可以将内含分支的loop向量化掉：\nbefore:\nfor (i = 0; i \u0026lt; N; i++) {\rif (Trigger[i] \u0026lt; Val) {\rA[i] = B[i] + 0.5;\r}else{\rA[i] = B[i] - 0.5;\r}\r}\rafter:\nfor (i = 0; i \u0026lt; N; i+=16) {\rTmpB= B[i:i+15];\rMask = Trigger[i:i+15] \u0026lt; Val\rTmpA1 = TmpB + 0.5;\rTmpA2 = TmpB - 0.5;\rTmpA = BLEND Mask, TmpA1, TmpA2\rA[i:i+15] = TmpA;\r} 看代码就知道啥意思了\n再记一下arm的一些支持 ARM的向量化指令功能叫做SVE:\n  SVE is vector length independent\n Allows hardware to be created and used between 128-bits and 2048-bits Current processors using it have 512-bit vectors    Programming approach allows executable to scale dynamically to available vector length\n  Designed to help improve auto-vectorization\n Instructions to support speculative vectorization to allow uncounted loops to be vectorized. Instructions to make it easier to vectorise outer loops, working with dependencies    Gather-load and scatter-store\n Loads a single vector register from non-contiguous memory locations.    Per-lane predication\n Operate on individual lanes of vector controlled by of a governing predicate register.    Predicate-driven loop control and management\n Eliminate loop heads and tails and other overhead by processing partial vectors.    Vector partitioning for software-managed speculation\n First-fault vector load instructions allow vector accesses to cross into invalid pages.    Extended floating-point and bitwise horizontal reductions\n In-order or tree-based floating-point sum, trade-off repeatability vs performance.    ","date":"2022-02-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/316_hu2053fdeb7f8648c516bbfdc346f4d20d_4844619_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%90%91%E9%87%8F%E5%8C%96/","title":"向量化"},{"content":"Spectre\u0026amp;Meltdown\n这是一种利用现代处理器特性来“窃取”内存中重要信息的漏洞。\nMeltdown  诱使CPU在乱序执行的基础上将高权限的data信息放置于cache中  首先有一个数组，直接访问它的第data*4096个index处的元素 这样这个index处的元素会被放进cache中   循环遍历这个数组，当遍历到第data*4096个index处元素时，载入速度明显变快，这就说明这里是当时载入cache的元素 取到data*4096这个index就取到了data  假设data是一个内核内存空间内的数据，我们就get到了机密的内核数据，这都依赖于cpu的乱序执行：\nexception(dont have priority to access))\raccess(array[data*4096])\r乱序执行使得在指令生效前，access运行在了exception之前，虽然指令未生效（寄存器状态等未变），但cache内却有了array[data*4096]这个元素\nSpectre Spectre基本原理与Meltdown类似，但他更强\nMeltdown一旦被从根本上避免就无法使用，比如内核和用户使用不同的页目录寄存器等\nSpectre并非依赖于乱序执行，而是依赖于分支预测。\n分支预测也会使cpu提前跑一下cpu认为正确的分支，尽管他不一定真的是接下来要执行的，同样会在cache里留下痕迹。\n但他要求代码有如下形式：\nif(index1\u0026lt;array_a_size) {\rindex2=array_a[index1];\rif(index2 \u0026lt; array_b_size);\rvalue = array_b[index2];\r}\r通过控制index1的长度，让array_b的特定下标的数据Cacheline被点亮，如果有办法访问一次array_b的全部内容，我们就可以窃取到index1这个值。\n","date":"2022-02-08T00:00:00Z","image":"https://gao377020481.github.io/p/spectremeltdown/325_hube714498331873c40184a998a513a196_5002269_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/spectremeltdown/","title":"Spectre\u0026Meltdown"},{"content":"从内存结构的角度来优化我们的程序。\nVector temporaries REAL V(1024,3), S(1024), U(3)\rDO I=1,1024\rS(I) = U(1)*V(I,1)\rEND DO\rDO I=1,1024\rS(I) = S(I) + U(2)*V(I,2)\rEND DO\rDO I=1,1024\rS(I) = S(I) + U(3)*V(I,3)\rEND DO\rDO J=1,3\rDO I=1,1024\rV(I,J) = S(I) * U(J)\rEND DO\rEND DO\r----------------------------------------\u0026gt;\rREAL V(1024,3), S, U(3)\rDO I=1,1024\rS = U(1)*V(I,1) + U(2)*V(I,2) + U(3)*V(I,3)\rDO J=1,3\rV(I,J) = S * U(J)\rEND DO\rEND DO\r将运算组织成近似向量的形式，编译器就很容易借助这种优势来将代码优化为向量的运算。\nHybrid REAL V(1024,3), S(8), U(3)\rDO K=1,1024,8\rDO I=0,7\rS(I) = U(1)*V(I+K,1) + U(2)*V(I+K,2) + U(3)*V(I+K,3)\rDO J=1,3\rV(I+K,J) = S(I) * U(J)\rEND DO\rEND DO\rEND DO\r这种版本的代码可以increase locality\nSubroutine merging 两个过程（可以是循环）有相似的迭代空间并且数据array只用来在他们之间传输数据，也就是他们之间这个array上没有其他操作。那就可以合并，合并的好处是很多的。\nIf two routines have the same iteration space and an data array only exists to pass data between them then consider merging\nreduce cache miss conflict code:\nREAL A(1024), B(1024), C(1024), X\rCOMMON /DAT/ A,B,C ! Contiguous\rDO I=1,1024\rA(I) = B(I) + X*C(I)\rEND DO\r每一个循环里A,B,C的I位置都映射到一个set里（当然是在直接映射cache下，如果multi-way那就好很多），每次都发生三次conflict\nCache padding REAL A(1024),B(1024),C(1024),T(8),T2(8),X\rCOMMON /DAT/ A,T,B,T2,C\rDO I=1,1024\rA(I) = B(I) + X*C(I)\rEND DO\rPadding 是常用技巧，不说了\nArray extension REAL A(1032,4), B(1024)\rDO I=1,1024\rDO J=1,4\rB(I) = B(I) + A(I,J)\rEND DO\rEND DO\r这个也可以叫做padding，是A自己的不同维度的vector发生coflict，把A自己padding一下就行了\nLoop re-ordering REAL A(1024,4),T(8), B(1024)\rDO J=1,4\rDO I=1,1024\rB(I) = B(I) + A(I,J)\rEND DO\rEND DO\r直接按列访问就没有A上的冲突了，B与A的冲突就依赖于T（8）的引入来消除。\nIndex re-ordering \rREAL A(4,1024), B(1024)\rDO I=1,1024\rDO J=1,4\rB(I) = B(I) + A(J,I)\rEND DO\rEND DO\r这样一来A和B的冲突也没了，但这种改变影响的是所有用到A的地方，一般需要在全局来修改，用脚本来改。\nIntroduce temporaries REAL A(1024,4), B(1024),T1,T2,…\rDO I=1,1024,8\rDO J=1,4\rT1 = A(I,J)\rT2 = A(I+1,J)\r…\rB(I) = B(I) + T1 +T2 …\rEND DO\rEND DO\r留一下，这里的优化感觉很有限，这句也没理解：\nOnly works if unroll is multiple of block size\n关于写操作的优化 cache 先说一下cache：\n写回，写穿透\n还有：\nwrite allocate ：写区域不在cache，就把它搞到cache里写\nNo write allocate： 写区域不在cache，就直接修改cache以下的内存层级\nWrite stalls CPU在寄存器里的值被拷贝回去之后才能继续进行操作，这就引入了stall。\n引入write buffer： 这是硬件的解决方案，写操作时直接写入write buffer，一般大小和一级缓存差不多，且如果比多个字长的话就可以合并这个write到一个单独的block write里去。\nArray initialization 初始化array的开销会比较大，尤其在array的size比较大的时候更明显。\n 使用特殊硬件配合编译器指令来做初始化 将初始化操作和第一次使用的loop放到一起（很明显这会导致一些错误，因为你无法保证第一次是什么时候）  Writes and vectorisation SIMD/Vector store operations usually work best when writing well-aligned contiguous blocks of data\n这和write buffer的使用很类似，所以只需要遵循向量化的数据访存就能顺带做到适应write buffer的优化。\nprefetch 预取数据到cache中\n很多处理器都有这个支持，编译器也可以做一些工作\n最好的情况是有一些指令可以给程序员自己使用\nWrite-allocate caches可能有一些清零cache line的指令\nvoid __builtin_prefetch (const void *addr, int rw, int locality)\rhttps://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html\rStreams 一般预取到cache里能优化的都是连续序列的cache miss。\n处理器还有一些stream buffer：\n 每一个都保存有active stream的地址 这个地址会随着data的load移动，指向未load的data 由一系列的连续地址的cache miss构建  如果一个loop需要比硬件支持的多的stream，那么就没有cache预取发生因为硬件流引擎将会不断地被重置到新的流上。\n这个还得回来研究一下，其实没太理解它到底是啥。\nNon-temporal operations 直接操作内存，忽视cache\n#pragma vector nontemporal\rvoid __builtin_nontemporal_store(T value, T *addr);\rT __builtin_nontemporal_load(T *addr);\ralignment load/store指令的大小是不一样的，对齐做好的话可以一次load就取出整数个元素。\nCompilers 做了什么？\n 内置类型和局部变量的对齐是编译器的强项。 参数指针和结构体变量编译器一般就无能为力了  如果编译器没办法确定对齐策略：\n 不用对齐的指令（带来低效率） 通过一些动态测试，测出程序的对齐大小  优化：\n 编译器提供对齐相关的指令 需要确保内存数据有一个良好的排列 malloc：只有最大的内置类型可以保证良好的对齐。64bit 有的话就不用malloc，换成其他提供的带对齐的内存分配api  Pointer aliasing 考虑一下，编译器需要：\n 当通过指针写，重新加载所有指针指向的值 当通过指针读，需要确保这个操作前的操作都可见。 如果有类似c++的unique指针就比较好  C语言里指针是unrestricted的，所以严重影响了性能。\n防止指针重叠，可以采用类似引用计数的方法（Explicit use of local scalar temporaries）\n分配在堆上的内存，一般都是比较随机的，除非一次分配很多一样的元素。 cache miss会少一些，但是因为缺少了局部性，TLB的miss会增多（缺页）。\n一些动态类型比如双向链表，它里面指针占的内存也很多，存在浪费。且会带来更多的cache miss。\n","date":"2022-02-04T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/430_hu5d968a7e44909eeacdbd0844a8dbe990_11030818_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/","title":"内存优化"},{"content":"讨论一些编译器会做的优化，这些优化一般不需要我们自己来做。\n但是我们讨论时从源码角度来模拟，实际上编译器是在中间代码（IR）层进行优化的。\n分四部分：\nIR optimisations Basic optimisations Constant folding 将一些变量转换为常量，这有助于减少内存的访问。 且常量相关的求值是可以在编译阶段就确定，不必等待运行阶段。\n转换前：\ninteger, parameter :: n=100\rdo i=1,n\r....\rend do\r转换后：\ndo i=1,100\r....\rend do\rAlgebraic simplifications 简化算术运算，包括了运用结合，交换和分配律等：\n(i-j)+(i-j)+(i-j) \u0026mdash;\u0026mdash;\u0026gt; 3*i - 3*i\n但是要注意浮点数操作，它不遵循结合律：\n1.0 + (MF - MF) = 1.0\n(1.0 + MF) - MF = 0.0\n这个例子里MF比1.0大，1.0在和MF结合后会被舍去，所以第二个结果出来就是0.0了，这是因为浮点运算会先算出精确值然后舍去多余位，所以结合律失效。\nCopy propagation 拷贝出来的变量在后续的使用中可以被原来的变量替代，那就少用一个寄存器。\nx = y\rc = x + 3\rd = x + y\r--------\u0026gt;\rx = y\rc = y + 3\rd = y + y\r这里x这个变量就不需要了。\nConstant propagation 一个常量的值，在后续的使用中可以直接被一个常量替换掉。\nx = 4\rc = x + 3\rd = x + y\r----------------\u0026gt;\rx = 4\rc = 4 + 3\rd = 4 + y\rRedundancy elimination Common subexpression elimination (CSE) 有重复使用的表达式可以被替换为一次表达式运算，然后存储他的结果，后续的重复表达式运算直接使用第一次得到的结果就可以。\na = b + (c - 2)\rd = 4 * b\re = d + (c - 2)\r------------\u0026gt;\rt1 = c - 2\ra = b + t1\rd = 4 * b\re = d + t1 Loop invariant code motion 在所有循环中结果都一样的表达式直接放到循环外来计算。\ndo i = 1,n\rdo j = 1,n\ra(j,i)=2*b(i+1)+j-(m*5)\rend do\rend do\r-------------------\u0026gt;\rt1 = m*5\rdo i = 1,n\rt2 = 2*b(i+1)\rdo j = 1,n\ra(j,i)=t2 + j - t1\rend do\rend do\rAlgebraic simplification in redundancy elimination 再做一下算术运算的简化。\ndo i=1,n\ra=b+i\rc=a-i\rd=a\rend do\r-------------\u0026gt;\rdo i=1,n\ra=b+i\rc=b\rd=a\rend do\rDead code elimination 去掉结果从不使用的代码。\nloop optimisations Strength reduction 简化计算，将每次循环的计算强度降低。尽量将基于迭代变量的运算转化为单纯的加法递增。\nfor (i=0;i\u0026lt;n;i++){\rj = 3*i+2;\ra[j] = b[i];\r}\r---------------\u0026gt;\rj=2;\rfor (i=0;i\u0026lt;n;i++){\ra[j] = b[i];\rj+=3;\r}\rInduction variable removal Induction variable归纳变量，指随着迭代连续变化的变量。\n有可能的话，使用一个（迭代变量）来表示所有的归纳变量，减少寄存器的使用。\nfor (i=0;i\u0026lt;n;i++){\rj = 3*i+2;\ra[j] = b[i];\r}\r----------\u0026gt;\rj=2;\rfor (i=0;i\u0026lt;n;i++){\ra[j] = b[i];\rj+=3;\r}\rBounds checking elimination int a[n];\rfor (i=lo;i\u0026lt;hi;i++){\ra[i] = i;\r}\r拿这个举例子，每次循环都需要检查i是否越界，这很耗费时间，其实只需要在一开始检查一下lo和hi是否满足：\nlo \u0026lt; n\rhi \u0026lt; n\rlo \u0026lt; hi\r就行。\nProcedure call optimisations Inlining 内联，即提示编译器去将函数以类似宏定义的形式展开在调用处。\n可能使代码量增大，使用寄存器的数量也会增多。\nExpansion Inlining是编译器在IR这里做的事情，但是expansion是在汇编/机器码这里做的事情。\n它是将函数调用改为一系列指令的组合，层次更低，可以利用一些特别的指令序列。\nInstruction scheduling 这是编译器应该在source level做的优化\n指令可以重排序，只要维持寄存器的依赖图不变就行了。\n而且有一些指令是不能pipeline的比如sqrt，那就需要在sqrt计算和结果之间插入一些指令来充分利用cpu。\n目标：\n 减少执行的指令 减少流水线停顿（data hazards, control hazards, structural hazards） 使用多发射(VLIW, superscalar)  Branch scheduling 分支会导致流水线停顿，两种方案：\n delayed branches（编译器来做，引入分支延迟槽，流水线中，分支指令执行时因为确定下一条指令的目标地址（紧随其后 or 跳转目标处？）一般要到第 2 级以后，在目标确定前流水线的取指级是不能工作的，即整个流水线就“浪费”（阻塞）了一个时间片，为了利用这个时间片，在体系结构的层面上规定跳转指令后 面的一个时间片为分支延迟槽（branch delay slot）。位于分支延迟槽中的指令总是被执行，与分支发生与否没有关系。这样就有效利用了一个时间片，消除了流水线的一个“气泡”。） 分支预测(依赖硬件Reorder Buffer，用于rollback结果)  编译器就找一些可以放到分支延迟槽的指令，把它们放进去就更有效地利用了流水线，消除了stall。当然可能还要尝试找一些可以忽视分支的指令。\nLoop unrolling 就是展开循环，因为小的循环中花费在分支上的代价太大，把他们展开，可能耗费更多寄存器但是减少了stall。\n同时一些特殊指令也可以被使用进去。\n有几点需要注意：\nunroll factor选择很重要，大了会用完寄存器，数量的选择可以考虑cache line，提高缓存的命中\nOuter loop unrolling： 在unroll的时候对于多层循环，尽量去unroll外层的循环，这有助于提升代码的局部性\n就类似内存读取时读取连续的值一样，有助于缓存命中\nVariable expansion 用于解除展开循环中的数据依赖：\nfor (i=0,i\u0026lt;n,i+=2){\rb+=a[i];\rb+=a[i+1];\r}\r---------------------\u0026gt;\rfor (i=0,i\u0026lt;n,i+=2){\rb1+=a[i];\rb2+=a[i+1];\r}\rb=b1+b2;\rRegister renaming 在一个block中寄存器可能被重用，这会带来不必要的依赖。\n使用多个寄存器可以提供更多的scheduling flexibility、\n有些CPU在硬件上实现了寄存器raname，但是这些都是对编译器不可见的。\nadd %f2,1,%f1\rst [%o1],f1\radd %f3,2,%f1\rst [%o2],f1\r----------------\u0026gt;rename\radd %f2,1,%f1\rst [%o1],f1\radd %f3,2,%f27\rst [%o2],f27\r----------------\u0026gt;reschedule\radd %f2,1,%f1\radd %f3,2,%f27\rst [%o1],f1\rst [%o2],f27\rSoftware pipelining 软件层面实现流水线来解决循环，看例子：\n\rfor (i=0;i\u0026lt;n;i++){\ra(i) += b;\r}\r------------------------\u0026gt;\rfor (i=0;i\u0026lt;n;i++){\rt1 = a(i); //L i\rt2 = b + t1; //A i\ra(i) = t2; //S i\r}\r---------------------------\u0026gt;\r//prologue\rt1 = a(0); //L 0\rt2 = b + t1; //A 0\rt1 = a(1); //L 1\rfor (i=0;i\u0026lt;n-2;i++){\ra(i) = t2; //S i\rt2 = b + t1; //A i+1\rt1 = a(i+2); //L i+2\r}\r//epilogue\ra(n-2) = t2; //S n-2\rt2 = b + t1; //A n-1\ra(n-1) = t2; //S n-1\r这样搞对向量化的架构很有好处（superscalar and VLIW），因为：\n• large loop body • few dependencies between instructions • lots of scope for multiple issue\noptimisations for the memory hierarchy 这一层级的优化就很大可能要自己手动实现\nLoop interchange 多层循环，交换循环次数，这个很熟悉了，为了cache命中率：\n\rfor (j=0;j\u0026lt;n;j++){\rfor (i=0;i\u0026lt;m;i++){\ra[i][j]+=b[i][j];\r}\r}\r------------------------------\u0026gt;\rfor (i=0;i\u0026lt;m;i++){\rfor (j=0;j\u0026lt;n;j++){\ra[i][j]+=b[i][j];\r}\r}\rLoop reversal 交换循环的次序，从前到后，从后到前，可以利用在array比cache大一点的时候，在循环开始前array末尾刚存进cache，先从末尾开始访问，命中率高\nfor(i=0;i=\u0026lt;m;i++){\ra[i]+=b[i];\r}\r----------------------\u0026gt;\rfor(i=m;i\u0026gt;=0;i--){\ra[i]+=b[i];\r}\rLoop skewing for (j=0;j\u0026lt;n;j++){\rfor (i=0;i\u0026lt;m;i++){\ra[j]+=b[i+j]+1;\r}\r}\r--------------------------\u0026gt;\rfor (j=0;j\u0026lt;n;j++){\rfor (i=j;i\u0026lt;m+j;i++){\ra[j]+=b[i]+1;\r}\r}\r转化成这种形式，可以惠及其他的转化方式。这种转化本身一般不会带来直接的好处。\nUnimodular transformations Loop skewing，Loop reversal，Loop interchange都是Unimodular transformations\nLoop fusion 将俩循环合并：\nfor(j=0;j\u0026lt;n;j++){\ra[j]+=1;\r}\rfor(i=0;i\u0026lt;n;i++){\rb[i]=a[i]*2;\r}\r------------------------------------\u0026gt;\rfor(j=0;j\u0026lt;n;j++){\ra[j]+=1;\rb[j]=a[j]*2;\r}\r可能可以减少内存访问的次数，比如这里a的访问放到一起去了\n循环块加大了，对于scheduler的优化也有更大的发挥空间\n附上dalao写的：https://zhuanlan.zhihu.com/p/315041435\nLoop distribution Loop fusion的反向操作，一般用来减轻寄存器pressure\nLoop tiling for (i=0;i\u0026lt;n;i++){\rfor (j=0;j\u0026lt;n;j++){\ra[i][j]+=b[i][j];\r}\r}\r-------------------------\u0026gt;\rfor (ii=0;ii\u0026lt;n;ii+=B){\rfor (jj=0;jj\u0026lt;n;jj+=B){\rfor (i=ii;i\u0026lt;ii+B;i++){\rfor (j=jj;j\u0026lt;jj+B;j++){\ra[i][j]+=b[i][j];\r}\r}\r}\r}\r这个比较复杂，直接看dalao写的： https://zhuanlan.zhihu.com/p/292539074\nhttps://zhuanlan.zhihu.com/p/301905385\nArray padding 根据cache的映射规则，加上padding的话，连续访问的元素也许就不会造成大量的cache conflict\nfloat a[2][4096];\rfor (j=0;j\u0026lt;n;j++){\ra[1][j]+=1;\ra[2][j]*=2;\r}\r---------------\u0026gt;\rfloat a[2][4096+64];\rfor (j=0;j\u0026lt;n;j++){\ra[1][j]+=1;\ra[2][j]*=2;\r}\r","date":"2022-01-30T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/324_hu5345abdaa640012db8d08e2af4454d52_5473626_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96/","title":"编译优化"},{"content":"C/C++编译器指令 attribute((packed)) 会告诉编译器不要对齐，收紧结构，这样struct的大小就是成员大小的和。\nstruct xfs_agfl {\r__be32 agfl_magicnum;\r__be32 agfl_seqno;\ruuid_t agfl_uuid;\r__be64 agfl_lsn;\r__be32 agfl_crc;\r} __attribute__((packed));\rxfs里这个AGFL（allocation group free internal list） 就用到了这个东西。\n","date":"2022-01-03T00:00:00Z","image":"https://gao377020481.github.io/p/struct%E6%94%B6%E7%B4%A7%E7%BB%93%E6%9E%84%E7%BC%96%E8%AF%91%E5%99%A8%E6%8C%87%E4%BB%A4/592_hu3104676e7a9cf87da9314016ca136cc6_10729930_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/struct%E6%94%B6%E7%B4%A7%E7%BB%93%E6%9E%84%E7%BC%96%E8%AF%91%E5%99%A8%E6%8C%87%E4%BB%A4/","title":"struct收紧结构编译器指令"},{"content":"原文：System Software for Persistent Memory\n这篇文章分析了NVM出现的情况下，软件层面（主要是内存和文件系统）的一些变革和适配。14年的文章了，intel的人参与，显然在傲腾研发的时候发出来的。\n后面的nova是借鉴了这里的思路的，堆砖砌瓦，总觉得这种知识的传承和发掘很有趣。。。\n这篇文章搞了个PMFS，发掘了fs在NVM上的优化思路。（细粒度logging和大规模的页IO）\n背景 先来回顾一下PM发展，从一开始将NAND flash与DRAM混合使用（NVDIMM），掉电时放入NAND flash，上电再从NAND flash中加载出来到DRAM中。（电池+NAND Flash+DRAM）这样的做法，但是因为受限于NAND，这玩意还是只能block addressable。想要byte-addressable还得看NVM（PCM等做法）。对于PM主要两种思路：1. 扩展虚拟内存管理器来将PM当内存用2. 把PM当块设备用（FS里，ext4的做法）3.直接做个文件系统，跳出块设备的层来管理PM。\n于是就有了PMFS：这是个POSIX semantic的fs，有一些针对NVM的特殊优化。\nPMFS:\n API完善（POSIX-compliant file system interface），所以向后兼容老的应用程序。 轻量级文件系统，绕过VFS的块设备层，避免了繁杂耗时的内核发生的拷贝（page cache），过程调用等 优化的mmap，绕过page cache（需要先从disk-\u0026gt;SSD拷贝到memory-\u0026gt;DRAM），这是为了加速，但在NVM里就不需要，直接从NVM映射用户空间页表就可以，应用需要的时候直接通过虚拟内存就能寻址到  . PMFS also implements other features, such as transparent large page support [18], to further optimize memory-mapped I/O. \u0026mdash;有待研究\n挑战：\n 现在的内存管理器把PM作为一个memory用，因为cache（WB）的存在，写回是异步的，这个memory并不会及时更新，比如cache还未写回PM，就掉电，那数据还是要丢失，durable和reliability就无法保证。所以需要一个原语来完成这种强制写回。PM write barrier or pm_wbarrier这是一个硬件实现. 为了性能，PM被PMFS直接整个映射到内核地址空间，这就带来了stray writes的问题（就是kernel里出bug的话\u0026ndash;一般是内核驱动程序出错，指针写到我PM的这一块内核地址空间去了，发生了脏写）。一个解决办法是平时在cpu的页表里把PM设置为只读，需要写的时候先改成写，写完再改成只读。但你这样搞，来回变页表，TLB要一直更新 global TLB shootdowns，cost很大。所以intel的人实现了个utilize processor write protection control to implement uninterruptible, temporal, write windows（回头看看是啥）。 测试可靠性（主要是consistency），Intel的人开发了个东西：For PMFS validation, we use a hypervisorbased validation tool that uses record-replay to simulate and test for ordering and durability failures in PM software (§3.5) mmap直接用过于底层，PMLib这个东西封装了一下mmap，提供给app调用，更方便。  Architecture \r\n几种之前的方法来保证一致性和durable：\n mapping PM as write-through \u0026mdash;\u0026ndash; 带宽有限，代价太高 limiting PM writes to use non-temporal store instructions to bypass the CPU caches \u0026mdash;\u0026mdash;\u0026mdash;suffer from performance issues for partial cacheline writes, further discouraging general use 大意就是用起来不具有普适性\u0026ndash;不用cache还是不行，cache毕竟快 a new caching architecture for epoch based ordering（这个东西有点意思 要看看）【Better I/O Through Byte-Addressable, Persistent Memory】 \u0026mdash;\u0026mdash;- require significant hardware modifications, such as tagged cachelines and complex write-back eviction policies。 Such hardware mechanisms would involve non-trivial changes to cache and memory controllers, especially for micro-architectures with distributed cache hierarchies. 这个策略要硬件修改还要配套协议，太复杂，在一个分布式环境下，要对所有node都修改，也是挺麻烦的non-trivial  分析了一波，还是用WB策略加显示的强制cache写到PM上靠谱。但这种实现问题很显然：durable？掉电，会丢失数据。假设只用：clflush + sfence , it does not guarantee that modified data actually reached the durability point，这是为啥呢，内存控制器和处理器的锅：第一硬件的写并不是一次性完成的，内存管理单元把写请求入队就返回并认为完成了，实际他可能还在内存控制器（或processor）那里排队呢。第二内存控制器为了维护处理器的内存访问模型，很可能将数据缓存在一个buffer（就是写缓冲区）里，等满足访问模型要求的数据顺序保证了之后再处理。 对于一般的内存架构（DRAM SSD），因为DRAM本来就是volatile的，返回写成功代表写入DRAM，所以开发人员是知道的因为写入DRAM还是会掉电丢失，就通过别的方法想办法保持durable，但现在PM在DRAM的位置替代了它，当我们以为返回成功也就是数据写入我们认为数据写入PM（durable了时），其实他可能只是被processor或memory controller放到写缓冲区去了，也就是没写到PM，相当于骗了我们。 这就是个大问题，所以clflush + sfence还不够，得搞个新的原语。\npm_wbarrier这个硬件原语来了，这个原语有俩平行的层次：\n on-demand variant（可以理解为主动调用生效）同步的，主动调用来保证durable。 lazy variant（被动生效）异步的，掉电前自动完成保护操作。  当然这篇文章用的是on-demand variant\n使用优化的clflush，提升性能（weaker ordering on cache write back operations.然后用sfence来保证ordering），相比于之前的clflush（可以看看之前的为啥差：【Implications of CPU Caching on Byte-addressable Non-Volatile Memory Programming】【Mnemosyne: Lightweight Persistent Memory】）\n按顺序来：\n clflush sfence pm_wbarrier  关于编程难度：多用库\n对于特定的流式写操作：PMFS使用non-temporal stores\n磨损问题： 还是让硬件来支持吧，软件支持太复杂了。\nDesign\u0026amp;Implementation \r\n看眼图就知道了，这是一个对比。关键是，怎么实现的：\n这是很大的一部分，三个层面来说：\nOptimizations for memory-mapped I/O 首先：基于PM和CPU的micro架构来设计\n\r\n上图可知，metadata用B-树组织(best for sparse data)，PM里SB后面紧跟着LOG。然后是DataPage。LOG空间有限自然要定期清除。\nB-tree不止组织元数据，还组织file datapage。\n Allocator:  extent-based allocations(ext4) 组织数据的最小单位可以自由的extent到各种大小。\nindirect block-based(ext2)\nPMFS是page-based的，page大小支持各种processor的microarch(4KB, 2MB,1GB) 为了支持Transparent hugepage support( works by scanning memory mappings in the background (via the “ khugepaged ” kernel thread), attempting to find or create (by moving memory around) contiguous 2MB ranges of 4KB mappings, that can be replaced with a single hugepage)\n目前PMFS的内存分配和管理机制还是比较朴素的，就是把page连续放在一起来避免fragment。\n Memory-mapped I/O (mmap):  mmap直接把PM映射到user 的address space，user 可以直接访问PM。\n用的是最大size的page来做映射，这样page fault少，TLBmiss也少但粗粒度的page也有缺点（internal fragmentation）。 大的page产生的page table也小，page table在DRAM上，DRAM空间少于PM，所以就很合理。\n默认page 大小4K，可以两种方法修改：\n mount的时候确定，对于fixed or similar的size工作比较合适 Using existing storage interfaces to provide file size hints， communicate this to PMFS using either fallocate (to allocate immediately) or ftruncate (to allocate lazily). These hints cause PMFS to use 1GB pages instead of 4KB pages for the file’s data nodes.需要给linux GNU 的fallocate加一点功能来支持大页。  最后，只有mmap的使用不是COW的（MAP_SHARED或只读区域），才能使用大页，否则都是4K。\nConsistency . In PMFS, every system call to the file system is an atomic operation\n而用mmap来做的区域就需要程序员自己保证consistency了。\n目前主要就三种技术：\n copy-onwrite (CoW) journaling (or logging) and logstructured updates  COW（）和LS（WAL）都有写放大的问题，metadata的修改粒度很细，对于metadata用journaling比较合适。\n以减小写的量和同步的次数（pm_wbarrier的次数）来衡量cost。\n主要结论是logging at cacheline or 64-byte granularity最好。\nPMFS里用的是atomic in-place updates and finegrained logging来实现metadata的update， COW来做file data的update，三合一\n journaling for metadata :  主要有undo和redo两个实现方式。各有优劣：\nundo是基于steal的，也就是uncommitted的操作也会持久化，就需要基于undo log撤销一下。在PMFS里实现就需要在transcation里的每一个log entry之间使用同步原语确保undo log持久化。\nredo是基于unforce的，就是committed的操作不需要立即持久化，可能被缓存等，那么恢复时就需要基于redo来重新committ一下。在PMFS里实现只需要在在transaction committed之前（准确的说是最后一个committed log之前）确保redo log持久化，这里需要两次同步原语。但有一个问题是，如果很多事务都没被持久化，比如现在有4个事务已经提交了但是都没有持久化，那在PM里就无法体现事务的修改，就需要去redo log里找信息，也就是说，所有对data的read操作都需要先确保data是最新的，对于journaling来说也就是要先从PM里复制出来，再对copy的data redo log过一遍，再做读操作。如果积累的未提交事务较多，后续事务里的read操作就产生很多redo的load。尤其是当这个data粒度很小，需要redo的log可能很多，因为PMFS用journaling做metadata的consistency，metadata很小，那就不用redo。\n虽然用了undo，但是在log很多但其中涉及修改的很少的时候，redo的同步开销还是导致性价比太低。作者说日后会分析。\n这东西叫做PMFS-Log符合undo语义，放张图看看，这图后面分析实现的时候还会用：\njournaling也不是一直用的，PMFS尽量使用atomic in-place updates操作能用的小数据操作，这样负担比较小，这个东西是基于processor的功能。\n Atomic in-place updates  PMFS的64，16bit原子操作有自己的优化：\n 8-byte atomic updates  用于update an inode’s access time on a file read.\n处理器都支持这个操作应该是CAS。\n16-byte atomic updates:  处理器using cmpxchg16b instruction (with LOCK prefix)来支持\n用处比较多，如： atomic update of an inode’s size and modification time when appending to a file.\n64-byte (cacheline) atomic updates:  The processor also supports atomic cacheline (64-byte) writes if Restricted Transactional Memory (RTM) is available 处理器得支持RTM这个东西，就能支持64位原子操作。 步骤是：用XBEGIN开启RTM事务然后修改然后用XEND关闭RTM事务。后续的clflush会把cache刷回PM。处理器不支持RTM的话PMFS就用上面的fine-grained logging来搞了。\nPMFSLog： fixed-size circular buffer 固定大小的换新缓冲区，data的图里也能看出来\n\r\ntrans_id：这个东西标识了这个log所属的事务，因为每个事务都有自己专属的trans_id\ngen_id: 需要和metadata的gen_id相同，不然就标识着这个log无效，系统恢复时判断log就看这个，系统恢复用过log之后就会把log的gen_id增加（就相当于无效化这个log了，环形的覆盖时也会给gen_id加一）\nlog有效性： 有一个情形：log的有效位被设置，但是log的内容还没来得及设置就掉电了。那么这个log就出现了错误。\n怎么解决呢？很朴素：就是确保log的内容先持久化到PM里，再确保log的有效位持久化到PM里，维持这个顺序就行。\n可是有两点会影响这个顺序的保持：1. CPU的reorder write 2. compiler的reorder ， 编译器的很好解决，只需要加个instruct就行， CPU的话就比较巧妙，PMFS用的LOG大小为64byte（ fix the size of log entries to a single (aligned)cacheline）， 利用CPU不会reorder一个cache line里的写顺序的特性，整个log的写都在一个cache line里，因为整个log被fix to a single (aligned)cacheline了，64byte在这里只是因为目前用的CPU cache line是64byte，重要的是要和cache line一样而不是64。这样的话，gen_id这个用来标记有效的成员就可以确保被最后一个写入了。\n当然还有一些其他的方法来保证log有效：using a checksum in the log entry header【 In Proceedings of the Twentieth ACM Symposium on Operating Systems Principles】 or tornbit RAWL【 In Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems】，回头可以研究一下具体是啥，再回来补上\n看log的图，左侧：顺序就是首先写log（这个log在data字段里记录下old的值是什么，用于undo恢复旧值，如果不记肯定没法恢复）并持久化它，然后正式修改PM里的metadata，重复这个过程知道修改操作都搞完，最后需要一个commit的log，在将commit的log放进去之前需要先用同步原语确保cache里的metadata都写进PM（覆盖旧的值），然后提交committed的log并持久化他，这就代表着这个事务或原子操作结束了。\n注意到需要记录旧的data，这就是undo+journaling，如果data太大都要复制，那当然overhead也大。\npm_wbarrier这个同步原语，如果能减少，性能肯定更好。这就要借助这个. A log cleaner thread，这个线程用于清理已经提交的事务相关的log，它的行为是先pm_wbarrier确保log事务真的提交（cache里的东西真的写入PM），然后修正head，让head掠过提交的log entry。这个线程是异步的，那既然他也要调用pm_wbarrier，那就干脆在commitlog写入之后就别再调用pm_wbarrier了，这样能提升性能。但是还是有一点点可能导致已经被commit的数据在重启后被undo log rollback，因为实际上commit log entry并没有进PM，恢复机制就不知道是不是真的已经commit了，只知道undolog里这些log entry的后面没commit log entry，他就会undo之前的log，就其实是把PM里已经完成修改的data又改回上一个版本了。\n Cow for file data:  大数据量操作使用COW机制来确保一致性，\n数据会在元数据之前被持久化。（一致性还可以更强，比如一起持久化）（ same as the guarantee provided by ext3 ext4 in ordered data mode.）\n问题是，如果我用huge page（1GB），即使我只修改了几MB，我也需要copy整个1GB的page，这么严重的写放大也需要解决，但这个作者说后面再解决。。\n 恢复机制  在mount的时候检查log，取出uncommitted的log（其实就是事务里一些没有用commit log entry标记结尾的log entries），undo他们，当然实际上committed的log也会被取出来（看gen_id取出有效的），还会构造出committed和uncommitted的log的俩list，然后把committed的list丢掉。。。\n Consistency of Allocator:  首先得知道什么是allocator structure，它就是管理你分配空间的数据结构方便我们快速找到我们文件系统管理的空间，因为在文件系统里free和alloc太多（指频次太高），如果都要在log内记录消耗太大，干脆直接在DRAM里记录这个东西，PMFS里就是freelists，通过一个节点能找到一个inode（用于free和alloc时）（NOVA里radix tree也是这么做的，用radix tree来快速找到inode log），宕机的话。在下次启动时直接扫描PM里的那个B-树来恢复这个allocator structure。正常unmount就在末尾将这个allocator structure持久化到internal的一个node上（还是在那个B-树里），下次启动直接copy过来继续用。\nWrite Protection Protect PM from stray writes.\n\r\n看一下这个表格，列是操作的人，行是被操作的数据所有者。\n user 操作 user 数据  这个进程的地址空间自动就隔离了。\n user 操作 kernel 数据  进程能访问到的地址空间权限控制。\n kernel操作user的数据  Supervisor Mode Access Prevention 是一个处理器的机制。激活他就能防止kernel操作user的数据。\n这个是必要的，因为mmap的使用。？？。。。\n kernel 操作 kernel的数据  多个OS组件操作同一片内核空间，这个问题变得很复杂。。。\n但是在这里只有个PMFS能操作PM，那就简单多了。\n首先来看如果加一个权限位进data，那么会发生什么： 权限会来回变，TLB要不停的换页项，产生TLB shotdown，with high overhead\n\r\n就按这张图这么搞，这张图应该从下往上看，顺序下右左。\n当然有一个前提：PMFS首先默认整个PM是只读的，当需要修改的时候再改成可写然后修改然后再改回只读。\n修改的流程就是上面那张图了。\nCR0.WP 是处理器提供的一个写保护机制，修改它可以修改处理器的写的能力：x86里的机制\nif (ring0 \u0026amp;\u0026amp; CR0.WP == 0)\rwrite(P) is allowed;\relse\rwrite(P) causes GP;\r设想如果在修改可写和只读的过程中被异步中断打断，那就G了。所以需要开关中断：看途中右边有disable_interrupts();\n这种方法就没改data，所以就很好没有TLB shotdown\n有一些比较牛逼的处理器（ Itanium）有诸如processor’s protection key registers (PKRs)这样的东西，并且在TLB里和写保护有关的tag。这种也是借助了其他的资源，没改data所以也没TLB shotdown\nImplementation  interface  首先， uses and extends the eXecute In Place (XIP) interface in the Linux kernel.使用和扩展了XIP，XIP是一个linux内核里的用于在PM上直接执行文件的接口（正常都是在DRAM上，考虑正常的一个spawn流程：fork进程解析ELF文件并将对应内容放到process的address的各个area（text，data。。。。这就是在DRAM上了），然后修改eip等寄存器，CPU就开始运行这个可执行文件了）。XIP直接就提供了一些VFS的回调函数，这些函数帮助我们跨过page cache和块设备层。也就是说PM不被识别为块设备，他的byte-addressable能力就能被发掘出来。（就是VFS的read，open等我们可以自己去写，然后注册到这个callback里去，PMFS就是xip_file_read, xip_file_write, xip_file_mmap替换了read,write,mmap）这些callback。XIP还有个callback需要PMFS实现并注册：get_xip_mem，这玩意是用来做PM上虚拟物理地址转换的。\n同时，直接将PM映射到用户地址空间，也需要实现缺页函数。基于xip_file_fault来实现，并拓展huge page的支持。\n还有个问题：页表太大（项太多），mount消耗时间太多。\n因为PM只有PMFS，对PM的所有页都要建立页表项，不像普通的DRAM，第一DRAM不大，第二不需要所有页表项都在页表里，也放不下，需要的时候置换就行。PM同时作为主要存储和临时存储一般都比较大：256GB都不算大，但256GB下mount time和pagetable cost都很大了，所以直接实现个ioremap_hpage替代原本的ioremap用最大的page（1G）去建表，当然1G只针对内核地址空间，因为内核基本上就是固定的（与slab固定的各种unit有异曲同工之妙）。普通的地址空间当然还是可以用小的page的，况且在一开始也没用户进程，都是内核进程，那么关于页表的开销就被分散到后面的操作中，不会堆积在mount的时候，mount的时候大都是内核相关操作。\n procedure  describe a few common file system operations in PMFS.\n 创建  开启新事务并在PMFS-Log里分配需要的log entry。 -\u0026gt; 先从DRAM里的freelist拿到inode并初始化，然后log一下这个操作（共消耗掉俩log entry） -\u0026gt; 然后去B-树里找一个新的叶子节点作为directory entry（VFS的dentry）让他指向file inode，这里只消耗一个log entry -\u0026gt; 修改directory inode的属性（modification time），log一下，这里也要一个log entry -\u0026gt; 写个commit log收尾代表事务完成\n写  首先，如果写的量比较小，不需要给file分配新页，那就直接COW机制写到file的data末尾然后atomic机制更新file inode的size and modification time。\n如果需要分配新页：\n开启事务，一样的先分配log entry， 分配页然后写数据到PM里（用 non-temporal store写，就是那个用于流的）。最后log和更新inode的B-树指针，让他加一个指向新分配的页，同时modification time这个元数据也需要更新。最后commit log entry收尾。\n删除  inode的引用清零就开始删除inode，否则删除文件就是inode的ref减一。VFS会用一个callback来实施inode删除操作。\nfree inode就两步骤： 1. 修改inode的一些属性 2. 还给DRAM里的freelist\n修改inode属性可以用64位的原子操作来做（如果有RTM的话），没RTM就用细粒度journaling做。操作涉及file inode和他的directory inode。\n将inode还给DRAM里的freelist这个操作不需要保证，因为就算因为掉电没还进去，下次restart的时候freelists重建也需要扫描B-树来重建。\n可能有用的tips： Non-temporal in this context means the data will not be reused soon, so there is no reason to cache it. These non-temporal write operations do not read a cache line and then modify it; instead, the new content is directly written to memory.\n","date":"2021-12-16T00:00:00Z","image":"https://gao377020481.github.io/p/nvm%E4%B8%8Bsoftware%E4%BC%98%E5%8C%96/392_huac36e2c85ed8649a34771cb2c2f77ce2_7021583_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/nvm%E4%B8%8Bsoftware%E4%BC%98%E5%8C%96/","title":"NVM下software优化（FS角度应用NVM，NVM + DRAM）"},{"content":"原文：Let’s Talk About Storage \u0026amp; Recovery Methods for Non-Volatile Memory Database Systems\ncmu几个大佬在16年的文章，主要讨论的是完全的NVM下的优化，偏向于应用allocator而非FS，当然他也模糊了两者之间的概念，得益于NVM的nonvolatile，allocator也能实现naming，使用固定指针存到固定位置就行（FS里当然是地址开始处由SB提供信息了）。\nbackground OLTP的增多，简单就是数据增删改增多。所以需要优化DBMS。但是DBMS一直以来都在权衡VM(volatile memory)和NVM(non-volatile memory)，因为他们自身的特性。DRAM作为VM可以按字节寻址，具有很高的读写性能，SSD/HDD等NVM只能按block寻址，随机写的性能也很差，一般都需要利用其顺序写速度快的特性来做优化(LSM in leveldb/rocksdb)。这里有一点：server里40%的能源都被flash DRAM用掉，DRAM需要一直刷新，即使这片数据不用也要刷新，不刷新就消失了。而SSD/HDD这些的缺点就不用说了，很慢，且写放大读放大都存在。\nNVM是个概念，有一系列的具体实现技术，他就是兼具DRAM的性能和寻址又掉电保留数据。目前的DBMS都没有好好利用这个东西：\n磁盘型DBMS如mysql orcal在内存中有一个cache，积累一下然后去顺序的以block写入磁盘，内存型DBMS如redis voltdb都有自己的持久化策略来缓和DRAM的易失性。这些多余的组件在NVM中都没必要。\n这篇文章是讲一种只使用NVM的情况下的DBMS，不是DRAM和NVM混合架构也不是只用NVM做log(nova就有点这个意思)。\n说一下模拟：\n使用的是intel的一个工具，有一个microcode，他会用dram模拟nvm，当nvm需要停顿的时候就让cpustall。（ The microcode estimates the additional cycles that the CPU would have to wait if DRAM is replaced by slower NVM and then stalls the CPU for those cycles. ）\n直接用libnuma来提供malloc，然后用fence来确保数据不会在cache中停留而是直接进入NVM。 文件系统也有对应的api，一个基于byte-addressable的FS。当然IO还是要过VFS tip：基于mmap可以不过VFS，特殊的mmap甚至可以掠过kernel里的page cache直接写数据到disk（NVMM）进一步加快速度。\n这里用memory allocator，但是restart的时候不好找到数据，文件系统有superblock可以找到inode数据结构获取文件目录结构信息，memory没办法。所以改进一下：NVM-aware Memory Allocator\n 虚拟内存到NVM的映射是固定的，基于此保存指向这个地址的pointer，每次重启拿到这个pointer就可以获取到之前的数据信息。 避免数据在cache中停留引发的不一致，依次使用clflush将cache中的data写回数据，再用SFENCE确保clflush在这里完成（对所有processor可见，因为你现在把cache刷回去了那你得让cache刷回到所有proccsor，cache分级的，每个processor都有自己的呢，我理解实现上把cacheline置为steal就可以了），之后提交事务，这就是sync原语。  参照 三个存储引擎类型： (1) inplace updates engine, (2) copy-on-write updates engine, and (3） log-structured updates engine\n从两部分来描述： 1. 事务怎么样作用 2. 怎么处理crash\n当然这些都是没针对NVM优化过的\nIn-Place Updates Engine \r\n像VoltDB 这样的内存型数据库，直接在原位置更新数据没有buffer pool。\nBlock Pool分两种，一种定长一种变长，block大于8Byte就用变长的pool\n还有一个Block lookup table用于找到block，对于变长的block，先通过Block lookup table在定长pool里找到其在变长Pool里的地址，然后再进变长pool找到真正的block\n一般使用WAL（write ahead log）来在事务change前记录信息： the transaction identifier, the table modified, the tuple identifier, and the before/after tuple images depending on the operation\nARIES是比较有名的协议，他会定期做checkpoint也就是snapshot，每次恢复使用最新的checkpoint结合WAL里的commit的log来将DB恢复到断开之前的状态。\nCopy-on-Write Updates Engine \r\npage cache加速查找，LRU来更新cache，最小单位是leaf，结构在leaf page layout里。\n需要修改时，复制整个directory（LMDB’s copy-on-write B+trees使用特殊技巧，只复制从leaf到root路径上的nodes，其他nodes由dirty directory和current directory共享）。\n复制完出现dirty directory之后开始修改，分两步：\n 在dirty directory上修改，指向新的tuple 事务commit之后，将master record指向dirty directory这时dirty就变成current了  master record是持久化在文件里的，他在某个特殊offset处。\n这种设计存在写放大（总是要复制，写新的，再丢掉旧的），对NVM来说磨损太大。但他恢复时直接用旧的就行，因为旧的里肯定是commit过的数据，\nLog-structured Updates Engine \r\n典型的：leveldb和他的“子集”rocksdb\n放个leveldb的arch省个事。。\n\r\nmemtable到达指定大小就开始打包成为level0的一部分去持久化\n各级level数量到达指定大小就开始打包向下一级迁移（compaction）\n这一设计有利于顺序写，但是存在读放大，因为根据key找目标在哪一个sst就很可能需要好几次，这个是比较暴力的，用bloom filter可以快速排除sst但是还是得去触发到FS的IO操作没法在一次读操作里完成。\n针对NVM的优化版本 In-Place Updates Engine \r\n NV的B+树存储索引，restart的时候不用再重建 WAL中的log只需要保存指针，不需要保存一大块的数据tuple image，因为指针指向的数据就存储在NVM中，不会因掉电失去。  显然需要先写WAL再写data才能保证事务。\n且log可以是一个链表，因为随机写的效率不再是问题。\nCopy-on-Write Updates Engine \r\n使用了非易失性的copy-on-write B+tree\n直接持久化拷贝后的tuple副本其实就是拷贝到NVM中，并且在The dirty directory中保存非易失性指针-》也是在nvm中\n用更轻量级的持久化机制来持久化copy-on-write B+tree中的更新操作\nLog-structured Updates Engine \r\n相比于之前使用sst还要分层来增加顺序性（sst）和减少读放大（compaction和分层），现在直接不需要sst，所有到了一定大小的memtable直接设置一个不可变位，变成immutable memtable就行，这些immutable memtable也可以再进行compaction的，同样每一个immutable memtable也有自己的bloom filter。 注意到memtable也在NVM中，那他就是非易失的，这就很像steal语义，uncommited的tuple被持久化了，那就需要再restart的时候吧这个uncommited的undo掉，所以这里的log是undolog。 且因为memtable本来就持久化在NVM中，不需要对他进行重建。\n","date":"2021-12-14T00:00:00Z","image":"https://gao377020481.github.io/p/nvm%E4%B8%8B%E7%9A%84%E6%96%B0dbms%E6%9E%B6%E6%9E%84/450_hu6a236b509d40a20723f23eba9bde37c2_4792235_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/nvm%E4%B8%8B%E7%9A%84%E6%96%B0dbms%E6%9E%B6%E6%9E%84/","title":"NVM下的新DBMS架构（VMM角度应用NVM，only NVM）"},{"content":"之前一直很好奇，在超算里（或大规模的集群里），是怎么减少network传输的损耗的，最近项目中接触到了mecury这个rpc框架，他依赖于OFI、libfabric这个网络框架来做集群内的高性能网络传输。\nOFI相比于普通的内核TCP/IP协议栈的优化思路与用户态协议栈有异曲同工之妙，准备好好写一下这篇笔记，先从内核里的TCP/IP协议栈的缺点开始，到优化思路与实现，再看看能不能学习一下ofi实现的trick。开个坑先，慢慢写。。\n从缺点找灵感 TCP/IP 缺点\n 用于提供可靠性的header占用很多带宽 同步的话耗时，异步的话耗空间（内核里的缓冲区）拷贝也耗时  高性能的网络API应该是什么样的？ 尽量少的内存拷贝 两点：\n 用户提供buffer，与协议栈用一个buffer 建立一个流量控制机制  异步操作 两种策略：\n 中断和信号，这种机制会打断正在运行的程序，evict CPU cache，而且一个随时能接受信号还不影响自己工作的程序就比较难开发 事件queue， 来了就进queue  Direct Hardware Access 主要两种思路：\n 越过kernel，直接与网卡的buffer交互（代表DPDK） 硬件软件配合来在用户空间共享同一块地址空间作为buffer（RDMA）  那应该怎么设计呢？ 先抄来个需要的interface：\n/* Notable socket function prototypes */\r/* \u0026quot;control\u0026quot; functions */\rint socket(int domain, int type, int protocol);\rint bind(int socket, const struct sockaddr *addr, socklen_t addrlen);\rint listen(int socket, int backlog);\rint accept(int socket, struct sockaddr *addr, socklen_t *addrlen);\rint connect(int socket, const struct sockaddr *addr, socklen_t addrlen);\rint shutdown(int socket, int how);\rint close(int socket); /* \u0026quot;fast path\u0026quot; data operations - send only (receive calls not shown) */\rssize_t send(int socket, const void *buf, size_t len, int flags);\rssize_t sendto(int socket, const void *buf, size_t len, int flags,\rconst struct sockaddr *dest_addr, socklen_t addrlen);\rssize_t sendmsg(int socket, const struct msghdr *msg, int flags);\rssize_t write(int socket, const void *buf, size_t count);\rssize_t writev(int socket, const struct iovec *iov, int iovcnt);\r/* \u0026quot;indirect\u0026quot; data operations */\rint poll(struct pollfd *fds, nfds_t nfds, int timeout);\rint select(int nfds, fd_set *readfds, fd_set *writefds,\rfd_set *exceptfds, struct timeval *timeout); 首先来看看这几类都有什么目标？\n\u0026ldquo;control\u0026rdquo; functions ： 这东西是用来控制的，一般调用次数一两次，不需要高性能。 \u0026ldquo;indirect\u0026rdquo; data operations： 这个东西虽然性能也重要的，但是跟操作系统调度带来的损耗相比，他的损耗直接被抹平\n所以主要：\n\u0026ldquo;fast path\u0026rdquo; data operations\n这种操作是最需要改善的\n更少的参数 很好理解，略\n更少的LOOP和分支 在interface内部的实现中，尽量少的使用分支和loop\nCommand Formatting 网卡驱动程序与网卡交互的时候有formatting，如果这个格式控制可以由application插手，那可以做很多特定的优化，将网路通信的path搞得更确定，比如我知道我只和这个peer通信，那么我直接给包加个头，这个头会由网卡取到然后发送给peer。流程更少，性能更高。\nMemory Footprint 减小内存占用：\n举个例子：一个socket address：\n/* IPv4 socket address - with typedefs removed */\rstruct sockaddr_in {\ruint16_t sin_family; /* AF_INET */\ruint16_t sin_port;\rstruct {\ruint32_t sin_addr;\r} in_addr;\r};\r这里面这个sin_family标识了协议栈，这个东西很多peer都一样，那直接让一样的用一个就行，能节省很多空间。\n这只是个例子，为了说明节省内存的思路。\nCommunication Resources 首先，可以使用预发布缓冲区的思路，就是我不知道我这一次要接受多少的数据，我直接post一块4MB的缓冲区，作为一次接收的缓冲区，意思就是哪怕这一次传输只传了1KB（很多浪费），我也不管。那么消息和缓冲区就一一对应。\n当然对大小不同的数据使用不同的通信协议也是很有必要的，因为他们对应的缓冲区的管理难度不同。\n共享接收队列 为每一个address peer维护一个缓冲区块集合好像有点成本太高。而且有的通信频繁的peer会阻塞，而其他的buffer却一直空着，也不均衡。\n现在考虑所有endpoint用一个缓冲区块集合，来请求的时候就从集合里分发一块。类似池的思路。\n多接收缓冲区 多接收缓冲区是另一种思路：\n它通过维护消息边界的方法，尝试重用每一块缓冲（那个4MB的），举个例子：一个1KB的消息到来，进入4MB的缓冲区内，剩下的将不会被浪费，而是给这1KB的数据加上头尾，区分边界，继续尝试利用4MB-1KB的空间接受更多的数据。\nOptimal Hardware Allocation 控制硬件资源（网卡），一般来讲网卡有自己的控制程序，为了极致的性能，考虑暴露接口给用户来控制网卡行为。\nSharing Command Queues 将使用的共享接收队列暴露给应用程序，叫用户自己控制多个endpoint怎么使用共享队列。（不同endpoint可能有不同优先级之类的）\nMultiple Queues 直接不用多个endpoint，用一个endpoint就行，这样address也就一个，充分利用网卡资源。\nOrdering 顺序问题，如果要保证顺序，那显然问题会变复杂，性能会变差。\nUDP是值得参考的，而且在local 网络内一般顺序是自动保证的，因为线都是自己连接的，可控程度很高。\nMessages 与UDP不同的是，OFI的设计可以是很大的message（1GB）。\n它将每一次发送都分成很小的块，比如要发送一个64KB的块和一个4KB的块，接收端有64KB和4KB俩buffer，如果乱序，64KB的就被截断。但是无所谓，因为每次发送1KB的小块，后面在组装起来。\nData 数据顺序就比较重要，尤其是数据在更新同一个缓冲区的情况，顺序更应该保证。\n但同一个message内的数据顺序就不那么重要，除非有的机制用到了最后一个数据块来标识message结束，那就需要保证一个message内的数据顺序。\nCompletions 操作的完成顺序，这个不知道有啥优化的点。。\nOFI Architecture \r\n","date":"2021-11-12T00:00:00Z","image":"https://gao377020481.github.io/p/ofi-for-hpc/786_hucac7feb76ac17aeca67ee18e353cf80e_4947876_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/ofi-for-hpc/","title":"OFI/libfabric"},{"content":"Non-Volatile Memory (NVM) 是相对DRAM（掉电后数据丢失）而言的，指可以持久化保存数据的存储介质\nNon Volatile Main Memory (NVMM) NVMM是对多种新型非易失存储介质的统称，目前的NVMM包括：相变存储器 Phase-Change Memory (PCM)、忆阻器 Memristor（ReRAM）、自旋扭矩转换随机存储器 Spin Transfer Torque - Magnetic Random Access Memory (STT-MRAM)等等。\nDIMM全称Dual-Inline-Memory-Modules，中文名叫双列直插式存储模块，是指奔腾CPU推出后出现的新型内存条，它提供了64位的数据通道。\nNAND Flash可以做成不同接口和不同形式的闪存设备：SATA接口一般用于普通固态硬盘（SSD）产品、PCIe接口则常用于高端服务器闪存产品。NVMM由于可以媲美DRAM的性能，可以做成类似内存条形式直接插在主板的DIMM插槽上。这类产品可以称为持久化内存，Persistent Memory (PM)，或存储级内存，Storage Class Memory (SCM)。\nNVDIMM NVDIMM又是什么呢？实际上早已有之，是基于NAND Flash的非易失型内存条——通常被做成“电池+NAND Flash+DRAM”的形式：在掉电时用电池电量将DRAM数据刷回NAND Flash实现持久化。\n3D Xpoint 英特尔同时也推出了基于3D Xpoint技术的Optane SSD，采用PCIe接口。相比基于NAND Flash的企业级SSD在顺序读写上似乎并没有太大提升，顺序写大约在2000MB/s的水平。但得益于稳定的低时延——读写均为10us，其4KB随机读写性能非常逆天,随机写达到500000 IOPS。\n","date":"2021-10-12T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/623_hu0b28264e11eb4142d69cccd53820e776_26598711_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/","title":"存储领域概念整理"},{"content":"一次关于proc的分析 运行：\n\rstrace file /proc/version\r来一小段\n\rexecve(\u0026quot;/usr/bin/file\u0026quot;, [\u0026quot;file\u0026quot;, \u0026quot;/proc/version\u0026quot;], 0x7ffc3c1ee838 /* 25 vars */) = 0\rbrk(NULL) = 0x5591347ae000\raccess(\u0026quot;/etc/ld.so.nohwcap\u0026quot;, F_OK) = -1 ENOENT (No such file or directory)\raccess(\u0026quot;/etc/ld.so.preload\u0026quot;, R_OK) = -1 ENOENT (No such file or directory)\ropenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3\rfstat(3, {st_mode=S_IFREG|0644, st_size=36168, ...}) = 0\rmmap(NULL, 36168, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f48f3973000\rclose(3) = 0\r/etc/ld. so.nohwcap 这个文件存在，链接器就只会去load完全无优化的库版本（哪怕CPU支持优化）\n/etc/ld.so.preload 这个文件就跟LD_PRELOAD 干一个事情，这玩意在GekkoFS的系统调用截取时用过，preload一下自己的IO库，当发生IO操作，链接器会讲call的func链接到自己的库上去而不是走正常的posix fileIOapi。\nbrk(NULL) = 0x5591347ae000\n这是什么，这就是通过brk找到heap的边界，好了，这里找到是0x5591347ae000,为啥要找呢，是谁要找。\u0026mdash;\u0026ndash;》 因为程序要用malloc了，那肯定得知道先知道边界，很合理。\nopenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3\rfstat(3, {st_mode=S_IFREG|0644, st_size=36168, ...}) = 0\rmmap(NULL, 36168, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f48f3973000\rclose(3) = 0\r这一段打开了这个链接器的缓存文件返回fd为3，统计了一下相关信息，mmap再把它映射到mmap region， 然后把他关上，实际上在很多程序中这一段都会出现，他是很通用的CRT和ld工作的流程。\n","date":"2021-10-02T00:00:00Z","image":"https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/362_hu71f3d2d3fb6d4d57de0ef2238e0556d9_4933750_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E4%B8%80%E6%AC%A1%E5%85%B3%E4%BA%8Eproc%E7%9A%84%E5%88%86%E6%9E%90/","title":"一次关于proc的分析"},{"content":"Openmpi 初步使用 安装与测试 直接官网下载release包\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.1.tar.gz linux下解压:\ntar -zxf openmpi-4.1.1.tar.gz 进入开始configure： prefix 为指定安装路径\ncd openmpi-4.1.1/ ./configure --prefix=/usr/local/openmpi 安装：\nmake sudo make install 设置环境变量\nsudo vim /etc/profile 加入：\nexport PATH=/usr/local/openmpi/bin:$PATH export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH 生效：\nsource /etc/profile 测试\nmpicc --version 写代码测试：hello.c\n#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;mpi.h\u0026#34; int main(int argc, char* argv[]) { int rank, size, len; char version[MPI_MAX_LIBRARY_VERSION_STRING]; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;size); MPI_Get_library_version(version, \u0026amp;len); printf(\u0026#34;Hello, world, I am %d of %d, (%s, %d)\\n\u0026#34;, rank, size, version, len); MPI_Finalize(); return 0; } 编译并运行,我这里是四核虚拟机\nmpicc hello.c -o hello mpirun -np 4 hello 管理 使用cmake管理\n模板：\ncmake_minimum_required(VERSION 3.5.1)SET(SRC_LIST hello_c.c)find_package(MPI REQUIRED)include_directories(${MPI_INCLUDE_PATH})add_executable(hello_c ${SRC_LIST})target_link_libraries(hello_c ${MPI_LIBRARIES})if(MPI_COMPILE_FLAGS) set_target_properties(hello_c PROPERTIES COMPILE_FLAGS \u0026#34;${MPI_COMPILE_FLAGS}\u0026#34;)endif()if(MPI_LINK_FLAGS) set_target_properties(hello_c PROPERTIES LINK_FLAGS \u0026#34;${MPI_LINK_FLAGS}\u0026#34;)endif()编译与运行：\nmkdir build cd build cmake .. make mpirun -np 4 hello_c ","date":"2021-09-17T00:00:00Z","image":"https://gao377020481.github.io/p/cmake/405_huaea55e1e24566107c91b0c14c5461267_4592730_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/cmake/","title":"Openmpi"},{"content":"QUIC\nquick udp internet connection\n是由 google 提出的使用 udp 进行多路并发传输的协议\n优势 Quic 相比现在广泛应用的 http2+tcp+tls 协议有如下优势\n 减少了 TCP 三次握手及 TLS 握手时间。 改进的拥塞控制。 避免队头阻塞的多路复用。 连接迁移。 前向冗余纠错。  0RTT建连 传输层和加密层总共只需要0RTT就可以建立连接。\n因为其握手数据和HTTP数据一同发送，建连过程可以认为是0RTT的。\n灵活的拥塞控制 QUIC默认使用了 TCP 协议的 Cubic 拥塞控制算法，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。\n但其：\n可插拔  该拥塞控制算法实现于应用层，不需要修改内核就可以对其快速迭代 同一程序的不同连接使用不同拥塞控制算法，针对不同用户做适配 变更算法只需要修改配置再重新加载，不需要停机  递增的packet number和维持顺序的stream offset 使用严格递增的packet number，即使是相同包的重发也递增。接收端就可以区分开这个包是重发的还是之前的。\n避免了在计算RTT的时候引发的歧义问题，因为发送方RTT计算需要计算的边界是包发出和包收到两处，如果使用重发包的时刻作为左边界，收到ack的时刻作为右边界，万一这个ack是初始发出的包的而不是重发的那就统计小了。\nSACK选项空间更大 QUIC的SACK选项空间256Bytes 对比TCP的30Bytes很大，能够提供更多已经收到segment的信息，方便发送端进行精度更高的选择重传\nAck Delay Ack Delay是在接收端进行处理的时间，该时间也需要记录并发送，TCP的timestamp区域并不记录这个，计算的RTT理论上就更大不够准确。\nQUIC在计算RTT时会减去Ack Delay\n基于 stream 和 connecton 级别的流量控制 connection: TCP连接 ，复用：一个connection上可能有多个stream\nstream: HTTP请求\nQUIC 实现流量控制：\n通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。\n通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。\nQUIC 的流量控制和 TCP 有点区别，TCP 为了保证可靠性，窗口左边沿向右滑动时的长度取决于已经确认的字节数。如果中间出现丢包，就算接收到了更大序号的 Segment，窗口也无法超过这个序列号。\n但 QUIC 不同，就算此前有些 packet 没有接收到，它的滑动只取决于接收到的最大偏移字节数。\n当读取端读取到未到达的包时，可以采取发特殊包的方法向发送端要。\n没有队头阻塞的多路复用 QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势。\nQUIC 一个连接上的多个 stream 之间没有依赖。这样假如 stream2 丢了一个 udp packet，也只会影响 stream2 的处理。不会影响 stream2 之前及之后的 stream 的处理。\nHTTP2的多个stream即使后面stream已经全部到达，前面stream出现丢包也需要后面stream跟着一起等待才能读取。\nHTTP2使用的TLS协议中最小单位是Record，最大不能超过 16K，由于一个 record 必须经过数据一致性校验才能进行加解密，所以一个 16K 的 record，就算丢了一个字节，也会导致已经接收到的 15.99K 数据无法处理，因为它不完整。\nQUIC 最基本的传输单元是 Packet，不会超过 MTU 的大小，整个加密和认证过程都是基于 Packet 的，不会跨越多个 Packet。这样就能避免 TLS 协议存在的队头阻塞。\nStream 之间相互独立，比如 Stream2 丢了一个 Pakcet，不会影响 Stream3 和 Stream4。不存在 TCP 队头阻塞。\n加密认证的报文 QUIC除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。\n而TCP报文头部完全裸露。\n连接迁移 TCP连接以四元组标识（俩IP和端口），任意一个或多个变化就发生断连。针对 TCP 的连接变化，MPTCP其实已经有了解决方案，但是由于 MPTCP 需要操作系统及网络协议栈支持，部署阻力非常大，目前并不适用。\n任何一条 QUIC 连接不再以四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。\n由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。\n","date":"2021-03-09T00:00:00Z","image":"https://gao377020481.github.io/p/quic/348_hu2fb516008fdf2abfd5c4a8aa9328ae4d_5294189_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/quic/","title":"QUIC"},{"content":"Micro-benchmarks (repeatable sections of code) can be useful but may not represent real-world behavior. Factors that can skew micro-benchmark performance include Java virtual machine warm-up time, and global code interactions.\nMacro-benchmarks (repeatable test sequences from the user point of view) test your system as actual end users will see it.\n","date":"2021-03-01T00:00:00Z","image":"https://gao377020481.github.io/p/benchmark/324_hu5345abdaa640012db8d08e2af4454d52_5473626_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/benchmark/","title":"micro 和 marco benchmarks"},{"content":"HTTP/1.1 建连 输入网址\n 有网址，无IP。所以先生成DNS查询报文，置于目的端口53的UDP报文中。目的地址就是建网时通过DHCP获得的DNS服务器IP。但是还无网关的MAC，所以使用ARP查询报文，其目的IP为网关IP，MAC地址为全1（广播）。网关收到后就通过ARP回答返回给客户机他自己的MAC地址。客户机拿到网关MAC就继续组装好DNS查询报文，发送给网关路由器。 DNS查询被网关转发到了最终DNS服务器，服务器查询到网址对应ip并返回给客户机。 客户机拿到IP就生成TCP套接字然后向IP所处机器发起连接请求，三次握手建立连接后向其发送HTTP GET报文。 网址服务器返回一个HTTP相应报文。 客户机拿到后浏览器渲染一下显示出来。  缺点   队头阻塞\n  低效的 TCP 利用\n  臃肿的消息首部\n  受限的优先级设置\n  HTTP2 ","date":"2021-02-26T00:00:00Z","image":"https://gao377020481.github.io/p/http-1-to-2/297_hu9ab82a7bab933b33e03c1f572bc922c1_4050766_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/http-1-to-2/","title":"HTTP 1 to 2"},{"content":"Alloc c++内存空间管理模块， 包含于:\n\u0026lt;memory\u0026gt;:\n \u0026lt;stl_construct.h\u0026gt;: 定义全局函数construct(),destory()，负责对象析构与构造 \u0026lt;stl_alloc.h\u0026gt;: 定义一二级配置器，名为alloc \u0026lt;stl_uninitialized.h\u0026gt;: 一些全局函数用来操作大块内存数据  construct\u0026amp;destory construct类似C++的placement new。\ndestory存在两版本：\n 版本一接收一指针参数，析构掉指针处的对象 版本二接收两指针参数，析构掉两指针之间的所有对象  alloc alloc空间配置器分一级配置器和二级配置器：\n 一级配置器就是直接封装malloc和free 二级配置器：  维护16个自由链表组成内存池，分别负责十六种小型区块配置，填充内存池调用malloc 出现内存不足，或要求区块大于128Bytes转一级配置器    二级配置器：\n 对于所需求区块无空闲块的情况直接去更大区块处查找，找到后出现的碎片会挂到与它大小匹配的区块链表上 都没有就分配40块需求区块，20块挂在链表上，20块作为后备  free free是如何知道对象的大小呢，这是因为malloc在分配内存时，会在对象的内存块的上下区域添加内存大小的cookie，VC6中malloc添加的是对象大小+1.不管怎么样，这都足够free去get到需要归还给操作系统的内存空间大小了，当然所谓归还也只是还给CRTL的内存池，只有空闲内存到达某种条件（例如整整几页的空间都空闲，他们可能还连续），那就使用系统调用来让内核释放掉这一块虚拟内存（其实也就是进task_struct里找到mm_sturct然后操作里面的bitmap，将要释放的内存对应的块设置为再次可用）。\nIterator  迭代器的根本还是一个指针，当然它是一种智能指针 通过指针萃取到指针所指对象的类型或一些其他信息很重要，这就是traits技法  分类上个侯捷老师的图：\n\r\ntraits 利用template自动推导和偏特化实现普遍可用的萃取特性方法，直接记录代码：\ntemplate \u0026lt;class T\u0026gt; struct MyIter { typedef T value_type; // 内嵌型别声明  T* ptr; MyIter(T* p = 0) : ptr(p) {} T\u0026amp; operator*() const { return *ptr; } }; // class type泛化版本 template \u0026lt;class T\u0026gt; struct iterator_traits { typedef typename T::value_type value_type; }; // 原生指针偏特化版本 template \u0026lt;class T\u0026gt; struct iterator_traits\u0026lt;T*\u0026gt; { typedef T value_type; }; // const对象指针偏特化版本 template \u0026lt;class T\u0026gt; struct iterator_traits\u0026lt;const T*\u0026gt; { typedef T value_type; }; template \u0026lt;class I\u0026gt; typename iterator_traits\u0026lt;I\u0026gt;::value_type //typename 告诉编译器这是一个type func(I ite) { std::cout \u0026lt;\u0026lt; \u0026#34;normal version\u0026#34; \u0026lt;\u0026lt; std::endl; return *ite; } 当然value_type只是常用的一种，STL还有其他几种，就不写了。\nDataStruct List 链表，而且是双向链表，需要注意的是该双向链表的实现使用了一个无名节点，使得这个双向链表形成了环。这也就是end指向的结尾得“未知值”。 侯捷老师的图拿来参考一下：\n\r\nVector 贴个侯捷老师的图：\n\r\nArray 贴个侯捷老师的图： \r\nForward List \r\nDeque 侯捷老师的图实在是太经典了，一看就懂\n\r\n具体实现的图中的map是一个原生数组，每一个元素中存有一个指向buffer得指针\nbuffersize现在是没法调整的，有一套较为复杂的计算方式，需要的话可以查一下\n从他的实现可以看到，这个设计可以模拟数组的随机访问，思路很简单就不说了\n还有一点需要注意是这个容器可以进行insert操作,在指定位置插入值时需要根据当前元素处于前半段还是后半段选择推前面还是推后面，推移操作直接循环搬就行。\nQueue 封装了deque的部分函数实现的，没啥好说的\nStack 封装了deque的部分函数实现的，没啥好说的\nRB-tree stl实现了红黑树，需要注意的是其类定义：\n_Rb_tree\u0026lt;int, int, _Identity\u0026lt;int\u0026gt;, less\u0026lt;int\u0026gt;\u0026gt; itree; 其中前两个是K和V，\n_Identity是一个从V中得到K的函数，为什么需要这个呢，举例：\nkey: int Value:pair\u0026lt;int, vector\u0026lt;int\u0026raquo;, 看出来了吧，这个Value其实是data和key的结合体，所以需要告诉怎么从结合体中取出这个data，逻辑意义上的key和value就对应这里的key和data了。\nless是一个重载了operator()的比较函数。\n有两种插入： insert_equal和insert_unique，看字面意思就知道是啥\n一般来讲rbtree的这个Value是不能修改的，但编译器不阻止\nSet\u0026amp;multi_set Set就是value为Key的rbtree\n其Value不能修改\nMap\u0026amp;Multi_map map的key不能改，但是data可以改\nHashtable 使用vector作为buckets，使用一个hash算法来将对象映射到不同bucket上。\n使用开链法解决冲突，负载因子大于1就触发扩容，直接扩为原来二倍左右。\nUnordered_map\u0026amp;Unorderd_set 底层为hashtable,封装个别方法实现\n","date":"2021-02-10T00:00:00Z","image":"https://gao377020481.github.io/p/stl/276_hu2c6c550ccde85d9a1158da3254c91a6a_2102977_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/stl/","title":"STL"},{"content":"读书笔记：深度探索c++对象模型\nC++ 对象模型 可用的对象模型有很多种，C++对象模型派生于简单对象模型，这里直接说C++对象模型：\n Non-static data member 存在于每一个object内 Static data member 存在于每一个object外 Static 与 Non-static function members 也被存放在所有object外 virtual function members：object内存在指向虚表vtb的指针（一般用于支持RTTI的type_info object的指针置于vtb第一个slot）  继承 虚继承 base class不管被继承多少次，永远只存在一个实体，虚继承产生的派生类只是有路径指向对应的base class实体。\n这里的“路径”可以是类似虚表的指针（bptr）也可以是一系列指针，视存取效率而定。\n然而，这样的话object的布局和大小会随着继承体系的改变而改变，所以这一部分的内容应被存放于对象模型的特殊部分。这个特殊部分就是“共享局部”，其他的固定不变的部分就是“不变局部”。所以为了实现虚继承，引入了两个局部的概念，虽然实现方式各编译器有所不同，但两个局部的概念一致。\n构造 默认构造函数 如果没有user-declared constructor, 一个trivial 的constructor可能被生成， 但他啥都不干。\n  只有编译器需要时，non-trivial 的constructor才会被编译器合成，但他也只干编译器需要的事情，比如A内有一个B类的成员（组合），B有一个构造函数，那么A也就需要一个non-trivial 的constructor，这个构造函数只会构造B，而不会管A内可能存在的其他成员。\n  如果A内有B,C,D三个类的成员，这三个类都有自己的构造函数，显然编译器会为A生成一个构造函数，他会依次调用B,C,D的构造函数，顺序取决于三成员在A内的排列顺序。\n  回到1，如果A内其他成员比如一个int值在user-declared constructor里初始化但是user-declared constructor内未初始化B，那编译器怎么办呢，编译器会扩写user-declared constructor，给里面加上个B的构造函数的调用。\n  类内有虚函数存在，编译器需要扩张已有构造函数或生成一个构造函数来完成虚表的初始化操作\n  虚继承的使用也会给构造函数增加工作量，编译器需要让构造函数给类添加执行期判断的能力，比如在派生类中添加一个bptr提供指向唯一基类实体的路径。\n  默认拷贝构造函数 三种调用情况：\n X x1 = x2 变量赋值 f(x1) 函数传参 return x1 返回值  不需要默认拷贝构造函数：\n 类内以及其子类（无限递归下去）内部全是内建类型（即全是int啥的），那么只需要简单的依次（对于子类递归的执行）将这些成员赋值。这叫做default memberwise initialization，这里编译器没有产生default 拷贝构造函数，这是一种bitwise copy semantic即bitwise的copy每一个变量值  需要默认拷贝构造函数：\n 类内及其子类内存在一个explicit（即显式声明）的拷贝构造函数，那类及其子类本身也需要生成一个拷贝构造函数，其中会调用那个explicit的拷贝构造函数。 类继承的base class有一个explicit的拷贝构造函数 类存在一个或多个virtual function 继承串联下存在虚继承  情况三的类存在一个或多个virtual function\n很容易想到的多态情况，以一个derive class A作为值赋给一个base class B指针或引用，发生修改扩张操作（多态），并且需要使用Base class的vptr而非A的vptr，这都需要编译器在生成的拷贝构造函数中纠正，使base class的vptr指向对应A的虚函数。\n情况四的继承串联下存在虚继承\n同样将derive class A作为值赋给一个base class B，同时这个base class B虚继承自一个virtual base class C。那么A的拷贝构造函数需要被编译器扩张或生成来完成bptr的指定。\n传值三种情况构造  X x1 = x2 变量赋值  转化为：\nX x1; x1 = X::X(x2);  f(x1) 函数传参\n 构造临时对象，通过引用传给函数 因为使用引用传参，所以函数也需要重写参数列表接收一个引用    return x1 返回值\n 给函数参数加一个引用参数，类型为返回值类型 将函数内的局部变量通过拷贝构造给这个参数，返回值体现在添加的参数里，外部可以感知到    尝试优化\n对于返回值，可以从使用者层面优化：\n函数内的局部变量不需要通过拷贝构造给这个参数，直接在函数内部不定义这个局部变量，将对于他的操作全部写在他的一个构造函数内，return X(y,z); 这样当转化为参数时也只是调用参数的构造函数，即可省去拷贝构造。\nmember initialization list 即：\nclass a{ int a1,a2; a(): a1(0), a2(0) {}; } 这种初始化方式。\n以下几种成员必须如此初始化：\n reference member const member call case class 的 constructor call member class 的 constructor  需要注意的是，变量的初始化顺序取决于成员在类内声明的顺序，而非member initialization list的顺序\n虚继承下构造 主要说钻石继承，见图：\n\r\n在构造Vertex3d或Pvertex时，Point的构造函数只应构造一次，且由最底的Vertex3d或Pvertex调用，而非递归的调用到。\n构造函数中调用虚函数 使用构造函数来调用虚函数，参考图： \r\n假如所有类都有一个虚函数size(), 这个虚函数在每个构造函数中被调用，那么在Pvertex构造函数调用时应当根据目前递归调用到那一层来决定size()的调用，也就是Pvertex构造中Point3d构造时就该调Point3d的虚函数size，返回Point3d的大小。 这就对vptr的初始化有了一定要求，维持虚函数调用的规则不变（也就是编译器不插手这种情况，编译器当然可以将size使用对应的静态调用方式来调用，但不好，不够优雅），对vptr的初始化顺序做一定限制，就可以完成。\nvptr在基类的构造函数调用之后但在程序员的code或member initialization初始化操作之前才初始化，这就确保了基类中调用的size依据的是基类的vptr，因为vptr要在程序员的code之前就初始化好。当然member initialization初始化操作其实还是在程序员的code之前的，在vptr初始化之后的。所以顺序是：\n基类的构造函数调用 -\u0026gt; vptr初始化 -\u0026gt; member initialization初始化操作 -\u0026gt; 程序员的code\n数据 数据布局  static成员不在对象布局内，在静态存储区域 其他的同一access section内相对次序与声明次序相关，不同之间编译器可以做自己的实现，不保证顺序 编译器自己生成的vptr等成员自己放置位置，目前多防止开头或结尾。  数据存取 static data member static data member存在于data segment内，编译器将其转化为一个指针，直接去存取。且static data member并不会冲突，得益于name-mangling机制，每一个类的static data member都有其唯一的id，通过id访问\nnonstatic data member 通过implicit class object\n对于函数的成员方法，其可以直接操作类内成员，编译器在参数列表添加一个this指针，方法是通过this指针来操作的\n存取操作\n通过计算成员变量的offset来存取，但是一旦存在virtual base class object，就多了一层间接性，因为虚基类的对象可能需要通过一个bptr来访问到。\n显然，以下两种情况当使用指针存取，成员存在于虚基类中，其效率会低，这是因为编译器实际上并不能确定aptr指向什么type，也就对于x的offset无从得知，一切判断都需要延迟到执行期。但是a.x的方式对于编译器来说是明显的，编译器可以确定的在生成的汇编代码里加上offset寻址到那个x。\nA a; A *aptr = \u0026amp;a; a.x; aptr-\u0026gt;x; 继承下数据布局 无虚函数和虚基类 各基类需要保持其“原样性”：\n见图可知：\n\r\n有虚函数无虚基类 见图：\n单一继承\n\r\n多重继承，derive class添加新的virtual func\n\r\n有虚基类（虚继承） 经典的钻石继承\n\r\n使用pointer 策略即bptr\n\r\n看到pPoint2d就是那个bptr\n使用table策略即通过在vptr内添加虚基类偏移找到\n\r\nData member 的地址 对一个Data member 取地址，取到的是data member在object内的偏移量。\n通过指向data member的指针来存取目标会更加耗时\n函数 nonstatic member function 为了获得和普通函数相同的调用效率，\n进行三步处理：\n 传一个对象this指针进去 内部对成员调用改为this-\u0026gt; 对函数名做mangling，起一个别的名字，一般来讲有class名称在内  nonstatic member function函数指针\n这也是一个需要绑定于object地址上才能调用的函数指针，也就是他本身是一种不完整的offset。\nvirtual member function 将虚函数调用转化为虚函数表的一项：\n(* ptr-\u0026gt;vptr[1])(this);\r单一继承下虚函数表布局 直接看图\n\r\n多继承下虚函数表布局 \r\n看到base1的虚函数表里居然有base2的虚函数，这是因为base1里的的虚函数表充当了derive class的主要虚函数表，base1 class就是主要实体，所以在他这个subobject内的虚函数表就需要一个完整的derive class能掉用到的虚函数表。\n那么如果将derive class object传给一个base2指针，因为是一个derive class，那么虚函数指针还是需要指向开头的主要实体的虚函数表这里，因为它可以调用所有的虚函数，这种设计给类似上面的行为提供了一种统一的做法，那就是：要访问虚函数，那就将调用指针移到开头去，从主要实体里找虚函数表。\n虚继承下虚函数表布局 \r\n明显虚继承下derive class的虚函数表也包含全部虚函数，虚基类的虚函数表也包含他自己有的全部虚函数，只是虚函数版本向derive class内的定义看齐\nvirtual member function函数指针\n 单一继承体系下  虚函数的地址只是一个它在虚函数表内的索引值。\n多继承体系下：  cfront的实现使用一个结构体与这个虚函数绑定，记录了他的表内索引和他所在的表的vptr，通过这俩找到这个虚函数\nstatic member function static member function直接被转换为非成员函数并调用\n不能存取class中的nonstatic members\n所以一般用于操作static member，因为static member一般不会被定义为public的，所以用一个function来操作很好，而普通成员函数又需要创建object才能用，static member function就不需要。\ninline function inline 函数会经历以下两步：\n 分析函数，决定是否能成为inline，如果不能就转换为static函数 在调用点上触发函数拓展，拓展时存在参数求值和临时对象管理问题  析构 首先最重要的一点：\n每一个derive class的析构函数会被编译器扩展，以静态调用的方式调用各层base class的虚析构函数（如果有的话），base class没有虚析构函数那就不调用了\n默认destructor 只有member 有一个destructor， 默认destructor才被合成，合成出来就是调用成员的destructor\n析构过程 \r\n还是这个图，Pvertex被析构时，他这个对象会依次“变成”各个基类，直到最后变成Point然后彻底消亡，vptr，object都会发生这种“蜕变”，很神奇。\n对象 全局对象 程序之始创建于data segment中，exit时销毁\n局部静态对象 local static object在c++ std要求下必须在这一局部被调用到才创建和赋值。且该对象只创建一次（就在被调用时），只销毁一次（整个程序退出时）。具体怎么实现被调用才创建就略了。\n对象数组 有一个循环的方法来依次创建和销毁数组中的每一个对象。\nnew和delete new： 配置内存malloc -\u0026gt; 构造\nplacement new： 在指定位置配置内存并构造\ndelete： 析构 -\u0026gt; 释放内存free\nRTTI 使用vtable的第一个slot装type_info这个object的地址\n以上就是使用dynamic_cast时所做的操作，他相比于static_cast更为耗时但安全，因为一旦类型不匹配那就转换失败。而static_cast就是强转。\n而对于非多态类或内建类型，这个type_info可能就需要单独定义，取得的时候就需要单独链接起来。\nTemplate 声明 + “具现”\n模板函数只声明，不使用编译器并不会“具现”他，也就是text 段中不会有这些函数。需要注意的是模板类中所有的member functions都会被具现出来，即使它们中的一些并未使用。在每个可以具现的地方具现，并在最后链接的时候只用一份，丢弃其他的，就可以缩小最终可执行文件的大小。\n一旦使用，那么type就确定，这个模板函数就可以转为普通函数来进行处理。\n但是模板类内部如果存在一个方法完全不依赖传入的type，也就是type不指定不影响方法的定义。那么这个方法内部很可能与当前程序文件的其他内容相关。比如那个方法调用了一个extern 函数其参数是double，虽然模板类内有个member是int（确定为int）的，方法内调用函数传的参数的也是这个int的member，但编译器还是会提前将这个extern函数链接上去，即使后面调用处有更合适的函数（一个extern 函数其参数是int），这里也不会再改变了。\n","date":"2021-01-18T00:00:00Z","image":"https://gao377020481.github.io/p/c-%E5%AF%B9%E8%B1%A1/380_hu950dcd652c2a06ef436608bca8fec292_6474607_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/c-%E5%AF%B9%E8%B1%A1/","title":"C++对象"},{"content":"JOS JOS不像其他操作系统一样在内核添加磁盘驱动，然后提供系统调用。我们实现一个文件系统进程来作为磁盘驱动。\n 引入一个文件系统进程（FS进程）的特殊进程，该进程提供文件操作的接口,并被赋予io权限。（x86处理器使用EFLAGS寄存器的IOPL为来控制保护模式下代码是否能执行设备IO指令，比如in和out。） 建立RPC机制，客户端进程向FS进程发送请求，FS进程真正执行文件操作，并将数据返回给客户端进程。 更高级的抽象，引入文件描述符。通过文件描述符这一层抽象就可以将控制台，pipe，普通文件，统统按照文件来对待。（文件描述符和pipe实现原理） 支持从磁盘加载程序并运行。  结构 superblock\n依然使用超级块来记录文件系统的元数据\nfile\nFile struct用来描述文件：文件名，大小，类型，保存文件内容的block号\ntips: Directories与file结构相同，只是内容是一些file\nBlock Cache 在FS进程的虚拟内存空间中映射一段磁盘区域。\nFS进程在创建时，set特殊的缺页处理函数。\n当发生缺页中断时call那个缺页处理函数，从磁盘上把数据读入物理内存。\n根据FS进程内存地址空间的映射关系，FS可以很方便的通过虚拟内存找到刚读入的数据在物理内存中的位置。\nThe Block Bitmap 磁盘块的是否使用的bitmap\nThe file system interface 文件系统建立好后，还需要通过ipc来构建供用户进程操作文件的API栈，课程的图拿来用一下：\n Regular env FS env\r+---------------+ +---------------+\r| read | | file_read |\r| (lib/fd.c) | | (fs/fs.c) |\r...|.......|.......|...|.......^.......|...............\r| v | | | | RPC mechanism\r| devfile_read | | serve_read |\r| (lib/file.c) | | (fs/serv.c) |\r| | | | ^ |\r| v | | | |\r| fsipc | | serve |\r| (lib/file.c) | | (fs/serv.c) |\r| | | | ^ |\r| v | | | |\r| ipc_send | | ipc_recv |\r| | | | ^ |\r+-------|-------+ +-------|-------+\r| |\r+-------------------+\rSpawning Processes spawn()创建一个新的进程，从文件系统加载用户程序，然后启动该进程来运行这个程序。spawn()就像UNIX中的fork()后面马上跟着exec():\n 从文件系统打开prog程序文件 调用系统调用sys_exofork()创建一个新的Env结构 调用系统调用sys_env_set_trapframe()，设置新的Env结构的Trapframe字段（该字段包含寄存器信息）。 根据ELF文件中program herder，将用户程序以Segment读入内存，并映射到指定的线性地址处。 调用系统调用sys_env_set_status()设置新的Env结构状态为ENV_RUNNABLE。  Linux Kernel VFS 虚拟文件系统，是一个中间层，向上给用户提供一致性的文件系统接口，向下兼容各类文件系统。\n规定了通用文件模型：\n结构  超级块对象： 存放已安装文件系统的元数据 索引节点对象（inode）：关于具体文件的一般信息，索引节点号唯一标识文件，记录文件操作函数指针。 文件对象file：打开文件会在内核生成的文件对象，通过fd可以找到 目录项对象dentry：存放目录项与对应文件进行链接的信息。  例子： 三个进程打开同一个文件，生成三个文件对象，其中两个进程用一个硬链接，所以使用两个目录对象，同一个文件所以使用一个inode，通过inode能找到超级块对象，然后来调用对应文件系统的方法IO，除此之外，尝试用的目录项对象还会被缓存在一个叫做目录cache里，方便下次直接取用。\n索引节点对象（inode）由链表组织。\nfile和dentry都没有磁盘映像，属于内核结构，显然slab里有对应的结构。\n对于进程查找路径上的每一个分量，都为其创建目录项对象，只是属于不同级：/var/init -\u0026gt; 第一级目录项对象：/ 第二级: var 第三级：init\n文件描述符： 文件描述符是进程对象task_struct里的files_struct这个记录进程的文件信息的结构内的一个成员：struct file*[] fd_array的索引。也就是说, 假如我们有一个文件描述符fd, 进程通过fd要操作文件，只需要去task_struct-\u0026gt;files_struct-\u0026gt;fd_array[fd]处查找，看有无对应的struct file*，找到那个指向内核缓存中的文件对象的指针。然后通过file找到对应的dentry再通过dentry找到inode再通过inode找到superblock，基于superblock解析inode数据找到磁盘上真正的存储位置。 fd_array的第一二项一般是标准输入输出文件，第三项一般是标准错误文件，其他一般指向打开文件\n所以进程可打开文件的数量，由这个数组限制。\n","date":"2020-12-07T00:00:00Z","image":"https://gao377020481.github.io/p/file-system/382_hu17f43c03682a00a7ede4cbea93286be4_8146359_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/file-system/","title":"File System"},{"content":"JOS 多CPU支持 支持\u0026quot;symmetric multiprocessing\u0026quot; (SMP)， 启动阶段CPU分为BSP和AP，BSP负责初始化系统和启动操作系统，AP由BSP激活。哪一个CPU是BSP由硬件和BISO决定，到目前位置所有JOS代码都运行在BSP上。在SMP系统中，每个CPU都有一个对应的local APIC（LAPIC），负责传递中断。CPU通过内存映射IO(MMIO)访问它对应的APIC，这样就能通过访问内存达到访问设备寄存器的目的。BSP读取mp configuration table中保存的CPU信息，初始化cpus数组，ncpu（总共多少可用CPU），bootcpu指针（指向BSP对应的CpuInfo结构）。然后BSP通过在内存上写值向AP传递中断来启动其他cpu（为他们设置寄存器值等操作）。\nCPU私有数据：1.内核栈 2.TSS 3.env 4.寄存器\n显然以上私有数据都需要创建新的一份。\n多CPU执行内核代码，需要锁来避免竞争，使用CAS机制实现一个内核锁就可以。当然这个粒度太大，在linux内核中有各种粒度的实现。\n协作调度 实现yield函数由进程调用主动让出cpu，显然需要将yield注册为一项系统调用，由内核来真正的做切换进程的工作。这里的调度也就是最简单的FIFO。\nfork 提供系统调用fork给用户创建进程的能力，fork()拷贝父进程的地址空间和寄存器状态到子进程。父进程从fork()返回的是子进程的进程ID，而子进程从fork()返回的是0。父进程和子进程有独立的地址空间，任何一方修改了内存，不会影响到另一方。\n基于写时复制的原理，子进程只需要在一开始拷贝父进程的页目录就可以，当真正触发写操作的时候再在缺页处理函数里做真正的拷贝。因为用户进程中已经有拷贝需要的所有信息（物理页位置等），所以只需要在用户进程中调用用户进程自己的缺页处理函数就可以。所以：\n 需要在进程fork的时候就设置新进程的缺页处理函数 同时fork的时候也要对页复制做对应处理（共享，写时复制，只读三种情况不同） 因为要在用户进程中处理异常，所以需要新建一个用户异常栈保存用于异常处理的函数需要的参数。 缺页中断发生时：trap()-\u0026gt;trap_dispatch()-\u0026gt;page_fault_handler() 这个page_fault_handler会进入汇编代码然后给用户异常栈赋好值再切换到用户栈，基于刚赋好的值，用户进程会直接执行真正的用户缺页处理函数，在这个却也处理函数里会判断是否因为写时复制导致的触发，是的话就拷贝这个物理页到新的地方然后建立虚拟地址到物理页的映射关系。 还有一点需要注意的是内核代码组织十分严格，所以默认内核态不会出现缺页异常，一旦出现可能内核被攻击了，所以在一开始page_fault_handler里需要判断由内核进程触发的话就要panic整个系统。  定时 外部时钟中断强制进入内核，内核判断当前周期到了没，可以将中断号+偏移量来控制时钟周期，到了就触发对应的处理函数。拿时间片轮转调度进程举例：在SMP上首先通过LAPIC来通知各个cpu然后让出进程。\nIPC Inter-Process communication\n进程间通信，这里进程间通信使用使两个进程的虚拟地址指向同一块物理页的机制来完成。调用recv的进程阻塞（让出cpu），调用send的进程陷入内核查找对应的recv进程，和其要接受到的虚拟地址，首先将要发送的物理地址找到，然后修改recv进程的要接受到的虚拟地址对应的页表项，将其映射到那个要发送的物理地址处。然后设置接收进程为就绪态等待内核调度。\nLinux Kernel 涉及进程调度、锁、进程通信\n进程调度 调度器 核心调度器\n调度器的实现基于两个函数：周期性调度器函数和主调度器函数。这些函数根据现有进程的优先级分配CPU时间。这也是为什么整个方法称之为优先调度的原因。\na.主调度器函数 在内核中的许多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数（schedule）。 主调度器负责将CPU的使用权从一个进程切换到另一个进程。周期性调度器只是定时更新调度相关的统计信息。cfs队列实际上是用红黑树组织的，rt队列是用链表组织的。\nb.周期性调度器函数\n周期性调度器在scheduler_tick中实现，如果系统正在活动中，内核会按照频率HZ自动调用该函数。该函数主要有两个任务如下：\n 更新相关统计量：管理内核中与整个系统和各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1。比如运行时间。 激活负责当前进程的调度类的周期性调度方法。  调度类\n为方便添加新的调度策略，Linux内核抽象一个调度类sched_class，允许不同进程有针对性的选择调度算法。\n运行队列\n每个处理器有一个运行队列，结构体是rq。rq是描述就绪队列，其设计是为每一个CPU就绪队列，本地进程在本地队列上排序。cfs和rt。\n调度进程\n主动调度进程的函数是schedule() ，它会把主要工作委托给__schedule()去处理。\n函数__shcedule的主要处理过程如下：\n  调用pick_next_task()以选择下一个进程。\n  调用context_switch()以切换进程。 调用context_switch：\n  切换用户虚拟地址空间\n  切换寄存器\n  主动调度\n即JOS中的主动让出，依赖系统调用\n周期调度\n内核依赖时钟来调度，JOS中也有\nSMP调度 这个调度旨在\n 均衡多处理器的负载 可以设置进程的处理器亲和性，即允许进程在哪些处理器上执行。 可以把进程从一个处理器迁移到另一个处理器。  进程的迁移只能发生在同一调度域内，调度域由若干个CPU组成\n锁 用于保护内核数据\n 原子操作：这些是最简单的锁操作。它们保证简单的操作，诸如计数器加1之类，可以不中断地原子执行。即使操作由几个汇编语句组成，也可以保证。 自旋锁：这些是最常用的锁选项。它们用于短期保护某段代码，以防止其他处理器的访问。在内核等待自旋锁释放时，会重复检查是否能获取锁，而不会进入睡眠状态（忙等待）。当然，如果等待时间较长，则效率显然不高。 信号量：这些是用经典方法实现的。在等待信号量释放时，内核进入睡眠状态，直至被唤醒。唤醒后，内核才重新尝试获取信号量。互斥量是信号量的特例，互斥量保护的临界区，每次只能有一个用户进入。 读者/写者锁：这些锁会区分对数据结构的两种不同类型的访问。任意数目的处理器都可以对数据结构进行并发读访问，但只有一个处理器能进行写访问。事实上，在进行写访问时，读访问是无法进行的。  IPC 管道\n管道可以看为文件，有文件描述符，但是在系统目录树上无法找到，因为它存在于一个特殊的vfs：pipefs（因为是vfs对象，所以无磁盘映像）中。所以在已安装的文件系统中没有相应的映像，可以使用pipe系统调用创建新管道，他返回一对文件描述符，然后进程通过fork将文件描述符传递给它的子进程，由此与进程共享管道。进程使用read读第一个fd，write写第二个fd，一个fd用来读一个用来写。\nFIFO\nFIFO在文件系统中有磁盘索引节点，虽然不占用数据块但是与内核的一个缓冲区关联，数据就在这个缓冲区里。而且FIFO的fd只有一个，可以read和write使用一个fd。\nSystem V IPC\n三种：\n 操作信号量同步其他进程 发送消息或接收消息（存放在消息队列中） 与其他进程share一段内存（JOS相似）  POSIX消息队列\n相比于System V IPC消息队列，POSIX消息队列：\n 基于文件的应用接口 支持消息优先级 用于阻塞接收发送的超时机制 支持消息到达的异步通知（信号或线程创建来实现）  ","date":"2020-11-16T00:00:00Z","image":"https://gao377020481.github.io/p/preemptive-multitasking/421_hu0aeeb71c7e367c906246b475dc53a127_3637554_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/preemptive-multitasking/","title":"Preemptive Multitasking"},{"content":"JOS 这一部分做三件事：\n 建立进程概念 异常处理，陷入内核 系统调用  进程和用户环境 JOS里进程是一个env结构，里面保存了简单的信息：页目录、寄存器缓存项（用于切换），连接指针（用于调度），状态等\n首先提供基础设施：内核中保存进程的链表，这个数组需要初始化在物理内存上\n显然创建一个进程，需要创建其对应的页目录，那就要在物理内存上分配对应的页目录和页表结构然后把他映射到虚拟内存里的页目录上。\n要在进程上运行程序，就需要加载和解析ELF文件，同样分配物理页，载入elf文件各个segment，建立虚拟地址到物理地址的映射关系，然后修改寄存器的值为elf的entry。就可以运行起来elf上的程序了。\n运行一个进程就只需要将进程结构内保存的寄存器值弹出到寄存器里。\ntrap与异常处理 一般来讲当异常发生，cpu会触发电信号，触发硬中断然后由中断控制器找到中断处理函数，但也有软中断的时候，通过指令提供的中断号结合IDT来查找到对应的中断处理函数。\n中断发生，陷入内核，处理器根据TSS寄存器找到TSS结构，将栈寄存器SS和ESP分别设置为其中的SS0和ESP0两个字段的值，这样就切换到内核栈了\n缺页异常与系统调用 缺页异常是一个中断，中断号是14，将这个号压入内核栈，然后call trap函数，在trap里dispatch到对应的处理函数就行。缺页的话就要分配物理页然后加载磁盘数据再建立映射关系，这一套操作目前由内核完成。\n系统调用是一个中断，我们设置中断处理函数sys_call，并在trap内部根据传入的系统调用号做对应的dispatch到对应的系统调用处理。上面内核栈已经切换成功，其实cpu就在内核态了，接下来只需要压入触发系统调用对应的int指令需要的中断号和其它参数就可以call trap，然后dispatch到sys_call并传递参数（参数保存在一个trapframe结构中）。\nLinux Kernel 进程 Linux内核把进程称为任务(task)，进程的虚拟地址空间分为用户虚拟地址空间和内核虚拟地址空间，所有进程共享内核虚拟地址空间，每个进程有独立的用户空间虚拟地址空间。\n进程有两种特殊形式：没有用户虚拟地址空间的进程称为内核线程，共享用户虚拟地址空间的进程称为用户线程。通用在不会引起混淆的情况下把用户线程简称为线程。共享同一个用户虚拟地址空间的所有用户线程组成一个线程组。\n四要素：\na.有一段程序供其执行。\nb.有进程专用的系统堆栈空间。\nc.在内核有task_struct数据结构。\nd.有独立的存储空间，拥有专有的用户空间。\n如果只具备前三条而缺少第四条，则称为“线程”。如果完全没有用户空间，就称为“内核线程”。而如果共享用户空间映射就称为“用户线程”。内核为每个进程分配一个task_struct结构时。实际分配两个连续物理页面(8192字节)，数据结构task_struct的大小约占1kb字节左右，进程的系统空间堆栈的大小约为7kb字节（不能扩展，是静态确定的）。\n","date":"2020-10-27T00:00:00Z","image":"https://gao377020481.github.io/p/user-environments/644_huaedcef13087ef596fe24c2fe63226878_25333423_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/user-environments/","title":"User Environments"},{"content":"内存管理\nJOS JOS中主要是在做：\n 线性地址到物理地址的映射（通过操作修改页表项实现） 在物理内存上划分出内核的页目录和页表存储区域并完成映射 保护模式的映射关系与段寄存器相关，32位的JOS通过段选择子找到段描述符，再解析段描述符的基地址，检查权限位后加上虚拟地址来实现 物理内存的分配在JOS内目前还无相应算法，只是单纯的连续分配  Linux kernel 最上层拿C++来说，其STL用于容器的alloc分配器的实现，又是基于malloc的，使用开链数组来存放多个线性增长大小的空闲内存块，当有分配需要时一次申请多块（应该是20块）该大小内存。且找不到对应块时还具有相应的回收，拼接，查找功能。对于大块内存的分配使用单独的路径来分配和存储。分层分级分区的思路是各级内存分配器的惯用思路。\n然后低一层的malloc根据选择动态链接库的不同（就使用不同的分配器lib），有不同实现，以下以glibc为例。\n至于内核中内存分配，由于其确定性，多用slab来做分配，也存在buddy allocation机制来管理。\n虚拟地址一般都是大于物理地址的，所以只需要搞好虚拟地址上的内存管理就行了，至于物理页如果dirty，自然有page fault interrupt 来操心。当然以下提到物理页连续等，是默认了虚拟页到物理页的映射关系，实际上mmap,brk这样的系统调用都不会真的操作物理页，很多时候都是系统告诉我们：“你要的物理页已经准备好了”，实际上他也就是改了一下task_struct里mm_struct里的一些页表项罢了（准备好的是映射，真正的物理页可还是dirty的）。\n所以内存分配器到底在哪一层呢，实际上内存分配器能看到的还就是虚拟内存，他一直在做修改页映射的操作，他并不会做将某个文件放到内存里的工作，这个工作显然是由缺页中断完成的紧跟其后的就是磁盘IO（以一个read操作为例\u0026ndash;，read -\u0026gt; vfs_read(首先要找到inode: 顺序是：check fd 在不在进程的打开文件列表里， 在的话找到file结构然后顺着找到dentry接着找到inode，check一下在不在page cache里这里就要通过inode里的address_space找到那颗组织page cache的基数树在上面查询了，查询用的是文件偏移量，不在就通过VFS里的inode做对应文件系统的operation，以xfs为例子，那就是解析inode数据结构，首先肯定要去它的data fork去找，xfs的data fork组织成两种形式，extent list or b+ tree, 无论是哪一种都能找到所有相关的block号了，一个一个把他们读出来就行了，这个工作显然是交给通用块设备层去做了，这层有点像VFS，下面马上就是各种不同的硬件驱动，上面是统一的请求，所以这一层用于整合，再下去就是到IO调度层，内核根据一些策略来做IO调度，这是因为磁盘读一次比较慢，尽量把连续扇区放一起读更快，再下去就真的从磁盘上读数据了。），只是它感性的认为自己在操作物理内存，比如：“太好了，这次我分配了0X10到0X2000这么多连续的内存空间”，实际上他就是把0X10到0X2000这一块虚拟内存的页表项改了一下，让虚拟页映射到了某一段连续物理页，实际的物理的IO并未发生呢\nglibc (example for user-space) 以ptmalloc为例：\nmalloc时，如果内存分配区（主，非主）真的无空间，调用mmap或brk/sbrk来在虚拟内存上取一块作为新的空间，mmap取得内存在mmap区域，brk/sbrk在heap上（在进程的描述符task_struct中的mm_struct里可以找到）。主分配区可以使用sbrk和mmap向os申请内存，而非分配区只能通过mmap向os申请内存。接下来os可能（因为mmap只是给一块虚拟地址，真实数据还未拷贝到物理内存上）要去物理内存上找可用区块然后将真实数据拷贝上去（用户触发缺页syscall），这里再涉及物理页的分配。\nptmalloc的内存管理方法就可以很好的解决每次要内存都要去向OS要的问题。它回收管理分配出的内存而不是每次都还给操作系统而是放入bin中留待下次使用，bin的整理等一些其他操作会在特定时候触发。\n首先每个进程都有一个主分配区和几个非主分配区，分配区在多线程间共享，对分配区的操作需要加线程互斥锁。主分配区由一号或主线程拥有。\n最小的内存管理单位为chunk，一段连续的内存被分成多个chunk，chunk内记录当前和前一个chunk的大小，用于合并。\n下一层级是分配区中的bins，bins分为:\n fast bin: 保存小块的chunk bins: 2.1 unsorted bin： 是一个chunk缓冲区，这里的chunk都是在等待整理的chunk（释放或合并出来的），同时也有可能是即将被用得到的chunk，因为很多程序会频繁的分配释放相同大小的内存，它在fastbin里找不到就会直接来这里找，速度快。chunk的size 没有限制。 2.2 small bin： 类似alloc分配器的开链数组实现，大小小于512字节的chunk被称为small chunk，分级每个相差8KB放入small bin对应槽位。共62个不同大小的bin 2.3 large bin： 与small bin类似，只是其中存的是大chunk，且不单纯线性增长，共63个不同大小的bin  chunk的分配与释放是一个很复杂的管理流程，这里只说管理层级，不谈细致流程。\nkernel space buddy 连续的物理页称为页块（page block），阶（order）是页的数量单位，2的n次方个连续页称为n阶页块。 如下条件的两个n阶页块称为伙伴（buddy）：\n 两个页块是相邻的，即物理地址是连续的； 页块的第一页的物理面页号必须是2的n次方的整数倍； 如果合并（n+1）阶页块，第一页的物理页号必须是2的括号(n+1)次方的整数倍。  分配的基本单位是n阶页块，不存在的话就去更高阶找，找到后拆开，都找不到就要使用非连续页分配机制，多次调用更低阶的页块分配\n物理内存首先被分为不同zone，并对zone维护一个使用率，根据区域水线来决定是否从其他区域借用物理页。\na. 高水线（HIGH）：如果内存区域的空闲页数大于高水线，说明该内存区域的内存充足。\nb. 低水线（LOW）：如果内存区域的空闲页数小于低水线，说明该内存区域的内存轻微不足。\nc. 最低水线（MIN）：如果内存区域空闲页数小于最低水线，说明该内存区域的内存严重不足。\n通用页分配入口\n__alloc_pages_nodemask\nslab slab分配器的作用不仅仅是分配小块内存，更重要的作用是针对经常分配和释放的对象充当缓存。slab分配器的 核心思路是：为每种对象类型创建一个内存缓存，每个内存缓存由多个大块组成，一个大块是由一个或多个连 续的物理页，每个大块包含多个对象。slab采用面向对象的思想，基于对象类型管理内存，每种对象被划分为一 类，比如进程描述符task_struct是一个类，每个进程描述符实例是一个对象。\n由于内核中各对象大小固定，所以该分配器给内核的物理内存分配提供了高效率。\n管理的对象缓存在slab cache中。\n多处理器情况：\n内存缓存为每个处理器创建一个数组缓存（结构体array_cahce）。释放对象时，把对象存放到当前处理器对应 的数组缓存中；分配对象的时候，先从当前处理器的数组缓存分配对象，采用后进先出（Last In First Out， LIFO）的原则，这种做可以提高性能。\n编程接口：\n分配内存kmalloc、重新分配内存krealloc、释放内存kfree\n非连续页 就是：\n 分配虚拟内存区域 分配不连续的物理内存区域并映射到连续的虚拟内存上（多次的低阶页块分配，基于buddy算法因为是多次不同大小的分配）  编程接口：\nvmalloc:分配不连续的物理页并且把物理页映射到连续的虚拟地址空间\nvfree:释放vmalloc分配的物理页和虚拟地址空间\nvmap:把已经分配的不连续物理页映射到连续的虚拟地址空间\nvunmap:释放使用vmap分配的虚拟地址空间\n供内核使用：\nkvmalloc：首先尝试使用kmalloc分配内存块，如果失败，那么使用vmalloc函数分配不连续的物理页\nkvfree：如果内存块是使用vmalloc分配的，那么使用vfree释放，否则使用kfree释放\n","date":"2020-10-04T00:00:00Z","image":"https://gao377020481.github.io/p/memory-management/317_hu87877ffe51e03f01d65ea0778bc1cc62_4684892_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/memory-management/","title":"Memory Management"},{"content":"以MIT6.828的分节方式来记录操作系统的笔记。\n所以第一章： Boot 启动\nJOS 在JOS里，Boot作为Lab1存在。\n实模式下运行BIOS，BIOS设置eip以启动bootloader，boot loader检查系统状态，开启保护模式，将磁盘上的kernel加载入物理内存，设置eip启动kernel。Kernel开启分页。\nBIOS BIOS的启动依赖于硬件的电信号，在qemu虚拟机里模拟了这样一个信号来启动BIOS。\nBootloader Bootloader：\n 从实模式进入保护模式，加载全局描述符表（虚拟地址到线性（物理地址）的映射开启） 从磁盘加载kernel到内存（通过读取ELF文件的方式）  Kernel 进入Kernel后：\n 开启分页（就是在物理内存的特定位置创建内核页目录和页表数组，实现线性地址到物理地址的映射关系） 这里还基于内存映射的关系，实现了向IO HOLE区域（VGA显存）写值的功能，得以在终端上输出了字符  Linux kernel 在linux kernel中，这一环节的基本流程很相似，参考深入理解Linux内核附录1记录一个很简要的流程：\nBIOS 硬件电信号拉起ROM上的BIOS程序，BIOS启动后简单检查和初始化一下硬件，随后在磁盘扇区上搜索操作系统来启动，找到磁盘第一个扇区（引导扇区）后将其拷贝到物理内存的0X00007C00处。\nBootloader 物理内存上有bootloader的第一部分了，第一部分可能会移动他的位置并将第二部分再装入物理内存的特定位置，第二部分会从磁盘中读取OS的映射表，提供给用户选项，选择启动哪一个操作系统，选中后bootloader就会调用BIOS过程来不停的装载内核映像，其中setup()函数在0X00090200处，下一步会跳转到这里\nsetup() 初始化硬件、和一些寄存器等，并跳转到startup_32()\nstartup_32() 初始化临时堆栈，段寄存器并解压内核映像放置到物理内存上，然后跳转到内核映像上启动\n解压的内核映像启动点仍是一个叫做startup_32()的函数，它会再检查一下硬件软件信息然后的跳转到start_kernel()函数\nstart_kernel() 完成linux内核初始化工作，具体工作过多，这里不说\n","date":"2020-09-20T00:00:00Z","image":"https://gao377020481.github.io/p/boot/365_hu00b5f61c23d3e384386ac3998acfa3c7_4845036_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/boot/","title":"Boot"},{"content":"视图 视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。其内容由查询定义。 基表：用来创建视图的表叫做基表。 通过视图，可以展现基表的部分数据。 视图数据来自定义视图的查询中使用的表，使用视图动态生成。\n优点  简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。  CREATEVIEW\u0026lt;视图名\u0026gt;AS\u0026lt;SELECT语句\u0026gt;触发器 触发器（trigger）是MySQL提供给程序员和数据分析员来保证数据完整性的一种方法，它是与表事件相关的特殊的存储过程，它的执行不是由程序调用，也不是手工启动，而是由事件来触发，比如当对一个表进行DML操作（ insert ， delete ， update ）时就会激活它执行。\n监视对象： table\n监视事件： insert 、 update 、 delete\n触发时间： before ， after\n触发事件： insert 、 update 、 delete\nCREATETABLE`work`(`id`INTPRIMARYKEYauto_increment,`address`VARCHAR(32))DEFAULTcharset=utf8ENGINE=INNODB;CREATETABLE`time`(`id`INTPRIMARYKEYauto_increment,`time`DATETIME)DEFAULTcharset=utf8ENGINE=INNODB;CREATETRIGGERtrig_test1AFTERINSERTON`work`FOREACHROWINSERTINTO`time`VALUES(NULL,NOW());存储过程 SQL语句需要先编译然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。\n存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。\n优点  能完成较复杂的判断和运算 有限的编程 可编程行强，灵活 SQL编程的代码可重复使用 执行的速度相对快一些 减少网络之间的数据传输，节省开销  CREATEPROCEDURE过程名([[IN|OUT|INOUT]参数名数据类型[,[IN|OUT|INOUT]参数名数据类型…]])[特性...]过程体存储过程根据需要可能会有输入、输出、输入输出参数，如果有多个参数用\u0026quot;,\u0026ldquo;分割开。\nMySQL 存储过程的参数用在存储过程的定义，共有三种参数类型 IN , OUT , INOUT 。\nIN ：参数的值必须在调用存储过程时指定，0在存储过程中修改该参数的值不能被返回，可以设置默认值\nOUT ：该值可在存储过程内部被改变，并可返回\nINOUT ：调用时指定，并且可被改变和返回\n过程体的开始与结束使用 BEGIN 与 END 进行标识。\n游标 游标是针对行操作的，对从数据库中 select 查询得到的结果集的每一行可以进行分开的独立的相同或者不相同的操作。\n对于取出多行数据集，需要针对每行操作；可以使用游标；游标常用于存储过程、函数、触发器、事件。\n游标相当于迭代器\n直接来个例子就知道：\nCREATEPROCEDUREproc_while(INage_inINT,OUTtotal_outINT)BEGIN-- 创建 用于接收游标值的变量 DECLAREp_id,p_age,p_totalINT;DECLAREp_sexTINYINT;-- 注意:接收游标值为中文时,需要给变量 指定字符集utf8 DECLAREp_nameVARCHAR(32)CHARACTERSETutf8;-- 游标结束的标志 DECLAREdoneINTDEFAULT0;-- 声明游标 DECLAREcur_teacherCURSORFORSELECTteacher_id,teacher_name,teacher_sex,teacher_ageFROMteacherWHEREteacher_age\u0026gt;age_in;-- 指定游标循环结束时的返回值 DECLARECONTINUEHANDLERFORNOTfoundSETdone=1;-- 打开游标 OPENcur_teacher;-- 初始化 变量 SETp_total=0;-- while 循环 WHILEdone!=1DOFETCHcur_teacherINTOp_id,p_name,p_sex,p_age;IFdone!=1THENSETp_total=p_total+1;ENDIF;ENDWHILE;-- 关闭游标 CLOSEcur_teacher;-- 将累计的结果复制给输出参数 SETtotal_out=p_total;END//delimiter;-- 调用 SET@p_age=20;CALLproc_while(@p_age,@total);SELECT@total;索引 分类：主键索引、唯一索引、普通索引、组合索引、以及全文索引；\n主键索引 非空唯一索引，一个表只有一个主键索引；在 innodb 中，主键索引的B+树包含表数据信息。\nPRIMARYKEY(key)唯一索引 不可以出现相同的值，可以有NULL值。\nUNIQUE(key)普通索引 允许出现相同的索引内容。\nINDEX(key)-- OR KEY(key[,...])组合索引 对表上的多个列进行索引。 符合最左匹配原则：从左到右依次匹配，遇到 \u0026gt; \u0026lt; between like 就停止匹配；\nINDEXidx(key1,key2[,...]);UNIQUE(key1,key2[,...]);PRIMARYKEY(key1,key2[,...]);全文索引 将存储在数据库当中的整本书和整篇文章中的任意内容信息查找出来的技术；关键词 FULLTEXT; 在短字符串中用 LIKE % ；在全文索引中用 match 和 against。\n主键选择规则 innodb 中表是索引组织表，每张表有且仅有一个主键。\n 如果显示设置 PRIMARY KEY ，则该设置的key为该表的主键。 如果没有显示设置，则从非空唯一索引中选择。  只有一个非空唯一索引，则选择该索引为主键。 有多个非空唯一索引，则选择声明的第一个为主键。   没有非空唯一索引，则自动生成一个 6 字节的 _rowid 作为主键。  索引实现 索引存储 innodb由段、区、页组成；段分为数据段、索引段、回滚段等。区大小为 1 MB（一个区由64个连续页构成）。页的默认值为16k。页为逻辑页，磁盘物理页大小一般为 4K 或者 8K。为了保证区中的页的连续，存储引擎一般一次从磁盘中申请 4~5 个区。\n聚集索引与辅助索引 聚集索引就是以主键为key构造B+树\n辅助索引就是以辅助索引为key构造B+树，value为对应的主键\n页和B+树 B+树单个节点是页，页中由id再向下分裂指向多个页。\n我们知道主键索引的id就是B+树节点的key，为了范围查询我们不使用hashmap，那为什么不用B树呢？\n这是因为B树在叶子节点上保存数据，执行范围查询时，可能需要反复载入不连续的页，cache和内存的利用率都不是很高。而B+树直接进入叶子节点，然后范围查询只需要根据叶子节点之间的指针载入连续的页就行，这种确定的内存访问优化空间更大，效率更高。\n索引失效  select \u0026hellip; where A and B 若 A 和 B 中有一个不包含索引，则索引失效。 索引字段参与运算，则索引失效；例如： from_unixtime(idx) = \u0026lsquo;2021-04-30\u0026rsquo;。 索引字段发生隐式转换，则索引失效；例如： \u0026lsquo;1\u0026rsquo; 隐式转换为 1 。 LIKE 模糊查询，通配符 % 开头，则索引失效；例如： select * from user where name like \u0026lsquo;%ark\u0026rsquo;。 在索引字段上使用 NOT \u0026lt;\u0026gt; != 索引失效；如果判断 id \u0026lt;\u0026gt; 0 则修改为 idx \u0026gt; 0 or idx \u0026lt; 0 。 组合索引中，没使用第一列索引，索引失效。  索引原则  查询频次较⾼且数据量⼤的表建⽴索引；索引选择使⽤频次较⾼，过滤效果好的列或者组合。 使⽤短索引；节点包含的信息多，较少磁盘io操作。 对于很长的动态字符串，考虑使用前缀索引。 对于组合索引，考虑最左侧匹配原则和覆盖索引。 尽量选择区分度⾼的列作为索引；该列的值相同的越少越好。 尽量扩展索引，在现有索引的基础上，添加复合索引。 不要 select * ； 尽量只列出需要的列字段。 索引列，列尽量设置为非空。  约束 为了实现数据的完整性，对于innodb，提供了以下几种约束，primary key，unique key，foreign key， default, not null。\n外键约束 外键用来关联两个表，来保证参照完整性；MyISAM存储引擎本身并不支持外键，只起到注释作用。而innodb完整支持外键。\n约束于索引的区别 创建主键索引或者唯一索引的时候同时创建了相应的约束；但是约束是逻辑上的概念；索引是一个数据结构既包含逻辑的概念也包含物理的存储方式；\n事务 事务将数据库从一种一致性状态转换为另一种一致性状态。\n在数据库提交事务时，可以确保要么所有修改都已经保存，要么所有修改都不保存。\n事务是访问并更新数据库各种数据项的一个程序执行单元。\n在 MySQL innodb 下，每一条语句都是事务。可以通过 set autocommit = 0， 设置当前会话手动提交。\n-- 显示开启事务 STARTTRANSACTION|BEGIN-- 提交事务，并使得已对数据库做的所有修改持久化 COMMIT-- 回滚事务，结束用户的事务，并撤销正在进行的所有未提交的修改 ROLLBACK-- 创建一个保存点，一个事务可以有多个保存点 SAVEPOINTidentifier-- 删除一个保存点 RELEASESAVEPOINTidentifier-- 事务回滚到保存点 ROLLBACKTO[SAVEPOINT]identifierACID特性 原子性（A） 事务操作要么都做（提交），要么都不做（回滚）。事务是访问并更新数据库各种数据项的一个程序执行单元，是不可分割的工作单位。通过undolog来实现回滚操作。undolog记录的是事务每步具体操作，当回滚时，回放事务具体操作的逆运算。\n隔离性（I） 事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离，也就是事务提交前对其他事务都不可见。通过 MVCC 和 锁来实现。MVCC 时多版本并发控制，主要解决一致性非锁定读，通过记录和获取行版本，而不是使用锁来限制读操作，从而实现高效并发读性能。锁用来处理并发 DML 操作。数据库中提供粒度锁的策略，针对表（聚集索引B+树）、页（聚集索引B+树叶子节点）、行（叶子节点当中某一段记录行）三种粒度加锁。\n持久性（D） 事务提交后，事务DML操作将会持久化（写入redolog磁盘文件 哪一个页 页偏移值 具体数据）。即使发生宕机等故障，数据库也能将数据恢复。redolog记录的是物理日志。\n一致性（C） 一致性指事务将数据库从一种一致性状态转变为下一种一致性的状态，在事务执行前后，数据库完整性约束没有被破坏。例如：一个表的姓名是唯一键，如果一个事务对姓名进行修改，但是在事务提交或事务回滚后，表中的姓名变得不唯一了，这样就破坏了一致性。一致性由原子性、隔离性以及持久性共同来维护的。\n事务并发异常 脏读 事务（A）可以读到另外一个事务（B）中未提交的数据，也就是事务A读到脏数据。在读写分离的场景下，可以将slave节点设置为 READ UNCOMMITTED。此时脏读不影响，在slave上查询并不需要特别精准的返回值。\n不可重复读 事务（A) 可以读到另外一个事务（B）中提交的数据。通常发生在一个事务中两次读到的数据是不一样的情况。不可重复读在隔离级别 READ COMMITTED 存在。一般而言，不可重复读的问题是可以接受的，因为读到已经提交的数据，一般不会带来很大的问题，所以很多厂商（如Oracle、SQL Server）默认隔离级别就是READ COMMITTED。\n幻读 事务中一次读操作不能支撑接下来的业务逻辑。通常发生在一个事务中一次读判断接下来写操作失败的情况。例如：以name为唯一键的表，一个事务中查询 select * from t where name =\u0026lsquo;mark\u0026rsquo;。不存在，接下来 insert into t(name) values (\u0026lsquo;mark\u0026rsquo;)。 出现错误，此时另外一个事务也执行了 insert 操作。幻读在隔离级别 REPEATABLE READ 及以下存在。但是可以在REPEATABLE READ 级别下通过读加锁（使用next-key locking）解决。\n隔离级别 ISO和ANIS SQL标准制定了四种事务隔离级别的标准，各数据库厂商在正确性和性能之间做了妥协，并没有严格遵循这些标准。MySQL innodb默认支持的隔离级别是 REPEATABLE READ。\nREAD UNCOMMITTED 读未提交：该级别下读不加锁，写加排他锁，写锁在事务提交或回滚后释放锁。\nREAD COMMITTED 读已提交：从该级别后支持 MVCC (多版本并发控制)，也就是提供一致性非锁定读。此时读取操作读取历史快照数据。该隔离级别下选择读取快照中历史版本的最新数据，所以读取的是已提交的数据。\nREPEATABLE READ 可重复读：该级别下也支持 MVCC，此时读取操作读取事务开始时的版本数据。\nSERIALIZABLE 可串行化：该级别下给读加了共享锁。所以事务都是串行化的执行，此时隔离级别最严苛。\n-- 设置隔离级别 SET[GLOBAL|SESSION]TRANSACTIONISOLATIONLEVELREPEATABLEREAD;-- 或者采用下面的方式设置隔离级别 SET@@tx_isolation=\u0026#39;REPEATABLE READ\u0026#39;;SET@@global.tx_isolation=\u0026#39;REPEATABLE READ\u0026#39;;-- 查看全局隔离级别 SELECT@@global.tx_isolation;-- 查看当前会话隔离级别 SELECT@@session.tx_isolation;SELECT@@tx_isolation;-- 手动给读加 S 锁 SELECT...LOCKINSHAREMODE;-- 手动给读加 X 锁 SELECT...FORUPDATE;-- 查看当前锁信息 SELECT*FROMinformation_schema.innodb_locks;锁 锁机制用于管理对共享资源的并发访问，用来实现事务的隔离级别。\n粒度 共享锁和排他锁都是行级锁。MySQL当中事务采用的是粒度锁。针对表（B+树）、页（B+树叶子节点）、行（B+树叶子节点当中某一段记录行）三种粒度加锁。\n意向共享锁和意向排他锁都是表级别的锁。\n共享锁（S） 事务读操作加的锁，对某一行加锁。\n在 SERIALIZABLE 隔离级别下，默认帮读操作加共享锁。\n在 REPEATABLE READ 隔离级别下，需手动加共享锁，可解决幻读问题。\n在 READ COMMITTED 隔离级别下，没必要加共享锁，采用的是 MVCC。\n在 READ UNCOMMITTED 隔离级别下，既没有加锁也没有使用 MVCC。\n排他锁（X） 事务删除或更新加的锁；对某一行加锁。\n在4种隔离级别下，都添加了排他锁，事务提交或事务回滚后释放锁。\n意向共享锁（IS） 对一张表中某几行加的共享锁。\n意向排他锁（IX） 对一张表中某几行加的排他锁。\n兼容性 由于innodb支持的是行级别的锁，意向锁并不会阻塞除了全表扫描以外的任何请求。\n意向锁之间是互相兼容的。\nIX 对共享锁和排他锁都不兼容。\nIS 只对排他锁不兼容。\n当想为某一行添加 S 锁，先自动为所在的页和表添加意向锁 IS，再为该行添加 S 锁。\n当想为某一行添加 X 锁，先自动为所在的页和表添加意向锁 IX，再为该行添加 X 锁。\n锁算法  Record Lock: 记录锁，单个行记录上的锁。 Gap Lock： 间隙锁，锁定一个范围，但不包含记录本身。全开区间。REPEATABLE READ级别及以上支持间隙锁。 Next-Key Lock： 记录锁+间隙锁，锁定一个范围，并且锁住记录本身。左开右闭区间。 Insert Intention Lock： 插入意向锁，insert操作的时候产生。在多事务同时写入不同数据至同一索引间隙的时候，并不需要等待其他事务完成，不会发生锁等待。 AUTO-INC Lock： 自增锁，是一种特殊的表级锁，发生在 AUTO_INCREMENT 约束下的插入操作。采用的一种特殊的表锁机制。完成对自增长值插入的SQL语句后立即释放。在大数据量的插入会影响插入性能，因为另一个事务中的插入会被阻塞。从MySQL 5.1.22开始提供一种轻量级互斥量的自增长实现机制，该机制提高了自增长值插入的性能。  MVCC 多版本并发控制，用来实现一致性的非锁定读。非锁定读是指不需要等待访问的行上X锁的释放。\n在 read committed 和 repeatable read下，innodb使用MVCC。\n对于快照数据的定义不同；\n 在 read committed 隔离级别下，对于快照数据总是读取被锁定行的最新一份快照数据。 repeatable read 隔离级别下，对于快照数据总是读取事务开始时的行数据版本。  redolog redo log用来实现事务的持久性。内存中包含 redo log buffer，磁盘中包含 redo log file。\n当事务提交时，必须先将该事务的所有日志写入到redolog进行持久化，待事务的commit操作完成才完成了事务的提交。\nredo log 顺序写，记录的是对每个页的修改（页、页偏移量、以及修改的内容）。在数据库运行时不需要对 redo log 的文件进行读取操作，只有发生宕机的时候，才会拿redo log进行恢复。\nundolog undo log用来帮助事务回滚以及MVCC的功能。\n存储在共享表空间中， undo 是逻辑日志，回滚时将数据库逻辑地恢复到原来的样子，根据 undo log 的记录，做之前的逆运算。\n比如事务中有insert 操作，那么执行 delete 操作， 对于 update 操作执行相反的 update 操作。\n同时 undo 日志记录行的版本信息，用于处理 MVCC 功能。\n主从复制  主库更新事件(update、insert、delete)通过io-thread写到binlog。 从库请求读取binlog，通过io-thread写⼊（write）从库本地 relay log（中继⽇志）。 从库通过sql-thread读取（read） relay log，并把更新事件在从库中执⾏（replay）⼀遍。  读写分离 写主（mysql）读从（内存式数据库）\n缓冲层 使用redis或memcached作为缓冲层，保存一些mysql的热点数据，用于应对高频的读操作。\n缓冲策略主要是优化的lru。\n但用户无法控制具体缓存哪些东西（与业务无关）\n一致性问题 主从之间存在数据一致性问题\n简单的读写策略可以解决大部分问题：（感觉可以参考CPU cache的策略）\n  写：写mysql，mysql写完成后同步给redis（有点问题的，因为redis里的还是旧的）\n  读：读redis，不可用的话再去mysql中找，mysql有就返回并把数据回写redis\n  强⼀致性 上面写策略还存在点漏洞，这里补齐：\n 写：先删除redis里的缓存，再去写到mysql中，这样就避免了读到旧数据。  但只适用于单数据中心，如果是多数据中心，可以设置强一致性需求忽略缓存层，或转化为单数据中心后操作。\n最终⼀致性 只确保最终的一致性，那就使用一开始的方案就行\n但还有一个替代方案：\n先写redis，设置key的过期时间为200ms（经验值），等待mysql回写redis，覆盖key，设置更⻓的过期时间。\n200ms 默认的是 写mysql到mysql同步到redis的时⻓；这个需要根据实际环境进⾏设置。\n缓存穿透 假设某个数据redis不存在，mysql也不存在，⽽且⼀直尝试读就会发生缓存穿透，数据最终压⼒依然堆积在mysql，可能造成mysql不堪重负⽽崩溃。\n方案\n 发现mysql不存在，将redis设置为 \u0026lt;key, nil\u0026gt; 设置过期时间 下次访问key的时候 不再访问mysql 容易造成redis缓存很多⽆效数据。 布隆过滤器，将mysql当中已经存在的key，写⼊布隆过滤器，不存在的直接pass掉；  缓存击穿 某些数据redis没有，但是mysql有。此时当⼤量这类数据的并发请求，同样造成mysql压力过⼤。\n方案\n 加锁：请求数据的时候获取锁，如果获取成功，则操作完后释放锁。获取失败，则休眠⼀段时间（200ms）再去获取。 增加redis中的存活时间：将很热的key，设置不过期（redis中）  缓存雪崩 表示⼀段时间内，缓存集中失效(redis⽆ mysql有)，导致请求全部⾛mysql，有可能搞垮数据库，使整个服务失效。\n方案\n 如果因为缓存数据库宕机，造成所有数据涌向mysql：那就采⽤⾼可⽤的集群⽅案，如哨兵模式、cluster模式； 如果因为设置了相同的过期时间，造成缓存集中失效：那就设置随机过期值或者其他机制错开失效。 如果因为系统重启的时候，造成缓存数据消失：重启时间短，redis开启持久化（过期信息也会持久化）就⾏了。 重启时间⻓，提前将热数据导⼊redis当中。  ","date":"2020-06-06T00:00:00Z","image":"https://gao377020481.github.io/p/mysql/537_huc4722358eae36c77295feb62b7419b0c_34031398_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mysql/","title":"Mysql"},{"content":"Redis\nRemote Dictionary Service， 远程字典服务，内存式数据库，非关系型，KV结构\n三张网图描述redis基本数据结构： \r \r \r\n数据与编码 String 字符数组，该字符串是动态字符串，字符串长度小于1M时，加倍扩容；超过1M每次只多扩1M；字符串最大长度为512M；\nTips：redis字符串是二进制安全字符串；可以存储图片，二进制协议等二进制数据；\n 字符串长度小于等于 20 且能转成整数，则使用 int 存储； 字符串长度小于等于 44，则使用 embstr 存储； 字符串长度大于 44，则使用 raw 存储；  44为界\n首先说明redis以64字节作为大小结构分界点，但其sdshdr和redisobject结构会占用一些空间，所以真正保存数据的大小小于64字节\n旧版本使用39为界，新版本使用44为界，这是因为旧版本中sdshdr占用8字节目前的sdshdr8是针对小结构的优化（大结构使用shshdr16，64），仅占用3字节，节省了5字节空间。所以新版本以44为界。\nraw和embstr\nraw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构， 而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间存储两结构\nembstr使用连续内存，更高效的利用缓存，且一次内存操作带来了更好的创建和销毁效率\n操作方式与转换\n  以一个浮点数的value作为例子，浮点数会被转换成字符串然后存储到数据库内。如果要对V进行操作，他也会先从字符串转换为浮点数然后再进行操作。\n  以一个整数2000的value作为例子，该数会被保存为int但使用append进行追加一个字符串“is a good number！”后， 该值会被转换为embstr， 然而embstr的对象从redis视角看来是只读的（未实现操作embstr的方法）， 所以该对象又会被转换raw然后实行相应操作并保存为raw\n  List 双向链表，很容易理解，但其node有讲究（压缩时使用ziplist）\n列表中数据是否压缩的依据：\n 元素长度小于 48，不压缩； 元素压缩前后长度差不超过 8，不压缩；  直接放数据结构，然后分析:\n/* Minimum ziplist size in bytes for attempting compression. */ #define MIN_COMPRESS_BYTES 48 /* quicklistNode is a 32 byte struct describing a ziplist for a quicklist. * We use bit fields keep the quicklistNode at 32 bytes. * count: 16 bits, max 65536 (max zl bytes is 65k, so max count actually \u0026lt; 32k). * encoding: 2 bits, RAW=1, LZF=2. * container: 2 bits, NONE=1, ZIPLIST=2. * recompress: 1 bit, bool, true if node is temporary decompressed for usage. * attempted_compress: 1 bit, boolean, used for verifying during testing. * extra: 10 bits, free for future use; pads out the remainder of 32 bits */ typedef struct quicklistNode { struct quicklistNode *prev; struct quicklistNode *next; unsigned char *zl; unsigned int sz; /* ziplist size in bytes */ unsigned int count : 16; /* count of items in ziplist */ unsigned int encoding : 2; /* RAW==1 or LZF==2 */ unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ unsigned int recompress : 1; /* was this node previous compressed? */ unsigned int attempted_compress : 1; /* node can\u0026#39;t compress; too small*/ unsigned int extra : 10; /* more bits to steal for future usage */ } quicklistNode; typedef struct quicklist { quicklistNode *head; quicklistNode *tail; unsigned long count; /* total count of all entries in all ziplists */ unsigned long len; /* number of quicklistNodes */ int fill : QL_FILL_BITS; /* fill factor for individual nodes */ unsigned int compress : QL_COMP_BITS; /* depth of end nodes not to compress;0=off */ unsigned int bookmark_count: QL_BM_BITS; quicklistBookmark bookmarks[]; } quicklist; OK, 可以很明显看到list本身是一个双向链表，但他会记录所有的entry的数目，node本身可能用来描述一个ziplist，ziplist本身是一块连续的内存空间，ziplist内的node不保存前后指针，因为其连续内存的特性，只需要保存当前size和前一项size即可完成向前和向后寻址，提供和双项链表一致的操作。\nhash 哈希表，很容易理解\n但底层使用ziplist和dict两种结构进行存储。\n 节点数量大于 512（hash-max-ziplist-entries） 或所有字符串长度大于 64（hash-max-ziplistvalue），则使用 dict 实现； 节点数量小于等于 512 且有一个字符串长度小于 64，则使用 ziplist 实现；  ziplist存储时将field与value（就是KV）连着放在一起，提供更高的存储效率。在未序列化的情况下，该方式相比于string更节省内存。不过如果把对象的各个KV序列化为一体然后存储为string，也许占用空间还更小。\ndict就是标准的哈希表实现，不过一个dict内部保存两个哈希表ht0和ht1，ht1用来进行rehash的中转。该表使用开链法解决hash冲突。一张引用自redisbook的图很清晰的说明了结构： \r\n当rehash工作量太大时，需要使用渐进式rehash，此时不会发生\nset 集合，不要求有序\n 元素都为整数且节点数量小于等于 512（set-max-intset-entries），则使用整数数组存储； 元素当中有一个不是整数或者节点数量大于 512，则使用字典存储；  类比hash内部，这里不再详细分析，字典编码Value为NULL即可\nzset 有序的集合\n 节点数量大于 128或者有一个字符串长度大于64，则使用跳表（skiplist）； 节点数量小于等于128（zset-max-ziplist-entries）且所有字符串长度小于等于64（zset-maxziplist-value），则使用 ziplist 存储；  ziplist编码\n|\u0026lt;-- element 1 --\u0026gt;|\u0026lt;-- element 2 --\u0026gt;|\u0026lt;-- ....... --\u0026gt;| +---------+---------+--------+---------+--------+---------+---------+---------+ | ZIPLIST | | | | | | | ZIPLIST | | ENTRY | member1 | score1 | member2 | score2 | ... | ... | ENTRY | | HEAD | | | | | | | END | +---------+---------+--------+---------+--------+---------+---------+---------+ score1 \u0026lt;= score2 \u0026lt;= ... ziplist 内连续的有序的保存entry\nskiplist编码\n/* * 有序集 */ typedef struct zset { // 字典  dict *dict; // 跳跃表  zskiplist *zsl; } zset; 使用dict来检索，使用skiplist维持顺序 引用redisbook的图，说的很清晰： \r\n协议与网络 网络层 redis6.0 单reactor模型，并发处理连接，线程串行处理命令。\nreactor管理触发的socket，调用其对应的callback（accept,recv or send），这里callback使用单线程串行处理\n\r\n事务 MULTI 开启事务，事务执行过程中，单个命令是入队列操作，直到调用 EXEC 才会一起执行；\n也可以使用DISCARD取消事务\nWATCH 检测key的变动，若在事务执行中，key变动则取消事务，在事务开启前调用，乐观锁实现（cas）， 若被取消则事务返回 nil。\nlua 脚本  lua 脚本实现原子性。 redis中加载了一个lua虚拟机，用来执行redis lua脚本。redis lua 脚本的执行是原子性的，当某个脚本正在执行的时候，不会有其他命令或者脚本被执行。 lua脚本当中的命令会直接修改数据状态。  Tips：如果项目中使用了lua脚本，不需要使用上面的事务命令。\n发布订阅 为了支持消息的多播机制，redis引入了发布订阅模块。disque 消息队列\n订阅\nstruct redisServer { // ...  dict *pubsub_channels; // ... }; redisServer 中一项pubsub_channels的字典保存了订阅频道的信息。 字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。\n发布\n首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端。\n订阅模式\n一个发布的信息，与其发布频道相匹配的所有模式频道也会将这个信息发布给自己旗下的订阅用户。\nstruct redisServer { // ...  list *pubsub_patterns; // ... }; 这次是pubsub_patterns链表了，每个节点都有一个pubsubPattern结构\ntypedef struct pubsubPattern { redisClient *client; robj *pattern; } pubsubPattern; client 属性保存着订阅模式的客户端，而 pattern 属性则保存着被订阅的模式。\n通过遍历整个 pubsub_patterns 链表，程序可以检查所有正在被订阅的模式，以及订阅这些模式的客户端。\n发布到模式\nPUBLISH 除了将 message 发送到所有订阅 channel 的客户端之外， 它还会将 channel 和 pubsub_patterns 中的模式进行对比， 如果 channel 和某个模式匹配的话， 那么也将 message 发送到订阅那个模式的客户端。\nredis6.0 io多线程 redis6.0版本后添加的 io多线程主要解决redis协议的压缩以及解压缩的耗时问题；一般项目中不 需要开启；如果有大量并发请求，且返回数据包一般比较大的场景才有它的用武之地；\n单线程\n正常的redis单线程处理事件，但是其read write操作都需要经过内核栈，需要从内核的内存空间中取出或发送包，内核到用户内存空间的复制又涉及页表项的修改等。所以总的来说IO占用的CPU时间较多。如果这时引入多线程IO，有效利用多核可以进一步提升性能。\n多线程实现\n外部来看还是单线程的状态，这是因为多线程参考主线程的phase。只有主线程处于read phase，其他线程才做read操作，只有主线程处于write phase其他线程才做write。并且任务的分配也由主线程完成，每个thread维护一个任务队列（链表组织），主线程唤醒thread去执行任务，可以使用cond去唤醒。\nredis扩展 Redis 通过对外提供一套 API 和一些数据类型，可以供开发者开发自己的模块并且加载到 redis 中。类似插件。在不侵入 redis 源码基础上，提供一种高效的扩展数据结构的方式，也可以用来实现原子操作。\n入口函数 int RedisModule_OnLoad(RedisModuleCtx *ctx, RedisModuleString **argv, int argc); // RedisModule_Init 应该要是第一个被调用的函数 static int RedisModule_Init(RedisModuleCtx *ctx, const char *name, int ver, int apiver); // RedisModule_CreateCommand 应该要是第二个被调用的函数 REDISMODULE_API int (*RedisModule_CreateCommand)(RedisModuleCtx *ctx, const char *name, RedisModuleCmdFunc cmdfunc, const char *strflags, int firstkey, int lastkey, int keystep); // 回调函数 typedef int (*RedisModuleCmdFunc)(RedisModuleCtx *ctx, RedisModuleString **argv, int argc); RedisBloom 布隆过滤器，准确判断不存在的值\n\rgit clone https://github.com/RedisBloom/RedisBloom.git\rcd RedisBloom\rmake\rcp redisbloom.so /path/to\rvi redis.conf\r# loadmodules /path/to/redisbloom.so\r具体使用直接看github\nhyperloglog 少量内存下统计一个集合中唯一元素数量的近似值。\n在Redis实现中，每个键只使用 12kb 进行计数，使用 16384 个桶子，每个桶子6bit，标准误差为0.8125% ，并且对可以计数的项目数没有限制，除非接近 个项目数（这似乎不太可能）\n持久化 redis 的数据全部在内存中，如果突然宕机，数据就会全部丢失，因此需要持久化来保证 Redis 的数据不会因为故障而丢失，redis 重启的时候可以重新加载持久化文件来恢复数据；\n三种方式：\n  aof (append only file)\n  rdb (Redis Database)\n  rdb + aof\n  AOF 可以认为是一系列的操作log，以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF 文件，以此达到记录数据库状态的目的。\n同步命令到 AOF 文件的整个过程可以分为三个阶段：\n 命令传播：Redis 将执行完的命令、命令的参数、命令的参数个数等信息发送到 AOF 程序中。 缓存追加：AOF 程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的 AOF 缓存中。 文件写入和保存：AOF 缓存中的内容被写入到 AOF 文件末尾，如果设定的 AOF 保存条件被满足的话， fsync 函数或者 fdatasync 函数会被调用，将写入的内容真正地保存到磁盘中。  保存模式\nRedis 目前支持三种 AOF 保存模式，它们分别是：\n AOF_FSYNC_NO ：不保存。 AOF_FSYNC_EVERYSEC ：每一秒钟保存一次。 AOF_FSYNC_ALWAYS ：每执行一个命令保存一次。  AOF 重写\nAOF文件会积累变大，所以积累到一定量时，会创建一个新的 AOF 文件来代替原有的 AOF 文件， 新 AOF 文件和原有 AOF 文件保存的数据库状态完全一样， 但新 AOF 文件的体积小于等于原有 AOF 文件的体积。相当于对冗杂的指令进行简化，使最终结果一样就行。\n重写在后台子进程中发生，子进程带有主进程的数据副本，使用子进程而不是线程，可以在避免锁的情况下，保证数据的安全性。\n不过， 使用子进程也有一个问题需要解决： 因为子进程在进行 AOF 重写期间， 主进程还需要继续处理命令， 而新的命令可能对现有的数据进行修改， 这会让当前数据库的数据和重写后的 AOF 文件中的数据不一致。\n为了解决这个问题， Redis 增加了一个 AOF 重写缓存， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个缓存中。\n重写完毕后会将缓存并入新的AOF文件中去。\nRDB 可以认为是当前存储的内存映像or快照，将数据库的快照（snapshot）以二进制的方式保存到磁盘中。\n核心方法：保存与载入\n保存\nrdbSave 函数负责将内存中的数据库数据以 RDB 格式保存到磁盘中， 如果 RDB 文件已存在， 那么新的 RDB 文件将替换已有的 RDB 文件。\n在保存 RDB 文件期间， 主进程会被阻塞， 直到保存完成为止。 有两种方法：SAVE 和 BGSAVE SAVE： 会阻塞主进程开始保存RDB，直到保存完毕返回 BGSAVE： 会fork出子进程进行保存RDB，但直到保存完毕中间的一段数据有可能会丢失 SAVE与BGSAVE不能同时执行，且BGSAVE也不能与REWRITEAOF同时执行，这是因为创建俩进程来做这个事情效率太低 SAVE的时候AOF一样会写入，因为AOF写入又后台线程完成\n载入\n在载入期间， 服务器每载入 1000 个键就处理一次所有已到达的请求， 不过只有 PUBLISH 、 SUBSCRIBE 、 PSUBSCRIBE 、 UNSUBSCRIBE 、 PUNSUBSCRIBE 五个命令的请求会被正确地处理， 其他命令一律返回错误。 等到载入完成之后， 服务器才会开始正常处理所有命令。\nTips:发布与订阅功能和其他数据库功能是完全隔离的，前者不写入也不读取数据库，所以在服务器载入期间，订阅与发布功能仍然可以正常使用，而不必担心对载入数据的完整性产生影响。\nRDB + AOF 因为RDB的保存过程较长可能会错过一些数据，所以在RDB进行保存的时候使用AOF缓存来记录保存时发生的写入操作，然后将AOF缓存持久化为AOF，通过AOF+RDB就可以完整的表述数据\n主从复制 主要用来实现 redis 数据的可靠性。防止主 redis 所在磁盘损坏，造成数据永久丢失。\n主从之间采用异步复制的方式。\n集群 Redis cluster集群\n","date":"2020-05-22T00:00:00Z","image":"https://gao377020481.github.io/p/redis/353_hue47bacef190d5647cba0a16e01e338ad_4596808_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/redis/","title":"Redis"},{"content":"Try Catch组件实现 先说明try catch组件使用setjump和longjump实现\nsetjump longjump语法 Int setjmp(jmp_buf env);\n返回值：若直接调用则返回0，若从longjmp调用返回则返回非0值的longjmp中的val值\nVoid longjmp(jmp_buf env,int val);\n调用此函数则返回到语句setjmp所在的地方，其中env 就是setjmp中的 env，而val 则是使setjmp的返回值变为val。\n当检查到一个错误时,则以两个参数调用longjmp函数，第一个就是在调用setjmp时所用的env，第二个参数是具有非0值val，它将成为从setjmp处返回的值。使用第二个参数的原因是对于一个setjmp可以有多个longjmp。\njmp_buf env;环境\nsetjump(env)设置回跳点，返回longjump(env,out)传的参数out，配套使用，longjump可穿越函数跳转\njmp_buf env; int c = setjump(env); longjump(env,3); 这里longjump后就会跳回setjump这一行，并且setjump会返回3，也就是c = 3。\nint count = 0; jmp_buf env; void a(int indx) { longjump(env,indx); } int main() { int idx = 0; count = setjump(env); if(count == 0) { a(env,idx++); } else if (count == 1) { a(env,idx++); } else { printf(\u0026#34;ok\u0026#34;); } return 0; } 如上，函数a会调回开头setjump处，如果是这样a调用多次，a又没有返回（a运行到longjump处进入了，没返回），a的栈会不会还存在，存在的话如果有无数个a，会不会发生栈溢出。\n答案是不会，因为a在进入longjump后，其栈指针直接失效，a的栈直接失效，在setjump函数所在函数block中被覆盖，所以a的多次调用不会发生栈溢出。\nsetjump 与 longjump本身是线程安全的\nsetjump longjump与try catch的关系 先来个 代码：\ntry{ //setjump to catch  throw(); // longjump(para)  } catch (para) { } finally () { } 来看对应的：\nint count = 0; jmp_buf env; void a(int indx) { longjump(env,indx); } int main() { int idx = 0; count = setjump(env); if(count == 0) // try()  { a(env,idx++); //throw(1)  } else if (count == 1)//catch(1)  { a(env,idx++);//throw(2)  } //finally  { printf(\u0026#34;ok\u0026#34;); } return 0; } 再上代码，直接用宏定义try catch throw finally：\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;setjmp.h\u0026gt; typedef struct _tagExcepSign { jmp_buf _stackinfo; int _exceptype; } tagExcepSign; #define ExcepType(ExcepSign) ((ExcepSign)._exceptype)  #define Try(ExcepSign) if (((ExcepSign)._exceptype = setjmp((ExcepSign)._stackinfo)) == 0)  #define Catch(ExcepSign, ExcepType) else if ((ExcepSign)._exceptype == ExcepType)  #define Finally\telse  #define Throw(ExcepSign, ExcepType)\tlongjmp((ExcepSign)._stackinfo, ExcepType)  void ExceptionTest(int expType) { tagExcepSign ex; expType = expType \u0026lt; 0 ? -expType : expType; Try (ex) { if (expType \u0026gt; 0) { Throw(ex, expType); } else { printf(\u0026#34;no exception\\n\u0026#34;); } } Catch (ex, 1) { printf(\u0026#34;no exception 1\\n\u0026#34;); } Catch (ex, 2) { printf(\u0026#34;no exception 2\\n\u0026#34;); } Finally { printf(\u0026#34;other exp\\n\u0026#34;); } } int main() { ExceptionTest(0); ExceptionTest(1); ExceptionTest(2); ExceptionTest(3); } 现在有个新问题，try catch嵌套怎么办。\n以下三个问题需要实现：\n 多个ex依次入栈，每次只处理栈顶ex，解决嵌套问题 需要在throw的时候抛出异常发生的位置（文件、行号、函数名），可使用编译器自带的宏： __FILE__ __LINE__ __func__  虽然函数setjump和longjump是线程安全的，但ex变量在多线程时被共用，那ex就成为临界变量，如何保证ex的线程安全  以下代码实现以上功能：\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdarg.h\u0026gt; #include \u0026lt;pthread.h\u0026gt;#include \u0026lt;setjmp.h\u0026gt; #define ntyThreadData\tpthread_key_t #define ntyThreadDataSet(key, value)\tpthread_setspecific((key), (value)) #define ntyThreadDataGet(key)\tpthread_getspecific((key)) #define ntyThreadDataCreate(key)\tpthread_key_create(\u0026amp;(key), NULL)  #define EXCEPTIN_MESSAGE_LENGTH\t512  typedef struct _ntyException { const char *name; } ntyException; ntyException SQLException = {\u0026#34;SQLException\u0026#34;}; ntyException TimeoutException = {\u0026#34;TimeoutException\u0026#34;}; ntyThreadData ExceptionStack; typedef struct _ntyExceptionFrame { jmp_buf env; int line; const char *func; const char *file; ntyException *exception; struct _ntyExceptionFrame *prev; char message[EXCEPTIN_MESSAGE_LENGTH+1]; } ntyExceptionFrame; #define ntyExceptionPopStack\t\\ ntyThreadDataSet(ExceptionStack, ((ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack))-\u0026gt;prev)  #define ReThrow\tntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, NULL) #define Throw(e, cause, ...) ntyExceptionThrow(\u0026amp;(e), __func__, __FILE__, __LINE__, cause, ##__VA_ARGS__, NULL)  enum { ExceptionEntered = 0, ExceptionThrown, ExceptionHandled, ExceptionFinalized }; #define Try do {\t\\ volatile int Exception_flag;\t\\ ntyExceptionFrame frame;\t\\ frame.message[0] = 0;\t\\ frame.prev = (ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack);\t\\ ntyThreadDataSet(ExceptionStack, \u0026amp;frame);\t\\ Exception_flag = setjmp(frame.env);\t\\ if (Exception_flag == ExceptionEntered) {\t\t#define Catch(e) \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } else if (frame.exception == \u0026amp;(e)) { \\ Exception_flag = ExceptionHandled;  #define Finally \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } { \\ if (Exception_flag == ExceptionEntered)\t\\ Exception_flag = ExceptionFinalized;  #define EndTry \\ if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \\ } if (Exception_flag == ExceptionThrown) ReThrow; \\ } while (0)\t static pthread_once_t once_control = PTHREAD_ONCE_INIT; static void init_once(void) { ntyThreadDataCreate(ExceptionStack); } void ntyExceptionInit(void) { pthread_once(\u0026amp;once_control, init_once); } void ntyExceptionThrow(ntyException *excep, const char *func, const char *file, int line, const char *cause, ...) { va_list ap; ntyExceptionFrame *frame = (ntyExceptionFrame*)ntyThreadDataGet(ExceptionStack); if (frame) { frame-\u0026gt;exception = excep; frame-\u0026gt;func = func; frame-\u0026gt;file = file; frame-\u0026gt;line = line; if (cause) { va_start(ap, cause); vsnprintf(frame-\u0026gt;message, EXCEPTIN_MESSAGE_LENGTH, cause, ap); va_end(ap); } ntyExceptionPopStack; longjmp(frame-\u0026gt;env, ExceptionThrown); } else if (cause) { char message[EXCEPTIN_MESSAGE_LENGTH+1]; va_start(ap, cause); vsnprintf(message, EXCEPTIN_MESSAGE_LENGTH, cause, ap); va_end(ap); printf(\u0026#34;%s: %s\\nraised in %s at %s:%d\\n\u0026#34;, excep-\u0026gt;name, message, func ? func : \u0026#34;?\u0026#34;, file ? file : \u0026#34;?\u0026#34;, line); } else { printf(\u0026#34;%s: %p\\nraised in %s at %s:%d\\n\u0026#34;, excep-\u0026gt;name, excep, func ? func : \u0026#34;?\u0026#34;, file ? file : \u0026#34;?\u0026#34;, line); } } /* ** **** ******** **************** debug **************** ******** **** ** */ ntyException A = {\u0026#34;AException\u0026#34;}; ntyException B = {\u0026#34;BException\u0026#34;}; ntyException C = {\u0026#34;CException\u0026#34;}; ntyException D = {\u0026#34;DException\u0026#34;}; void *thread(void *args) { pthread_t selfid = pthread_self(); Try { Throw(A, \u0026#34;A\u0026#34;); } Catch (A) { printf(\u0026#34;catch A : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(B, \u0026#34;B\u0026#34;); } Catch (B) { printf(\u0026#34;catch B : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(C, \u0026#34;C\u0026#34;); } Catch (C) { printf(\u0026#34;catch C : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(D, \u0026#34;D\u0026#34;); } Catch (D) { printf(\u0026#34;catch D : %ld\\n\u0026#34;, selfid); } EndTry; Try { Throw(A, \u0026#34;A Again\u0026#34;); Throw(B, \u0026#34;B Again\u0026#34;); Throw(C, \u0026#34;C Again\u0026#34;); Throw(D, \u0026#34;D Again\u0026#34;); } Catch (A) { printf(\u0026#34;catch A again : %ld\\n\u0026#34;, selfid); } Catch (B) { printf(\u0026#34;catch B again : %ld\\n\u0026#34;, selfid); } Catch (C) { printf(\u0026#34;catch C again : %ld\\n\u0026#34;, selfid); } Catch (D) { printf(\u0026#34;catch B again : %ld\\n\u0026#34;, selfid); } EndTry; } #define THREADS\t50  int main(void) { ntyExceptionInit(); Throw(D, NULL); Throw(C, \u0026#34;null C\u0026#34;); printf(\u0026#34;\\n\\n=\u0026gt; Test1: Try-Catch\\n\u0026#34;); Try { Try { Throw(B, \u0026#34;recall B\u0026#34;); } Catch (B) { printf(\u0026#34;recall B \\n\u0026#34;); } EndTry; Throw(A, NULL); } Catch(A) { printf(\u0026#34;\\tResult: Ok\\n\u0026#34;); } EndTry; printf(\u0026#34;=\u0026gt; Test1: Ok\\n\\n\u0026#34;); printf(\u0026#34;=\u0026gt; Test2: Test Thread-safeness\\n\u0026#34;); #if 1 \tint i = 0; pthread_t threads[THREADS]; for (i = 0;i \u0026lt; THREADS;i ++) { pthread_create(\u0026amp;threads[i], NULL, thread, NULL); } for (i = 0;i \u0026lt; THREADS;i ++) { pthread_join(threads[i], NULL); } #endif \tprintf(\u0026#34;=\u0026gt; Test2: Ok\\n\\n\u0026#34;); } ","date":"2020-05-12T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/529_huc139e517d88aa84bf66b7ba5d5d134e7_26286259_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","title":"异常处理"},{"content":"#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define LL_ADD(item, list) do { \\ item-\u0026gt;prev = NULL;\t\\ item-\u0026gt;next = list;\t\\ list = item;\t\\ } while(0)  #define LL_REMOVE(item, list) do {\t\\ if (item-\u0026gt;prev != NULL) item-\u0026gt;prev-\u0026gt;next = item-\u0026gt;next;\t\\ if (item-\u0026gt;next != NULL) item-\u0026gt;next-\u0026gt;prev = item-\u0026gt;prev;\t\\ if (list == item) list = item-\u0026gt;next;\t\\ item-\u0026gt;prev = item-\u0026gt;next = NULL;\t\\ } while(0)  typedef struct NWORKER {//工作线程信息 \tpthread_t thread; //线程id \tint terminate; //是否要终止 \tstruct NWORKQUEUE *workqueue; //线程池，用于找到工作队列 \tstruct NWORKER *prev; struct NWORKER *next; } nWorker; typedef struct NJOB { //工作个体 \tvoid (*job_function)(struct NJOB *job); void *user_data; struct NJOB *prev; struct NJOB *next; } nJob; typedef struct NWORKQUEUE { struct NWORKER *workers; //所有工作线程的链表 \tstruct NJOB *waiting_jobs; //工作队列 \tpthread_mutex_t jobs_mtx; pthread_cond_t jobs_cond; } nWorkQueue; typedef nWorkQueue nThreadPool; static void *ntyWorkerThread(void *ptr) { //工作线程取用工作 \tnWorker *worker = (nWorker*)ptr; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //先获取工作队列的操作互斥锁  while (worker-\u0026gt;workqueue-\u0026gt;waiting_jobs == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_cond, \u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果工作队列为空，这个线程就阻塞在条件变量上等待事件发生 \t} if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //如果检测到工作线程被终止，那么这个线程就需要结束工作，但在结束工作前需要将对工作队列的取用权限放开，所以这里在break前需要解锁这个互斥锁 \tbreak; } nJob *job = worker-\u0026gt;workqueue-\u0026gt;waiting_jobs; //从工作队列中获取一个工作 \tif (job != NULL) { LL_REMOVE(job, worker-\u0026gt;workqueue-\u0026gt;waiting_jobs); //从工作队列中移除掉获取的这个工作 \tif (job != NULL) { } pthread_mutex_unlock(\u0026amp;worker-\u0026gt;workqueue-\u0026gt;jobs_mtx); //已经取到工作了，就可以放开对工作队列的占有了  if (job == NULL) continue; job-\u0026gt;job_function(job); //针对工作调用他的回调函数处理，处理结束后继续循环去工作队列中取 \t} free(worker); //工作线程被终止那当然需要释放其堆上内存 \tpthread_exit(NULL); } int ntyThreadPoolCreate(nThreadPool *workqueue, int numWorkers) { //创建线程池  if (numWorkers \u0026lt; 1) numWorkers = 1; memset(workqueue, 0, sizeof(nThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; //条件变量用于通知所有的工作线程事件发生 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_cond, \u0026amp;blank_cond, sizeof(workqueue-\u0026gt;jobs_cond)); pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; // 互斥锁用于锁住工作队列确保同时只有一个工作线程在从工作队列中取工作 \tmemcpy(\u0026amp;workqueue-\u0026gt;jobs_mtx, \u0026amp;blank_mutex, sizeof(workqueue-\u0026gt;jobs_mtx)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { nWorker *worker = (nWorker*)malloc(sizeof(nWorker)); //这里在堆上创建线程，那就需要在线程终止时释放 \tif (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return 1; } memset(worker, 0, sizeof(nWorker)); worker-\u0026gt;workqueue = workqueue; //初始化工作线程信息，线程池用于找到工作队列  int ret = pthread_create(\u0026amp;worker-\u0026gt;thread, NULL, ntyWorkerThread, (void *)worker); //创建线程，传入该线程信息，达到信息和线程的绑定关系 \tif (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return 1; } LL_ADD(worker, worker-\u0026gt;workqueue-\u0026gt;workers); //头插法在所有工作线程的链表中插入新建的工作线程 \t} return 0; } void ntyThreadPoolShutdown(nThreadPool *workqueue) { //关闭线程池 \tnWorker *worker = NULL; for (worker = workqueue-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate = 1; //所有工作线程的terminate关键字置为1 \t} pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); workqueue-\u0026gt;workers = NULL; //清空工作线程链表 \tworkqueue-\u0026gt;waiting_jobs = NULL; // 清空工作队列  pthread_cond_broadcast(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉所有工作线程有事件发生(shutdown，下一步检查terminate关键字)  pthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } void ntyThreadPoolQueue(nThreadPool *workqueue, nJob *job) { //向工作队列中添加工作  pthread_mutex_lock(\u0026amp;workqueue-\u0026gt;jobs_mtx); LL_ADD(job, workqueue-\u0026gt;waiting_jobs); pthread_cond_signal(\u0026amp;workqueue-\u0026gt;jobs_cond); //告诉任意一个工作线程有事件发生(目前有新的工作出现在工作队列里了，下一步get互斥锁并取工作) \tpthread_mutex_unlock(\u0026amp;workqueue-\u0026gt;jobs_mtx); } /************************** debug thread pool **************************/ //sdk --\u0026gt; software develop kit // 提供SDK给其他开发者使用  #if 1  #define KING_MAX_THREAD\t80 #define KING_COUNTER_SIZE\t1000  void king_counter(nJob *job) { int index = *(int*)job-\u0026gt;user_data; printf(\u0026#34;index : %d, selfid : %lu\\n\u0026#34;, index, pthread_self()); free(job-\u0026gt;user_data); free(job); } int main(int argc, char *argv[]) { nThreadPool pool; ntyThreadPoolCreate(\u0026amp;pool, KING_MAX_THREAD); int i = 0; for (i = 0;i \u0026lt; KING_COUNTER_SIZE;i ++) { nJob *job = (nJob*)malloc(sizeof(nJob)); if (job == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } job-\u0026gt;job_function = king_counter; job-\u0026gt;user_data = malloc(sizeof(int)); *(int*)job-\u0026gt;user_data = i; ntyThreadPoolQueue(\u0026amp;pool, job); } getchar(); printf(\u0026#34;\\n\u0026#34;); } #endif  ","date":"2020-05-07T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/241_hudd714e7c3305b4c239dc02d266d283b9_7211866_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%BF%9B%E9%98%B6%E7%89%88/","title":"线程池进阶版"},{"content":"线程池 基本功能模块：  线程池创建函数 线程池删除函数 线程池回调函数 线程池添加函数 线程池数据结构 线程任务数据结构 线程本身数据结构（由pid唯一确认）  首先实现数据结构： 线程任务数据结构：\nstruct nTask { void (*task_func)(struct nTask *task); void *user_data; struct nTask *prev; struct nTask *next; }; 这是任务中的一个个体，任务队列头存储在线程池数据结构中 void (*task_func)(struct nTask *task)函数指针表明函数为task_func且参数为struct nTask， 参数若为void是否更好\n线程本身数据结构：\nstruct nWorker { pthread_t threadid; int terminate; struct nManager *manager; struct nWorker *prev; struct nWorker *next; }; pid唯一标识线程，terminate用于标识该线程应被删除，存储manager（也就是所属线程池）是为了通过manager找到task队列以获取task\n线程池数据结构：\ntypedef struct nManager { struct nTask *tasks; struct nWorker *workers; pthread_mutex_t mutex; pthread_cond_t cond; } ThreadPool; 可以看到线程池其实只是一个管理者，使用mutex控制各个线程对进程内公共资源的访问，保证同时只有一个线程在访问公共资源，cond来控制各个线程的状态（处于等待队列（阻塞）或可以运行（运行、就绪态））细节在回调函数中\n然后实现API： 线程池创建函数：\nint nThreadPoolCreate(ThreadPool *pool, int numWorkers) { if (pool == NULL) return -1; if (numWorkers \u0026lt; 1) numWorkers = 1; memset(pool, 0, sizeof(ThreadPool)); pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;cond, \u0026amp;blank_cond, sizeof(pthread_cond_t)); //pthread_mutex_init(\u0026amp;pool-\u0026gt;mutex, NULL); \tpthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER; memcpy(\u0026amp;pool-\u0026gt;mutex, \u0026amp;blank_mutex, sizeof(pthread_mutex_t)); int i = 0; for (i = 0;i \u0026lt; numWorkers;i ++) { struct nWorker *worker = (struct nWorker*)malloc(sizeof(struct nWorker)); if (worker == NULL) { perror(\u0026#34;malloc\u0026#34;); return -2; } memset(worker, 0, sizeof(struct nWorker)); worker-\u0026gt;manager = pool; //  int ret = pthread_create(\u0026amp;worker-\u0026gt;threadid, NULL, nThreadPoolCallback, worker); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); free(worker); return -3; } LIST_INSERT(worker, pool-\u0026gt;workers); } // success \treturn 0; } 根据传入线程数量参数，创建含有指定数量线程的线程池，初始化条件变量和互斥量，初始化线程本身然后放入队列\n线程池回调函数：\nstatic void *nThreadPoolCallback(void *arg) { struct nWorker *worker = (struct nWorker*)arg; while (1) { pthread_mutex_lock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); while (worker-\u0026gt;manager-\u0026gt;tasks == NULL) { if (worker-\u0026gt;terminate) break; pthread_cond_wait(\u0026amp;worker-\u0026gt;manager-\u0026gt;cond, \u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); } if (worker-\u0026gt;terminate) { pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); break; } struct nTask *task = worker-\u0026gt;manager-\u0026gt;tasks; LIST_REMOVE(task, worker-\u0026gt;manager-\u0026gt;tasks); pthread_mutex_unlock(\u0026amp;worker-\u0026gt;manager-\u0026gt;mutex); task-\u0026gt;task_func(task); //go task \t} free(worker); } 线程在创建并将其pid放入队列中后就运行回调函数，通过回调函数可以看到线程会阻塞在mutex上也可能阻塞在cond上 pthread_cond_wait函数使用两个参数： cond和mutex 这个函数等待在cond上并在收到signal或broadcast后返回，使主函数继续运行 在函数等待的时候，首先将该线程放到等待队列上然后释放mutex，这样可以保证其他线程对公共资源的访问 在收到cond的single或broadcast后线程会争夺mutex锁住临界区资源，然后自己消费，消费完后释放互斥锁 使用while循环可以保证在有资源到来的时候也就是signal cond的时候，速度慢的线程（没有抢到互斥锁的线程）可以发现资源已经被消耗完并重新通过pthread_cond_wait进入等待区\n可以看到只有在对临界区资源的访问中才加锁：访问任务队列并从中获取任务\n线程池添加函数：\nint nThreadPoolPushTask(ThreadPool *pool, struct nTask *task) { pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); LIST_INSERT(task, pool-\u0026gt;tasks); pthread_cond_signal(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); } 添加后通知整个线程队列，让他们消费\n线程池删除函数：\nint nThreadPoolDestory(ThreadPool *pool, int nWorker) { struct nWorker *worker = NULL; for (worker = pool-\u0026gt;workers;worker != NULL;worker = worker-\u0026gt;next) { worker-\u0026gt;terminate; } pthread_mutex_lock(\u0026amp;pool-\u0026gt;mutex); pthread_cond_broadcast(\u0026amp;pool-\u0026gt;cond); pthread_mutex_unlock(\u0026amp;pool-\u0026gt;mutex); pool-\u0026gt;workers = NULL; pool-\u0026gt;tasks = NULL; return 0; } 设置删除位terminate之后唤醒所有等待队列中的线程叫他们检查自己的删除位terminate 如果要删除该线程就退出while循环然后释放worker再退出\n附一个使用代码：\n#if 1  #define THREADPOOL_INIT_COUNT\t20 #define TASK_INIT_SIZE\t1000  void task_entry(struct nTask *task) { //type  //struct nTask *task = (struct nTask*)task; \tint idx = *(int *)task-\u0026gt;user_data; printf(\u0026#34;idx: %d\\n\u0026#34;, idx); free(task-\u0026gt;user_data); free(task); } int main(void) { ThreadPool pool = {0}; nThreadPoolCreate(\u0026amp;pool, THREADPOOL_INIT_COUNT); // pool --\u0026gt; memset(); \tint i = 0; for (i = 0;i \u0026lt; TASK_INIT_SIZE;i ++) { struct nTask *task = (struct nTask *)malloc(sizeof(struct nTask)); if (task == NULL) { perror(\u0026#34;malloc\u0026#34;); exit(1); } memset(task, 0, sizeof(struct nTask)); task-\u0026gt;task_func = task_entry; task-\u0026gt;user_data = malloc(sizeof(int)); *(int*)task-\u0026gt;user_data = i; nThreadPoolPushTask(\u0026amp;pool, task); } getchar(); } #endif ","date":"2020-04-26T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/240_huc6553c3d943592fc8492e4a8a4385797_5666304_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","title":"线程池"},{"content":"Reactor  组成：⾮阻塞的io + io多路复⽤； 特征：基于事件循环，以事件驱动或者事件回调的⽅式来实现业务逻辑； 表述：将连接的io处理转化为事件处理；  单Reactor \r\n代表：redis 内存数据库 操作redis当中的数据结构 redis 6.0 多线程\n单reactor模型 + 任务队列 + 线程池 \r\n代表：skynet\n多Reactor \r\n应⽤： memcached accept(fd, backlog) one eventloop per thread\n 多进程应用  \r\n应⽤：nginx\n多reactor + 消息队列 + 线程池 ","date":"2020-04-24T00:00:00Z","image":"https://gao377020481.github.io/p/reactor/516_huf7159154d5ef9e1554acf4947cd6a0ec_16113861_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/reactor/","title":"Reactor"},{"content":"定时器 定时器作用很多，常见于心跳检测，冷却等等 实现时区别主要在于定时器队列的管控（排序）\n基本方案： 红黑树： 使用红黑树对定时器排序，排序依据为定时器过期时间，每隔单位时间检查红黑树中最小时间是否小于等于当前时间，如果小于等于就删除节点并触发节点的callback。时间复杂度增删O(logn)，Nginx使用红黑树。删除添加操作自旋。\n最小堆： 最小堆根节点最小，直接拿出根节点与当前时间比较即可，删除操作将根节点与末尾节点对换并删除末尾节点然后将新的根节点下沉，添加时加入末尾节点并上升。\n时间轮：  时间轮可以分为单层级与多层级。简单的单层级时间轮使用初始化好的链表数组来存储对应的事件节点链表，时间数据结构中一般包含引用计数，该数据结构只有在引用计数置零后销毁，一般也代表着事件对应的资源可以释放。单层时间轮的大小至少需要大于最长定时时间/单位时间，举例：每5秒发送一个心跳包，连接收到心跳包时需要开启一个10秒的定时器并将事件引用计数加一（事件数据结构插入链表数组中10秒后的链表中），也就是最长定时10秒，10秒后检查该连接对应的事件并将引用计数减一，如果减一后为0就说明连接超时，释放所有资源，关闭事件。在该例子中，初始化的链表数组大小至少为11，因为假如在第0秒来一个心跳包，我们就需要在第10号位置将该连接对应的事件节点加入事件链表中，如果小于11，比如为8，那从0开始往后10个的位置就是在2号位置，那2秒后就得触发了，这与我们设置的10秒定时时间不一致。\n\r\n代码实现： 红黑树： 红黑树数据结构直接使用nginx自带的rbtree头文件，就不自己写了\n红黑树定时器头文件： #ifndef _MARK_RBT_ #define _MARK_RBT_  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdint.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stddef.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026#34;rbtree.h\u0026#34; ngx_rbtree_t timer; static ngx_rbtree_node_t sentinel; typedef struct timer_entry_s timer_entry_t; typedef void (*timer_handler_pt)(timer_entry_t *ev); struct timer_entry_s { ngx_rbtree_node_t timer; timer_handler_pt handler; }; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int init_timer() { ngx_rbtree_init(\u0026amp;timer, \u0026amp;sentinel, ngx_rbtree_insert_timer_value); return 0; } void add_timer(timer_entry_t *te, uint32_t msec) { msec += current_time(); printf(\u0026#34;add_timer expire at msec = %u\\n\u0026#34;, msec); te-\u0026gt;timer.key = msec; ngx_rbtree_insert(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void del_timer(timer_entry_t *te) { ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); } void expire_timer() { timer_entry_t *te; ngx_rbtree_node_t *sentinel, *root, *node; sentinel = timer.sentinel; uint32_t now = current_time(); for (;;) { root = timer.root; if (root == sentinel) break; node = ngx_rbtree_min(root, sentinel); if (node-\u0026gt;key \u0026gt; now) break; printf(\u0026#34;touch timer expire time=%u, now = %u\\n\u0026#34;, node-\u0026gt;key, now); te = (timer_entry_t *) ((char *) node - offsetof(timer_entry_t, timer)); te-\u0026gt;handler(te); ngx_rbtree_delete(\u0026amp;timer, \u0026amp;te-\u0026gt;timer); free(te); } } #endif  expire_timer() 检查红黑树中是否有过期定时器，清理所有过期定时器并触发对应的回调函数\nadd_timer 向红黑树中插入事件，使用nginx定时器专用的红黑树插入函数，这里红黑树使用黑色哨兵sentinel，其含义如下：\n红黑树有一个性质是：\r叶结点的左右两边必须为黑色。也就是本来叶结点如果没有左右孩子直接初始化为NULL就是了，但它居然要黑色，意味着我需要新分配两块内存空间啥数据也不保存，tm就是为了给它涂上黑色然后挂到叶结点的left、right。\r当叶结点多起来的时候你说多浪费内存空间？理想的二叉树结构是为了让我们保存数据（key），而不是为了保存颜色吧？\r所以哨兵这个外援就来了，我们申请一块内存命名为哨兵，然后把这块内存涂上黑色，之后所有没有孩子的叶结点left、right都指向这个已涂上黑色的哨兵。以上是红黑树哨兵的作用。\r其他函数较简单\n红黑树定时器主文件： #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;rbt-timer.h\u0026#34; void print_hello(timer_entry_t *te) { printf(\u0026#34;hello world time = %u\\n\u0026#34;, te-\u0026gt;timer.key); } int main() { init_timer(); timer_entry_t *te = malloc(sizeof(timer_entry_t)); memset(te, 0, sizeof(timer_entry_t)); te-\u0026gt;handler = print_hello; add_timer(te, 3000); for (;;) { expire_timer(); usleep(10000); } return 0; } 主函数中主事件循环使用单位10秒usleep(10000),usleep中参数单位为ms\n最小堆： 最小堆定时器头文件： #pragma once  #include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;using namespace std; typedef void (*TimerHandler) (struct TimerNode *node); struct TimerNode { int idx = 0; int id = 0; unsigned int expire = 0; TimerHandler cb = NULL; }; class MinHeapTimer { public: MinHeapTimer() { _heap.clear(); _map.clear(); } static inline int Count() { return ++_count; } int AddTimer(uint32_t expire, TimerHandler cb); bool DelTimer(int id); void ExpireTimer(); private: inline bool _lessThan(int lhs, int rhs) { return _heap[lhs]-\u0026gt;expire \u0026lt; _heap[rhs]-\u0026gt;expire; } bool _shiftDown(int pos); void _shiftUp(int pos); void _delNode(TimerNode *node); private: vector\u0026lt;TimerNode*\u0026gt; _heap; map\u0026lt;int, TimerNode*\u0026gt; _map; static int _count; }; int MinHeapTimer::_count = 0; 需要分析的是struct TimerNode这个数据结构，idx指示在最小堆数组中的位置，用于在定时器过期时确认位置，id用于与_map协作以快速定位并删除特定id的节点，id的值自增长，expire为定时长度，cd是回调函数,具体可以看代码。\n最小堆定时器主文件： #include \u0026lt;unistd.h\u0026gt;#if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #include \u0026lt;iostream\u0026gt; #include \u0026#34;minheap.h\u0026#34; static uint32_t current_time() { uint32_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint32_t)ti.tv_sec * 1000; t += ti.tv_nsec / 1000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint32_t)tv.tv_sec * 1000; t += tv.tv_usec / 1000; #endif \treturn t; } int MinHeapTimer::AddTimer(uint32_t expire, TimerHandler cb) { int64_t timeout = current_time() + expire; TimerNode* node = new TimerNode; int id = Count(); node-\u0026gt;id = id; node-\u0026gt;expire = timeout; node-\u0026gt;cb = cb; node-\u0026gt;idx = (int)_heap.size(); _heap.push_back(node); _shiftUp((int)_heap.size() - 1); _map.insert(make_pair(id, node)); return id; } bool MinHeapTimer::DelTimer(int id) { auto iter = _map.find(id); if (iter == _map.end()) return false; _delNode(iter-\u0026gt;second); return true; } void MinHeapTimer::_delNode(TimerNode *node) { int last = (int)_heap.size() - 1; int idx = node-\u0026gt;idx; if (idx != last) { std::swap(_heap[idx], _heap[last]); _heap[idx]-\u0026gt;idx = idx; if (!_shiftDown(idx)) { _shiftUp(idx); } } _heap.pop_back(); _map.erase(node-\u0026gt;id); delete node; } void MinHeapTimer::ExpireTimer() { if (_heap.empty()) return; uint32_t now = current_time(); do { TimerNode* node = _heap.front(); if (now \u0026lt; node-\u0026gt;expire) break; for (int i = 0; i \u0026lt; _heap.size(); i++) std::cout \u0026lt;\u0026lt; \u0026#34;touch idx: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34; id: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;id \u0026lt;\u0026lt; \u0026#34; expire: \u0026#34; \u0026lt;\u0026lt; _heap[i]-\u0026gt;expire \u0026lt;\u0026lt; std::endl; if (node-\u0026gt;cb) { node-\u0026gt;cb(node); } _delNode(node); } while(!_heap.empty()); } bool MinHeapTimer::_shiftDown(int pos){ int last = (int)_heap.size()-1; int idx = pos; for (;;) { int left = 2 * idx + 1; if ((left \u0026gt;= last) || (left \u0026lt; 0)) { break; } int min = left; // left child  int right = left + 1; if (right \u0026lt; last \u0026amp;\u0026amp; !_lessThan(left, right)) { min = right; // right child  } if (!_lessThan(min, idx)) { break; } std::swap(_heap[idx], _heap[min]); _heap[idx]-\u0026gt;idx = idx; _heap[min]-\u0026gt;idx = min; idx = min; } return idx \u0026gt; pos; } void MinHeapTimer::_shiftUp(int pos) { for (;;) { int parent = (pos - 1) / 2; // parent node  if (parent == pos || !_lessThan(pos, parent)) { break; } std::swap(_heap[parent], _heap[pos]); _heap[parent]-\u0026gt;idx = parent; _heap[pos]-\u0026gt;idx = pos; pos = parent; } } void print_hello(TimerNode *te) { std::cout \u0026lt;\u0026lt; \u0026#34;hello world time = \u0026#34; \u0026lt;\u0026lt; te-\u0026gt;idx \u0026lt;\u0026lt; \u0026#34;\\t\u0026#34; \u0026lt;\u0026lt; te-\u0026gt;id \u0026lt;\u0026lt; std::endl; } int main() { MinHeapTimer mht; mht.AddTimer(0, print_hello); mht.AddTimer(1000, print_hello); mht.AddTimer(7000, print_hello); mht.AddTimer(2000, print_hello); mht.AddTimer(9000, print_hello); mht.AddTimer(10000, print_hello); mht.AddTimer(6000, print_hello); mht.AddTimer(3000, print_hello); for (;;) { mht.ExpireTimer(); usleep(10000); } return 0; } 思路与红黑树的大致相同，不做赘述\n单层时间轮： 与最小堆不同，我们这里需要使用静态数组，宏定义数组大小即可，最长定时时间10秒，单位时间1秒，所以使用大于10的数组大小。\n#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stdint.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  #define MAX_TIMER ((1\u0026lt;\u0026lt;17)-1) #define MAX_CONN ((1\u0026lt;\u0026lt;16)-1)  typedef struct conn_node { //也可以考虑添加callback  uint8_t used; int id; } conn_node_t; typedef struct timer_node { //定时器节点 \tstruct timer_node *next; struct conn_node *node; uint32_t idx; } timer_node_t; static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储 static conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储  static uint32_t t_iter = 0; //创建过的定时器节点数目 static uint32_t c_iter = 0;//创建过的连接节点数目  timer_node_t * get_timer_node() { // 注意：没有检测定时任务数超过 MAX_TIMER 的情况！！！  t_iter++; while (timer_nodes[t_iter \u0026amp; MAX_TIMER].idx \u0026gt; 0) { t_iter++; //这个定时器节点正在被使用，换下一个试试  } timer_nodes[t_iter].idx = t_iter; return \u0026amp;timer_nodes[t_iter]; } conn_node_t * get_conn_node() { // 注意：没有检测连接数超过 MAX_CONN 的情况  c_iter++; while (conn_nodes[c_iter \u0026amp; MAX_CONN].used \u0026gt; 0) { c_iter++;//这个连接节点正在被使用，换下一个试试  } return \u0026amp;conn_nodes[c_iter]; } #define TW_SIZE 16 #define EXPIRE 10 #define TW_MASK (TW_SIZE - 1) static uint32_t tick = 0; typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; void add_conn(link_list_t *tw, conn_node_t *cnode, int delay) { link_list_t *list = \u0026amp;tw[(tick+EXPIRE+delay) \u0026amp; TW_MASK]; timer_node_t * tnode = get_timer_node(); cnode-\u0026gt;used++; tnode-\u0026gt;node = cnode; list-\u0026gt;tail-\u0026gt;next = tnode; list-\u0026gt;tail = tnode; tnode-\u0026gt;next = NULL; } //尾插，因为先到的定时任务需要先访问，比较严谨  void link_clear(link_list_t *list) { list-\u0026gt;head.next = NULL; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); } void check_conn(link_list_t *tw) { int32_t itick = tick; //获取当前时间戳  tick++;//全局时间戳加一  link_list_t *list = \u0026amp;tw[itick \u0026amp; TW_MASK];// 获取当前时间戳对应的定时器链表，对其进行检查  timer_node_t *current = list-\u0026gt;head.next;//定时器链表的头并不是一个实际的定时器节点，所以获取head的next，这是第一个实际的定时器节点  while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; //下一个  conn_node_t *cn = temp-\u0026gt;node; //从定时器节点中拿到对应的连接  cn-\u0026gt;used--; //连接的引用计数减一  temp-\u0026gt;idx = 0;//该定时器可以回收留待下次使用  if (cn-\u0026gt;used == 0) { //引用计数为0，说明连接失活，可以做相应处理  printf(\u0026#34;fd:%d kill down\\n\u0026#34;, cn-\u0026gt;id); temp-\u0026gt;next = NULL; continue; } printf(\u0026#34;fd:%d used:%d\\n\u0026#34;, cn-\u0026gt;id, cn-\u0026gt;used); } link_clear(list);//检查完这一时间戳的事件就可以删除了，等待下一次继续添加 } static time_t current_time() { time_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (time_t)ti.tv_sec; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (time_t)tv.tv_sec; #endif \treturn t; } int main() { memset(timer_nodes, 0, MAX_TIMER * sizeof(timer_node_t)); memset(conn_nodes, 0, MAX_CONN * sizeof(conn_node_t)); // init link list  link_list_t tw[TW_SIZE]; memset(tw, 0, TW_SIZE * sizeof(link_list_t)); for (int i = 0; i \u0026lt; TW_SIZE; i++) { link_clear(\u0026amp;tw[i]); } // 该测试起始时间为0秒，所以 delay 不能添加超过10的数。  { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10001; add_conn(tw, node, 0); add_conn(tw, node, 5); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10002; add_conn(tw, node, 0); } { conn_node_t *node = get_conn_node(); node-\u0026gt;id = 10003; add_conn(tw, node, 3); } time_t start = current_time(); for (;;) { time_t now = current_time(); if (now - start \u0026gt; 0) { for (int i=0; i\u0026lt;now-start; i++) //循环保证单位时间移动（1秒走一格）  check_conn(tw); start = now; printf(\u0026#34;check conn tick:%d\\n\u0026#34;, tick); } usleep(20000); } return 0; } static timer_node_t timer_nodes[MAX_TIMER] = {0}; //所有定时器节点存储\nstatic conn_node_t conn_nodes[MAX_CONN] = {0};//所有连接节点存储\n创建节点存储数组可以做到节点的回收利用，减少节点的创建和删除，静态数组连续存储，访问时间快\n#define TW_SIZE 16 时间轮大小\n#define EXPIRE 10 定时时间\n#define TW_MASK (TW_SIZE - 1) 用于轮转的掩码\nstatic uint32_t tick = 0; 时间戳,指示当前位置 add_conn(link_list_t *tw, conn_node_t *cnode, int delay)\n可以设置delay，根据提供的conn_node_t初始化定时任务，并放入tw中正确位置：(tick+EXPIRE+delay) \u0026amp; TW_MASK\nlink_clear(link_list_t *list)清空一个时间戳上的整个定时器链表\ncheck_conn(link_list_t *tw) 检查当前时间戳对应定时器链表的连接，并对齐引用计数减一，引用计数为0可以做相应处理（回调函数中可以关闭conn，或者给一个信号到管理连接的线程等等）\n多层时间轮： 多层时间轮就是在单层的基础上添加几个粒度更大的层，我们只在粒度最小的层上运行定时器（粒度最小的层运行与单层时间轮一样）即可，用钟表举例：最小层运行一周代表秒针转一圈也就是需要转分针了，这时就从第二层上映射下一分钟的所有定时任务到最小层上，第二层每一分钟的任务同时也需要前移（只要移动指针就可以，不用移动所有任务链表），前移操作很容易理解，但映射需要说一下。举例：\n我们有一个定时任务是在1分27秒后触发，模拟时钟，第一层的数组大小为60，代表60秒，第二层也是60代表60分钟，第三层24。假设此时tick（时间戳）为0，我们需要将这个定时任务放置于第二层的第0号位置（这里先不区分粒度），并在任务内保存准确的expire时间（1分27），当第一层60秒转完后，我们就需要开始映射，将第二层0号位置上的一串定时任务遍历，一个一个根据其保存的expire时间对60秒取余获得其应该在第一层的位置（27秒对应26号位置），映射到第一层上的26号位置（串到26号下面的链表中）。在下一轮的第一层运转中就会触发定时任务啦。同理，第三层（时针）到第二层（分针）的映射也是相同，使用mask取出定时任务的分位（比如1小时32分15秒就是取出32分，对应第二层的31号位置），找到对应位置放到对应链表中去。\n写的代码的时间轮结构如图：\n\r\n这里所有层级的位数加起来刚好是32位，最低层级使用2的8次方，后续四层都为2的四次方，这样的设计正好用完一个unsigned INT数，在使用mask的时候十分方便。最低层级粒度为10ms。\n这里直接附上代码，自行阅读即可：\n头文件\n#ifndef _MARK_TIMEWHEEL_ #define _MARK_TIMEWHEEL_  #include \u0026lt;stdint.h\u0026gt; #define TIME_NEAR_SHIFT 8 #define TIME_NEAR (1 \u0026lt;\u0026lt; TIME_NEAR_SHIFT) #define TIME_LEVEL_SHIFT 6 #define TIME_LEVEL (1 \u0026lt;\u0026lt; TIME_LEVEL_SHIFT) #define TIME_NEAR_MASK (TIME_NEAR-1) #define TIME_LEVEL_MASK (TIME_LEVEL-1)  typedef struct timer_node timer_node_t; typedef void (*handler_pt) (struct timer_node *node); struct timer_node { struct timer_node *next; uint32_t expire;//过期时间（非定时时间）  handler_pt callback; uint8_t cancel; //由于idx很多且不断变化很难找到特定节点，通过设计cancel成员来在触发时取消触发操作 \tint id; // 此时携带参数 }; timer_node_t* add_timer(int time, handler_pt func, int threadid); void expire_timer(void); void del_timer(timer_node_t* node); void init_timer(void); void clear_timer(); #endif  主文件\n#include \u0026#34;spinlock.h\u0026#34;#include \u0026#34;timewheel.h\u0026#34;#include \u0026lt;string.h\u0026gt;#include \u0026lt;stddef.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; #if defined(__APPLE__) #include \u0026lt;AvailabilityMacros.h\u0026gt;#include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;mach/task.h\u0026gt;#include \u0026lt;mach/mach.h\u0026gt;#else #include \u0026lt;time.h\u0026gt;#endif  typedef struct link_list { timer_node_t head; timer_node_t *tail; }link_list_t; typedef struct timer { link_list_t near[TIME_NEAR]; link_list_t t[4][TIME_LEVEL]; struct spinlock lock; uint32_t time; uint64_t current; uint64_t current_point; }s_timer_t; static s_timer_t * TI = NULL; timer_node_t * link_clear(link_list_t *list) { timer_node_t * ret = list-\u0026gt;head.next; list-\u0026gt;head.next = 0; list-\u0026gt;tail = \u0026amp;(list-\u0026gt;head); return ret; } void link(link_list_t *list, timer_node_t *node) { list-\u0026gt;tail-\u0026gt;next = node; list-\u0026gt;tail = node; node-\u0026gt;next=0; } void add_node(s_timer_t *T, timer_node_t *node) { uint32_t time=node-\u0026gt;expire; uint32_t current_time=T-\u0026gt;time; uint32_t msec = time - current_time; if (msec \u0026lt; TIME_NEAR) { //[0, 0x100) \tlink(\u0026amp;T-\u0026gt;near[time\u0026amp;TIME_NEAR_MASK],node); } else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+TIME_LEVEL_SHIFT))) {//[0x100, 0x4000) \tlink(\u0026amp;T-\u0026gt;t[0][((time\u0026gt;\u0026gt;TIME_NEAR_SHIFT) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+2*TIME_LEVEL_SHIFT))) {//[0x4000, 0x100000) \tlink(\u0026amp;T-\u0026gt;t[1][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else if (msec \u0026lt; (1 \u0026lt;\u0026lt; (TIME_NEAR_SHIFT+3*TIME_LEVEL_SHIFT))) {//[0x100000, 0x4000000) \tlink(\u0026amp;T-\u0026gt;t[2][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 2*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} else {//[0x4000000, 0xffffffff] \tlink(\u0026amp;T-\u0026gt;t[3][((time\u0026gt;\u0026gt;(TIME_NEAR_SHIFT + 3*TIME_LEVEL_SHIFT)) \u0026amp; TIME_LEVEL_MASK)],node);\t} } timer_node_t* add_timer(int time, handler_pt func, int threadid) { timer_node_t *node = (timer_node_t *)malloc(sizeof(*node)); spinlock_lock(\u0026amp;TI-\u0026gt;lock); node-\u0026gt;expire = time+TI-\u0026gt;time;// 每10ms加1 0 \tnode-\u0026gt;callback = func; node-\u0026gt;id = threadid; if (time \u0026lt;= 0) { node-\u0026gt;callback(node); free(node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return NULL; } add_node(TI, node); spinlock_unlock(\u0026amp;TI-\u0026gt;lock); return node; } void move_list(s_timer_t *T, int level, int idx) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;t[level][idx]); while (current) { timer_node_t *temp=current-\u0026gt;next; add_node(T,current); current=temp; } } void timer_shift(s_timer_t *T) { int mask = TIME_NEAR; uint32_t ct = ++T-\u0026gt;time; if (ct == 0) { move_list(T, 3, 0); } else { // ct / 256 \tuint32_t time = ct \u0026gt;\u0026gt; TIME_NEAR_SHIFT; int i=0; // ct % 256 == 0 \twhile ((ct \u0026amp; (mask-1))==0) { int idx=time \u0026amp; TIME_LEVEL_MASK; if (idx!=0) { move_list(T, i, idx); break;\t} mask \u0026lt;\u0026lt;= TIME_LEVEL_SHIFT; time \u0026gt;\u0026gt;= TIME_LEVEL_SHIFT; ++i; } } } void dispatch_list(timer_node_t *current) { do { timer_node_t * temp = current; current=current-\u0026gt;next; if (temp-\u0026gt;cancel == 0) temp-\u0026gt;callback(temp); free(temp); } while (current); } void timer_execute(s_timer_t *T) { int idx = T-\u0026gt;time \u0026amp; TIME_NEAR_MASK; while (T-\u0026gt;near[idx].head.next) { timer_node_t *current = link_clear(\u0026amp;T-\u0026gt;near[idx]); spinlock_unlock(\u0026amp;T-\u0026gt;lock); dispatch_list(current); spinlock_lock(\u0026amp;T-\u0026gt;lock); } } void timer_update(s_timer_t *T) { spinlock_lock(\u0026amp;T-\u0026gt;lock); timer_execute(T); timer_shift(T); timer_execute(T); spinlock_unlock(\u0026amp;T-\u0026gt;lock); } void del_timer(timer_node_t *node) { node-\u0026gt;cancel = 1; } s_timer_t * timer_create_timer() { s_timer_t *r=(s_timer_t *)malloc(sizeof(s_timer_t)); memset(r,0,sizeof(*r)); int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_clear(\u0026amp;r-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_clear(\u0026amp;r-\u0026gt;t[i][j]); } } spinlock_init(\u0026amp;r-\u0026gt;lock); r-\u0026gt;current = 0; return r; } uint64_t gettime() { uint64_t t; #if !defined(__APPLE__) || defined(AVAILABLE_MAC_OS_X_VERSION_10_12_AND_LATER) \tstruct timespec ti; clock_gettime(CLOCK_MONOTONIC, \u0026amp;ti); t = (uint64_t)ti.tv_sec * 100; t += ti.tv_nsec / 10000000; #else \tstruct timeval tv; gettimeofday(\u0026amp;tv, NULL); t = (uint64_t)tv.tv_sec * 100; t += tv.tv_usec / 10000; #endif \treturn t; } void expire_timer(void) { uint64_t cp = gettime(); if (cp != TI-\u0026gt;current_point) { uint32_t diff = (uint32_t)(cp - TI-\u0026gt;current_point); TI-\u0026gt;current_point = cp; int i; for (i=0; i\u0026lt;diff; i++) { timer_update(TI); } } } void init_timer(void) { TI = timer_create_timer(); TI-\u0026gt;current_point = gettime(); } void clear_timer() { int i,j; for (i=0;i\u0026lt;TIME_NEAR;i++) { link_list_t * list = \u0026amp;TI-\u0026gt;near[i]; timer_node_t* current = list-\u0026gt;head.next; while(current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;near[i]); } for (i=0;i\u0026lt;4;i++) { for (j=0;j\u0026lt;TIME_LEVEL;j++) { link_list_t * list = \u0026amp;TI-\u0026gt;t[i][j]; timer_node_t* current = list-\u0026gt;head.next; while (current) { timer_node_t * temp = current; current = current-\u0026gt;next; free(temp); } link_clear(\u0026amp;TI-\u0026gt;t[i][j]); } } } ","date":"2020-04-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/427_hud2c5f23b59d3c6c6e1b3f4674af3ff5b_6078631_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%AE%9A%E6%97%B6%E5%99%A8/","title":"定时器"},{"content":"进阶TCP服务器 该模块涉及TCP服务器常见点：并发量、IO模型\nIO网络模型 阻塞IO （Blocking IO） \r\n阻塞IO的情况下，我们如果需要更多的并发，只能使用多线程，一个IO占用一个线程，资源浪费很大但是在并发量小的情况下性能很强。\n非阻塞IO （Non-Blocking IO） \r\n在非阻塞状态下，recv() 接口在被调用后立即返回，返回值代表了不同的含义。如在本例中，\n recv() 返回值大于 0，表示接受数据完毕，返回值即是接受到的字节数； recv() 返回 0，表示连接已经正常断开； recv() 返回 -1，且 errno 等于 EAGAIN，表示 recv 操作还没执行完成； recv() 返回 -1，且 errno 不等于 EAGAIN，表示 recv 操作遇到系统错误 errno。  非阻塞的接口相比于阻塞型接口的显著差异在于，在被调用之后立即返回。使用如下的函数可以将某句柄 fd 设为非阻塞状态：\nfcntl( fd, F_SETFL, O_NONBLOCK ); 多路复用IO （IO Multiplexing） \r\n多路复用IO，select/poll、epoll。\nselect/poll select和poll很相似，在检测IO时间的时候都需要遍历整个FD存储结构，只是select使用数组存储FD，其具有最大值限制，而poll使用链表无最大值限制（与内存大小相关）。\n先来分析select的优缺点，这样就知道epoll相比select的优势等。\nselect 本质上是通过设置或检查存放fd标志位的数据结构进行下一步处理。 这带来缺点： 单个进程可监视的fd数量被限制，即能监听端口的数量有限 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试 一般该数和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认1024个，64位默认2048。\n当socket较多时，每次select都要通过遍历FD_SETSIZE个socket，不管是否活跃，这会浪费很多CPU时间。如果能给 socket 注册某个回调函数，当他们活跃时，自动完成相关操作，即可避免轮询，这就是epoll与kqueue。\nselect 调用流程\n  使用copy_from_user从用户空间拷贝fd_set到内核空间\n  注册回调函数__pollwait\n  遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）\n  以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。\n  __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk-\u0026gt;sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。\n  poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。\n  如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。\n  把fd_set从内核空间拷贝到用户空间。\n    内核需要将消息传递到用户空间，都需要内核拷贝动作。需要维护一个用来存放大量fd的数据结构，使得用户空间和内核空间在传递该结构时复制开销大。\n  每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大\n  同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大\n  select支持的文件描述符数量太小了，默认是1024\n  epoll  没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口） 效率提升，不是轮询，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数 即Epoll最大的优点就在于它只关心“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 epoll通过内核和用户空间共享一块内存来实现的  异步 IO（Asynchronous I/O） \r\n异步IO工作流程可以看图\n信号驱动 IO（signal driven I/O， SIGIO） \r\n服务器模型 Reactor 首先来回想一下普通函数调用的机制：程序调用某函数，函数执行，程序等待，函数将 结果和控制权返回给程序，程序继续处理。Reactor 释义“反应堆”，是一种事件驱动机制。 和普通函数调用的不同之处在于：应用程序不是主动的调用某个 API 完成处理，而是恰恰 相反，Reactor 逆置了事件处理流程，应用程序需要提供相应的接口并注册到 Reactor 上， 如果相应的时间发生，Reactor 将主动调用应用程序注册的接口，这些接口又称为“回调函 数”。\nReactor 模式是处理并发 I/O 比较常见的一种模式，用于同步 I/O，中心思想是将所有要 处理的 I/O 事件注册到一个中心 I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上； 一旦有 I/O 事件到来或是准备就绪(文件描述符或 socket 可读、写)，多路复用器返回并将事 先注册的相应 I/O 事件分发到对应的处理器中。\nReactor 模型有三个重要的组件：\n 多路复用器：由操作系统提供，在 linux 上一般是 select, poll, epoll 等系统调用。 事件分发器：将多路复用器中返回的就绪事件分到对应的处理函数中。 事件处理器：负责处理特定事件的处理函数。  Proactor Proactor 最大的特点是 使用异步 I/O。所有的 I/O 操作都交由系统提供的异步 I/O 接口去执行。工作线程仅仅负责业务逻辑。在 Proactor 中，用户函数启动一个异步的文件操作。同时将这个操作注册到多路复用器上。多路复用器并不关心文件是否可读或可写而是关心这个异步读操作是否完成。异步操作是操作系统完成，用户程序不需要关心。多路复用器等待直到有完成通知到来。当操作系统完成了读文件操作——将读到的数据复制到了用户先前提供的缓冲区之后，通知多路复用器相关操作已完成。多路复用器再调用相应的处理程序，处理数据。\nProactor 增加了编程的复杂度，但给工作线程带来了更高的效率。Proactor 可以在 系统态将读写优化，利用 I/O 并行能力，提供一个高性能单线程模型。在 windows 上，由于没有 epoll 这样的机制，因此提供了 IOCP 来支持高并发， 由于操作系统做了较好的优化，windows 较常采用 Proactor 的模型利用完成端口来实现服务器。在 linux 上，在2.6 内核出现了 aio 接口，但 aio 实际效果并不理想，它的出现，主要是解决 poll 性能不佳的问题，但实际上经过测试，epoll 的性能高于 poll+aio，并且 aio 不能处理 accept，因此 linux 主要还是以 Reactor 模型为主。在不使用操作系统提供的异步 I/O 接口的情况下，还可以使用 Reactor 来模拟 Proactor， 差别是：使用异步接口可以利用系统提供的读写并行能力，而在模拟的情况下，这需要在用户态实现。具体的做法只需要这样：\n 注册读事件（同时再提供一段缓冲区） 事件分离器等待可读事件 事件到来，激活分离器，分离器（立即读数据，写缓冲区）调用事件处理器 事件处理器处理数据，删除事件(需要再用异步接口注册)  我们知道，Boost.asio 库采用的即为 Proactor 模型。不过 Boost.asio 库在 Linux 平台采用 epoll 实现的 Reactor 来模拟 Proactor，并且另外开了一个线程来完成读写调度。\n并发量 通过linux系统的配置、使用多路复用的IO就可以很容易达到百万的并发量这个数量级。\n配置： net.ipv4.tcp_mem = 252144 524288 786432 net.ipv4.tcp_wmem = 2048 2048 4096 net.ipv4.tcp_rmem = 2048 2048 4096 fs.file-max = 1048576 net.nf_conntrack_max = 1048576 net.netfilter.nf_conntrack_tcp_timeout_established = 1200 fs.file-max为fd可以达到的最大值\nnet.nf_conntrack_max为防火墙允许的最大连接数\nnet.ipv4.tcp_mem 三道台阶，协议栈占有空间大于这三道台阶时采取不同的优化方案\nnet.ipv4.tcp_wmem 和 net.ipv4.tcp_rmem 为写和收buffer大小阶梯，默认为中间大小，动态变化的最大值为第三个值。\n上面的buffer根据传输的文件类型可以采用不同的值。\nulimit -a 可以看文件描述符数量限制，调整到1048576（ulimit -n）\nIO： 使用epoll，监听多个端口（100就够）\n实现（Reactor-Epoll） #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt;#include \u0026lt;netinet/tcp.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt;#include \u0026lt;pthread.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;sys/epoll.h\u0026gt; #define BUFFER_LENGTH\t1024  struct sockitem { // \tint sockfd; int (*callback)(int fd, int events, void *arg); char recvbuffer[BUFFER_LENGTH]; // \tchar sendbuffer[BUFFER_LENGTH]; int rlength; int slength; }; // mainloop / eventloop --\u0026gt; epoll --\u0026gt; struct reactor { int epfd; struct epoll_event events[512]; }; struct reactor *eventloop = NULL; int recv_cb(int fd, int events, void *arg); int send_cb(int fd, int events, void *arg) { struct sockitem *si = (struct sockitem*)arg; send(fd, si-\u0026gt;sendbuffer, si-\u0026gt;slength, 0); //  struct epoll_event ev; ev.events = EPOLLIN | EPOLLET; //ev.data.fd = clientfd; \tsi-\u0026gt;sockfd = fd; si-\u0026gt;callback = recv_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_MOD, fd, \u0026amp;ev); } // ./epoll 8080  int recv_cb(int fd, int events, void *arg) { //int clientfd = events[i].data.fd; \tstruct sockitem *si = (struct sockitem*)arg; struct epoll_event ev; //char buffer[1024] = {0}; \tint ret = recv(fd, si-\u0026gt;recvbuffer, BUFFER_LENGTH, 0); if (ret \u0026lt; 0) { if (errno == EAGAIN || errno == EWOULDBLOCK) { // \treturn -1; } else { } ev.events = EPOLLIN; //ev.data.fd = fd; \tepoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_DEL, fd, \u0026amp;ev); close(fd); free(si); } else if (ret == 0) { //  // \tprintf(\u0026#34;disconnect %d\\n\u0026#34;, fd); ev.events = EPOLLIN; //ev.data.fd = fd; \tepoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_DEL, fd, \u0026amp;ev); close(fd); free(si); } else { printf(\u0026#34;Recv: %s, %d Bytes\\n\u0026#34;, si-\u0026gt;recvbuffer, ret); si-\u0026gt;rlength = ret; memcpy(si-\u0026gt;sendbuffer, si-\u0026gt;recvbuffer, si-\u0026gt;rlength); si-\u0026gt;slength = si-\u0026gt;rlength; struct epoll_event ev; ev.events = EPOLLOUT | EPOLLET; //ev.data.fd = clientfd; \tsi-\u0026gt;sockfd = fd; si-\u0026gt;callback = send_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_MOD, fd, \u0026amp;ev); } } int accept_cb(int fd, int events, void *arg) { struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(fd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); if (clientfd \u0026lt;= 0) return -1; char str[INET_ADDRSTRLEN] = {0}; printf(\u0026#34;recv from %s at port %d\\n\u0026#34;, inet_ntop(AF_INET, \u0026amp;client_addr.sin_addr, str, sizeof(str)), ntohs(client_addr.sin_port)); struct epoll_event ev; ev.events = EPOLLIN | EPOLLET; //ev.data.fd = clientfd;  struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem)); si-\u0026gt;sockfd = clientfd; si-\u0026gt;callback = recv_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_ADD, clientfd, \u0026amp;ev); return clientfd; } int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { return -1; } int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); if (sockfd \u0026lt; 0) { return -1; } struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if (bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt; 0) { return -2; } if (listen(sockfd, 5) \u0026lt; 0) { return -3; } eventloop = (struct reactor*)malloc(sizeof(struct reactor)); // epoll opera  eventloop-\u0026gt;epfd = epoll_create(1); struct epoll_event ev; ev.events = EPOLLIN; struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem)); si-\u0026gt;sockfd = sockfd; si-\u0026gt;callback = accept_cb; ev.data.ptr = si; epoll_ctl(eventloop-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); while (1) { int nready = epoll_wait(eventloop-\u0026gt;epfd, eventloop-\u0026gt;events, 512, -1); if (nready \u0026lt; -1) { break; } int i = 0; for (i = 0;i \u0026lt; nready;i ++) { if (eventloop-\u0026gt;events[i].events \u0026amp; EPOLLIN) { //printf(\u0026#34;sockitem\\n\u0026#34;); \tstruct sockitem *si = (struct sockitem*)eventloop-\u0026gt;events[i].data.ptr; si-\u0026gt;callback(si-\u0026gt;sockfd, eventloop-\u0026gt;events[i].events, si); } if (eventloop-\u0026gt;events[i].events \u0026amp; EPOLLOUT) { struct sockitem *si = (struct sockitem*)eventloop-\u0026gt;events[i].data.ptr; si-\u0026gt;callback(si-\u0026gt;sockfd, eventloop-\u0026gt;events[i].events, si); } } } } 实现（Reactor-Epoll的百万并发） 前面有reactor-epoll自己实现的了，这里直接使用netty的reactor实现就可以。\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;errno.h\u0026gt; #define BUFFER_LENGTH\t1024 #define MAX_EPOLL_EVENTS\t1024*1024 //connection #define MAX_EPOLL_ITEM\t102400 //con #define SERVER_PORT\t8888  #define LISTEN_PORT_COUNT\t100  typedef int NCALLBACK(int ,int, void*); struct ntyevent { int fd; int events; void *arg; int (*callback)(int fd, int events, void *arg); int status; char buffer[BUFFER_LENGTH]; int length; long last_active; }; struct ntyreactor { int epfd; struct ntyevent *events; // 1024 * 1024 }; int recv_cb(int fd, int events, void *arg); int send_cb(int fd, int events, void *arg); void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) { ev-\u0026gt;fd = fd; ev-\u0026gt;callback = callback; ev-\u0026gt;events = 0; ev-\u0026gt;arg = arg; ev-\u0026gt;last_active = time(NULL); return ; } int nty_event_add(int epfd, int events, struct ntyevent *ev) { struct epoll_event ep_ev = {0, {0}}; ep_ev.data.ptr = ev; ep_ev.events = ev-\u0026gt;events = events; int op; if (ev-\u0026gt;status == 1) { op = EPOLL_CTL_MOD; } else { op = EPOLL_CTL_ADD; ev-\u0026gt;status = 1; } if (epoll_ctl(epfd, op, ev-\u0026gt;fd, \u0026amp;ep_ev) \u0026lt; 0) { printf(\u0026#34;event add failed [fd=%d], events[%d]\\n\u0026#34;, ev-\u0026gt;fd, events); return -1; } return 0; } int nty_event_del(int epfd, struct ntyevent *ev) { struct epoll_event ep_ev = {0, {0}}; if (ev-\u0026gt;status != 1) { return -1; } ep_ev.data.ptr = ev; ev-\u0026gt;status = 0; epoll_ctl(epfd, EPOLL_CTL_DEL, ev-\u0026gt;fd, \u0026amp;ep_ev); return 0; } int recv_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; struct ntyevent *ev = reactor-\u0026gt;events+fd; int len = recv(fd, ev-\u0026gt;buffer, BUFFER_LENGTH, 0); nty_event_del(reactor-\u0026gt;epfd, ev); if (len \u0026gt; 0) { ev-\u0026gt;length = len; ev-\u0026gt;buffer[len] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;C[%d]:%s\\n\u0026#34;, fd, ev-\u0026gt;buffer); nty_event_set(ev, fd, send_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLOUT, ev); } else if (len == 0) { close(ev-\u0026gt;fd); printf(\u0026#34;[fd=%d] pos[%ld], closed\\n\u0026#34;, fd, ev-reactor-\u0026gt;events); } else { close(ev-\u0026gt;fd); printf(\u0026#34;recv[fd=%d] error[%d]:%s\\n\u0026#34;, fd, errno, strerror(errno)); } return len; } int send_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; struct ntyevent *ev = reactor-\u0026gt;events+fd; int len = send(fd, ev-\u0026gt;buffer, ev-\u0026gt;length, 0); if (len \u0026gt; 0) { printf(\u0026#34;send[fd=%d], [%d]%s\\n\u0026#34;, fd, len, ev-\u0026gt;buffer); nty_event_del(reactor-\u0026gt;epfd, ev); nty_event_set(ev, fd, recv_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, ev); } else { close(ev-\u0026gt;fd); nty_event_del(reactor-\u0026gt;epfd, ev); printf(\u0026#34;send[fd=%d] error %s\\n\u0026#34;, fd, strerror(errno)); } return len; } int accept_cb(int fd, int events, void *arg) { struct ntyreactor *reactor = (struct ntyreactor*)arg; if (reactor == NULL) return -1; struct sockaddr_in client_addr; socklen_t len = sizeof(client_addr); int clientfd; if ((clientfd = accept(fd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;len)) == -1) { if (errno != EAGAIN \u0026amp;\u0026amp; errno != EINTR) { } printf(\u0026#34;accept: %s\\n\u0026#34;, strerror(errno)); return -1; } int i = 0; do { #if 0for (i = 0;i \u0026lt; MAX_EPOLL_EVENTS;i ++) { if (reactor-\u0026gt;events[i].status == 0) { break; } } if (i == MAX_EPOLL_EVENTS) { printf(\u0026#34;%s: max connect limit[%d]\\n\u0026#34;, __func__, MAX_EPOLL_EVENTS); break; } #endif \tint flag = 0; if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) \u0026lt; 0) { printf(\u0026#34;%s: fcntl nonblocking failed, %d\\n\u0026#34;, __func__, MAX_EPOLL_EVENTS); break; } nty_event_set(\u0026amp;reactor-\u0026gt;events[clientfd], clientfd, recv_cb, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, \u0026amp;reactor-\u0026gt;events[clientfd]); } while (0); printf(\u0026#34;new connect [%s:%d][time:%ld], pos[%d]\\n\u0026#34;, inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), reactor-\u0026gt;events[i].last_active, i); return 0; } int init_sock(short port) { int fd = socket(AF_INET, SOCK_STREAM, 0); fcntl(fd, F_SETFL, O_NONBLOCK); struct sockaddr_in server_addr; memset(\u0026amp;server_addr, 0, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = htonl(INADDR_ANY); server_addr.sin_port = htons(port); bind(fd, (struct sockaddr*)\u0026amp;server_addr, sizeof(server_addr)); if (listen(fd, 20) \u0026lt; 0) { printf(\u0026#34;listen failed : %s\\n\u0026#34;, strerror(errno)); } printf(\u0026#34;listen port : %d\\n\u0026#34;, port); return fd; } int ntyreactor_init(struct ntyreactor *reactor) { if (reactor == NULL) return -1; memset(reactor, 0, sizeof(struct ntyreactor)); reactor-\u0026gt;epfd = epoll_create(1); if (reactor-\u0026gt;epfd \u0026lt;= 0) { printf(\u0026#34;create epfd in %s err %s\\n\u0026#34;, __func__, strerror(errno)); return -2; } reactor-\u0026gt;events = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent)); if (reactor-\u0026gt;events == NULL) { printf(\u0026#34;create epfd in %s err %s\\n\u0026#34;, __func__, strerror(errno)); close(reactor-\u0026gt;epfd); return -3; } } int ntyreactor_destory(struct ntyreactor *reactor) { close(reactor-\u0026gt;epfd); free(reactor-\u0026gt;events); } int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) { if (reactor == NULL) return -1; if (reactor-\u0026gt;events == NULL) return -1; nty_event_set(\u0026amp;reactor-\u0026gt;events[sockfd], sockfd, acceptor, reactor); nty_event_add(reactor-\u0026gt;epfd, EPOLLIN, \u0026amp;reactor-\u0026gt;events[sockfd]); return 0; } int ntyreactor_run(struct ntyreactor *reactor) { if (reactor == NULL) return -1; if (reactor-\u0026gt;epfd \u0026lt; 0) return -1; if (reactor-\u0026gt;events == NULL) return -1; struct epoll_event events[MAX_EPOLL_ITEM]; int checkpos = 0, i; while (1) { #if 0long now = time(NULL); for (i = 0;i \u0026lt; 100;i ++, checkpos ++) { if (checkpos == MAX_EPOLL_EVENTS) { checkpos = 0; } if (reactor-\u0026gt;events[checkpos].status != 1) { continue; } long duration = now - reactor-\u0026gt;events[checkpos].last_active; if (duration \u0026gt;= 60) { close(reactor-\u0026gt;events[checkpos].fd); printf(\u0026#34;[fd=%d] timeout\\n\u0026#34;, reactor-\u0026gt;events[checkpos].fd); nty_event_del(reactor-\u0026gt;epfd, \u0026amp;reactor-\u0026gt;events[checkpos]); } } #endif  int nready = epoll_wait(reactor-\u0026gt;epfd, events, MAX_EPOLL_ITEM, 1000); if (nready \u0026lt; 0) { printf(\u0026#34;epoll_wait error, exit\\n\u0026#34;); continue; } for (i = 0;i \u0026lt; nready;i ++) { struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr; if ((events[i].events \u0026amp; EPOLLIN) \u0026amp;\u0026amp; (ev-\u0026gt;events \u0026amp; EPOLLIN)) { ev-\u0026gt;callback(ev-\u0026gt;fd, events[i].events, ev-\u0026gt;arg); } if ((events[i].events \u0026amp; EPOLLOUT) \u0026amp;\u0026amp; (ev-\u0026gt;events \u0026amp; EPOLLOUT)) { ev-\u0026gt;callback(ev-\u0026gt;fd, events[i].events, ev-\u0026gt;arg); } } } } int main(int argc, char *argv[]) { unsigned short port = SERVER_PORT; if (argc == 2) { port = atoi(argv[1]); } struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor)); ntyreactor_init(reactor); int listenfd[LISTEN_PORT_COUNT] = {0}; int i = 0; for (i = 0;i \u0026lt; LISTEN_PORT_COUNT;i ++) { listenfd[i] = init_sock(port+i); ntyreactor_addlistener(reactor, listenfd[i], accept_cb); } ntyreactor_run(reactor); ntyreactor_destory(reactor); for (i = 0;i \u0026lt; LISTEN_PORT_COUNT;i ++) { close(listenfd[i]); } return 0; } ","date":"2020-04-03T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/406_huf20403e8966ebf3b89c1d187871e8eda_4587138_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%BF%9B%E9%98%B6tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"进阶TCP服务器"},{"content":"设计模式 首先先上所有设计模式的原则，这些原则贯彻于每一种设计模式中，是各种设计模式的根\n原则   依赖倒置原则\n 高层模块不应该依赖低层模块，⼆者都应该依赖抽象。 抽象不应该依赖具体实现，具体实现应该依赖于抽象。    开放封闭原则\n ⼀个类应该对扩展开放，对修改关闭。    面向接口编程\n 不将变量类型声明为某个特定的具体类，而是声明为某个接⼝。 客户程序无需获知对象的具体类型，只需要知道对象所具有的接口。 减少系统中各部分的依赖关系，从而实现“高内聚、松耦合”的类型设计⽅案。    封装变化点\n 将稳定点和变化点分离，扩展修改变化点；让稳定点与变化点的实现层次分离。    单一职责原则\n ⼀个类应该仅有⼀个引起它变化的原因。    里氏替换原则\n 子类型必须能够替换掉它的父类型；主要出现在⼦类覆盖父类实现，原来使用父亲类型的程序可能出现错误；覆盖了父类方法却没实现父类方法的职责；    接口隔离原则\n 不应该强迫客户依赖于他们不用的方法。 ⼀般用于处理⼀个类拥有比较多的接口，而这些接口涉及到很多职责    对象组合优于类继承\n 继承耦合度⾼，组合耦合度低    模板模式 \r\n首先，先提下使用到的原则：\n 依赖倒置原则 单一职责原则 接口隔离原则  定义： 定义⼀个操作中的算法的骨架 ，⽽将⼀些步骤延迟到子类中。 Template Method使得子类可以不 改变⼀个算法的结构即可重定义该算法的某些特定步骤。\n直接上代码，代码分两部分，第一部分为未使用模板模式时的代码，第二部分为使用模板模式的代码：\n未使用模板模式： #include \u0026lt;iostream\u0026gt;using namespace std; class ZooShow { public: void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show0\u0026#34; \u0026lt;\u0026lt; endl; } void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show2\u0026#34; \u0026lt;\u0026lt; endl; } }; class ZooShowEx { public: void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } void Show3(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; int main () { ZooShow *zs = new ZooShow; ZooShowEx *zs1 = new ZooShowEx; // 流程是固定的（稳定点），应该抽象出来；另外子流程是不应该暴露给客户，违反了接口隔离原则；  zs-\u0026gt;Show0(); zs1-\u0026gt;Show1(); zs-\u0026gt;Show2(); zs1-\u0026gt;Show3(); return 0; } 可以看到主函数中流程一览无余，应该由框架提供接口给客户调用一下走完整个流程\n使用模板模式： #include \u0026lt;iostream\u0026gt;using namespace std; class ZooShow { public: // 固定流程封装到这里  void Show() { Show0(); Show1(); Show2(); Show3(); } protected: // 子流程 使用protected保护起来 不被客户调用 但允许子类扩展  virtual void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show0\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show2\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show1() { } virtual void Show3() { } }; class ZooShowEx : public ZooShow { protected: virtual void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show3(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show4() { //  } }; class ZooShowEx1 : public ZooShow { protected: virtual void Show0(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; class ZooShowEx2 : public ZooShow { protected: virtual void Show1(){ cout \u0026lt;\u0026lt; \u0026#34;show1\u0026#34; \u0026lt;\u0026lt; endl; } virtual void Show2(){ cout \u0026lt;\u0026lt; \u0026#34;show3\u0026#34; \u0026lt;\u0026lt; endl; } }; /* 依赖倒置原则 单一职责原则 接口隔离原则 反向控制：应用程序 框架 应用程序（变化的）应该依赖框架（稳定的），应该是框架去调应用程序，而不是应用程序去调框架 */ int main () { ZooShow *zs = new ZooShowEx; ZooShow *zs1 = new ZooShowEx1; ZooShow *zs2 = new ZooShowEx2; zs-\u0026gt;Show(); return 0; } 使用虚函数，根据不同子类做不同扩展，且流程封装在show内部，满足了接口隔离和依赖倒置原则，主类只负责展示流程，流程的具体细节由子类自己实现，职责单一\n观察者模式 \r\n定义： 定义对象间的⼀种⼀对多（变化）的依赖关系，以便当⼀个对象(Subject)的状态发生改变时，所有 依赖于它的对象都得到通知并自动更新。\n未使用观察者模式： class DisplayA { public: void Show(float temperature); }; class DisplayB { public: void Show(float temperature); }; class WeatherData { }; class DataCenter { public: float CalcTemperature() { WeatherData * data = GetWeatherData(); // ...  float temper/* = */; return temper; } private: WeatherData * GetWeatherData(); // 不同的方式 }; int main() { DataCenter *center = new DataCenter; DisplayA *da = new DisplayA; DisplayB *db = new DisplayB; float temper = center-\u0026gt;CalcTemperature(); da-\u0026gt;Show(temper); db-\u0026gt;Show(temper); return 0; } // 终端变化（增加和删除） 数据中心 不应该受终端变化的影响  使用观察者模式： #include \u0026lt;vector\u0026gt; class IDisplay { public: virtual void Show(float temperature) = 0; virtual ~IDisplay() {} }; class DisplayA : public IDisplay { public: virtual void Show(float temperature); }; class DisplayB : public IDisplay{ public: virtual void Show(float temperature); }; class WeatherData { }; class DataCenter { public: void Attach(IDisplay * ob); void Detach(IDisplay * ob); void Notify() { float temper = CalcTemperature(); for (auto iter = obs.begin(); iter != obs.end(); iter++) { (*iter)-\u0026gt;Show(temper); } } private: virtual WeatherData * GetWeatherData(); virtual float CalcTemperature() { WeatherData * data = GetWeatherData(); // ...  float temper/* = */; return temper; } std::vector\u0026lt;IDisplay*\u0026gt; obs; }; int main() { DataCenter *center = new DataCenter; IDisplay *da = new DisplayA(); IDisplay *db = new DisplayB(); center-\u0026gt;Attach(da); center-\u0026gt;Attach(db); center-\u0026gt;Notify(); //-----  center-\u0026gt;Detach(db); center-\u0026gt;Notify(); return 0; } 可以看到我们将接口统一到datacenter（气象中心）里，由气象中心来对外开放功能，同样的原材料（地点）也要交给我们设置的中心来处理，相当于我们买房需要经过房产中介，卖方也需要经过房产中介，我们都基于房产中介的原则来做事，这就是依赖倒置原则。同时，也符合面向接口编程原则。\n责任链模式 \r\n定义： 使多个对象都有机会处理请求，从⽽避免请求的发送者和接收者之间的耦合关系。将这些对象连成⼀条链，并沿着这条链传递请求，直到有⼀个对象处理它为⽌。\n未使用责任链模式： #include \u0026lt;string\u0026gt; class Context { public: std::string name; int day; }; class LeaveRequest { public: // 随着判断的增加，LeaveRequest类变得不稳定  bool HandleRequest(const Context \u0026amp;ctx) { if (ctx.day \u0026lt;= 3) HandleByMainProgram(ctx); else if (ctx.day \u0026lt;= 10) HandleByProjMgr(ctx); else HandleByBoss(ctx); } private: bool HandleByMainProgram(const Context \u0026amp;ctx) { } bool HandleByProjMgr(const Context \u0026amp;ctx) { } bool HandleByBoss(const Context \u0026amp;ctx) { } }; 可以看到我们每次都需要在handlerequest里加一个判断，作为中间的稳定框架不应被用户修改，我们需要找出稳定的部分独立出来。不稳定的部分可以看到是判断条件和对应的处理方式，我们需要将这一部分通过子类重载，使用虚函数即可。\n使用责任链模式： #include \u0026lt;string\u0026gt; class Context { public: std::string name; int day; }; class IHandler { public: virtual ~IHandler() {} void SetNextHandler(IHandler *next) { next = next; } bool Handle(ctx) { if (CanHandle(ctx)) { return HandleRequest(); } else if (GetNextHandler()) { return GetNextHandler()-\u0026gt;HandleRequest(ctx); } else { // err  } } protected: virtual bool HandleRequest(const Context \u0026amp;ctx) = 0; virtual bool CanHandle(const Context \u0026amp;ctx) =0; IHandler * GetNextHandler() { return next; } private: IHandler *next; }; class HandleByMainProgram : public IHandler { protected: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } virtual bool CanHandle() { //  } }; class HandleByProjMgr : public IHandler { protected: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } virtual bool CanHandle() { //  } }; class HandleByBoss : public IHandler { public: virtual bool HandleRequest(const Context \u0026amp;ctx){ //  } protected: virtual bool CanHandle() { //  } }; int main () { IHandler * h1 = new MainProgram(); IHandler * h2 = new HandleByProjMgr(); IHandler * h3 = new HandleByBoss(); h1-\u0026gt;SetNextHandler(h2); Context ctx; h1-\u0026gt;handle(ctx); return 0; } CanHandle 是判断条件 HandleRequest 就是对应的处理方式了\n我们只需要重载这两个函数就可以\n责任链顾名思义就是一条链，所以需要在对象中保存链中下一个对象的指针，IHandler *next;\n可以看到：依赖倒置原则、开放封闭原则比较明显\n装饰器模式 \r\n定义： 使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成⼀条链，并沿着这条链传递请求，直到有一个对象处理它为止。\n未使用装饰器模式： // 普通员工有销售奖金，累计奖金，部门经理除此之外还有团队奖金；后面可能会添加环比增长奖金，同时可能产生不同的奖金组合； // 销售奖金 = 当月销售额 * 4% // 累计奖金 = 总的回款额 * 0.2% // 部门奖金 = 团队销售额 * 1% // 环比奖金 = (当月销售额-上月销售额) * 1% // 销售后面的参数可能会调整 class Context { public: bool isMgr; // User user;  // double groupsale; }; class Bonus { public: double CalcBonus(Context \u0026amp;ctx) { double bonus = 0.0; bonus += CalcMonthBonus(ctx); bonus += CalcSumBonus(ctx); if (ctx.isMgr) { bonus += CalcGroupBonus(ctx); } return bonus; } private: double CalcMonthBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } double CalcSumBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } double CalcGroupBonus(Context \u0026amp;ctx) { double bonus/* = */; return bonus; } }; int main() { Context ctx; // 设置 ctx  Bonus *bonus = new Bonus; bonus-\u0026gt;CalcBonus(ctx); } 使用装饰器模式： // 普通员工有销售奖金，累计奖金，部门经理除此之外还有团队奖金；后面可能会添加环比增长奖金，同时可能产生不同的奖金组合； // 销售奖金 = 当月销售额 * 4% // 累计奖金 = 总的回款额 * 0.2% // 部门奖金 = 团队销售额 * 1% // 环比奖金 = (当月销售额-上月销售额) * 1% // 销售后面的参数可能会调整 class Context { public: bool isMgr; // User user;  // double groupsale; }; // 试着从职责出发，将职责抽象出来 class CalcBonus { public: CalcBonus(CalcBonus * c = nullptr) {} virtual double Calc(Context \u0026amp;ctx) { return 0.0; // 基本工资  } virtual ~CalcBonus() {} protected: CalcBonus* cc; }; class CalcMonthBonus : public CalcBonus { public: CalcMonthBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double mbonus /*= 计算流程忽略*/; return mbonus + cc-\u0026gt;Calc(ctx); } }; class CalcSumBonus : public CalcBonus { public: CalcSumBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double sbonus /*= 计算流程忽略*/; return sbonus + cc-\u0026gt;Calc(ctx); } }; class CalcGroupBonus : public CalcBonus { public: CalcGroupBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double gbnonus /*= 计算流程忽略*/; return gbnonus + cc-\u0026gt;Calc(ctx); } }; class CalcCycleBonus : public CalcBonus { public: CalcGroupBonus(CalcBonus * c) : cc(c) {} virtual double Calc(Context \u0026amp;ctx) { double gbnonus /*= 计算流程忽略*/; return gbnonus + cc-\u0026gt;Calc(ctx); } }; int main() { // 1. 普通员工  Context ctx1; CalcBonus *base = new CalcBonus(); CalcBonus *cb1 = new CalcMonthBonus(base); CalcBonus *cb2 = new CalcSumBonus(cb1); cb2-\u0026gt;Calc(ctx1); // 2. 部门经理  Context ctx2; CalcBonus *cb3 = new CalcGroupBonus(cb2); cb3-\u0026gt;Calc(ctx2); } 与责任链模式不同的是，装饰器模式是累加而非替换，所以我们不需要一个完全新的类（重载掉整个处理函数）只需要在新的类中调用旧的类中的方法（那就需要传入旧类的对象）并在此基础上加入自己的新的处理部分，这就是装饰。\n我们把稳定的部分（计算）抽象了出来，在此基础上装饰，当我们需要新的接口的时候只需要在稳定的基础上扩展子类就可以，满足了面向接口、封装变化点等\n单例模式 \r\n定义： 保证⼀个类仅有⼀个实例，并提供⼀个该实例的全局访问点。\n通过多个版本来说明单例模式\n单例模式版本1： 简单的单例模式\n// 内存栈区 // 内存堆区 // 常数区 // 静态区 系统释放 // ⼆进制代码区 class Singleton { public: static Singleton * GetInstance() { if (_instance == nullptr) { _instance = new Singleton(); } return _instance; } private: Singleton(){}//构造  Singleton(const Singleton \u0026amp;clone){} //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化  问题在于我们要销毁的时候没有对应的处理函数\n单例模式版本2： class Singleton { public: static Singleton * GetInstance() { if (_instance == nullptr) { _instance = new Singleton(); atexit(Destructor); } return _instance; } ~Singleton() {} private: static void Destructor() { if (nullptr != _instance) { delete _instance; _instance = nullptr; } } Singleton();//构造  Singleton(const Singleton \u0026amp;cpy); //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化  加入销毁（atexit在函数退出时调用）但是存在线程安全问题，单例模式创建的对象需要全局唯一，多线程下如果同时进入创建函数内可能创建多个对象。\n单例模式版本3： 使用互斥锁保证线程安全\n#include \u0026lt;mutex\u0026gt;class Singleton { // 懒汉模式 lazy load get的时候才创建 public: static Singleton * GetInstance() { //std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); // 3.1 切换线程  if (_instance == nullptr) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); // 3.2  if (_instance == nullptr) { _instance = new Singleton(); atexit(Destructor); } } return _instance; } private: static void Destructor() { if (nullptr != _instance) { delete _instance; _instance = nullptr; } } Singleton(){} //构造  Singleton(const Singleton \u0026amp;cpy){} //拷⻉构造  Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static Singleton * _instance; static std::mutex _mutex; } Singleton* Singleton::_instance = nullptr;//静态成员需要初始化 std::mutex Singleton::_mutex; //互斥锁初始化  这一版本还存在问题，首先在3.1处加锁的话锁的粒度太大，线程切换调用的开销巨大，试想即使另一个线程拿到锁已经初始化完成，此时有线程要来取得单例对象，可是锁已经被拿走，即使对象已经创建还是需要将这个线程挂起，开销巨大。\n第二个问题，如果我们将锁加到3.2位置，多线程环境下可能有线程取到空值的单例对象，这是因为有线程已经拿到锁开始创建对象，但是从底层汇编的角度来看，对象的构建分三步：1、取得内存2、构造3、赋值给指针。但是汇编器可能进行优化（reorder，指令重排，为什么会重排呢，因为指令操作重排后可能具有更好的性能，对于指令来说可能提高分支预测的成功率等等），使得对象构建的顺序由123变成132，如果13进行完毕，代表其他线程会认为对象已经构建完成，因为指针指向了一块地址，但其实这一块内存还未进行构造就是未知的，其他线程就会取到未知的数据，产生错误。\n综上，在3.1处加锁粒度太大浪费计算资源，在3.2处加锁还是无法保证线程安全。\n那我们该怎么办呢？ 就有了版本4：\n单例模式版本4： 使用编译屏障\n一般来讲屏障是汇编器带有的功能，我们也可以内联汇编来实现但是比较复杂且可读性不高。\n汇编：（定义一个显式屏障）\n#define barrier() __asm__ __volatile__(\u0026quot;\u0026quot;: : :\u0026quot;memory\u0026quot;)\r当然显式屏障的作用远不止关闭reorder，还有：\n告诉compiler内存中的值已经改变，之前对内存的缓存（缓存到寄存器）都需要抛弃，barrier()之后的内存操作需要重新从内存load，而不能使用之前寄存器缓存的值。并且可以防止compiler优化barrier()前后的内存访问顺序。\n可以看到，memory还具备刷新缓存的功能，所以不仅可以防止reorder优化，还可以防止其他指令优化，比如真值替换等。至于多核心的L1,L2缓存，MESI协议下的多核缓存一致性已经保持的很好了\nc++11 中的编译屏障：\n#include \u0026lt;mutex\u0026gt;#include \u0026lt;atomic\u0026gt;class Singleton { public: static Singleton * GetInstance() { Singleton* tmp = _instance.load(std::memory_order_relaxed); std::atomic_thread_fence(std::memory_order_acquire);//获取内存屏障  if (tmp == nullptr) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(_mutex); tmp = _instance.load(std::memory_order_relaxed); if (tmp == nullptr) { tmp = new Singleton; std::atomic_thread_fence(std::memory_order_release);//释放内存屏障  _instance.store(tmp, std::memory_order_relaxed); atexit(Destructor); } } return tmp; } private: static void Destructor() { Singleton* tmp = _instance.load(std::memory_order_relaxed); if (nullptr != tmp) { delete tmp; } } Singleton(){} Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} static std::atomic\u0026lt;Singleton*\u0026gt; _instance; static std::mutex _mutex; }; std::atomic\u0026lt;Singleton*\u0026gt; Singleton::_instance;//静态成员需要初始化 std::mutex Singleton::_mutex; //互斥锁初始化 // g++ Singleton.cpp -o singleton -std=c++11  添加内存屏障后，cpu不会reorder，因为这块内存只能在这里操作，优化器并不会进行优化，只有一个线程可以操作这一块内存，也就不存在其他线程来取用的时候取到未知值的情况了，因为其他线程都无法取用，只能等待。同时这种操作下粒度也是很小的，并不影响大部分情况下的其他线程获得未创建成功的信息。\n单例模式版本5： // c++11 magic static 特性：如果当变量在初始化的时候，并发同时进⼊声明语句，并发 线程将会阻塞等待初始化结束。 class Singleton { public: ~Singleton(){} static Singleton\u0026amp; GetInstance() { static Singleton instance; return instance; } private: Singleton(){} Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator=(const Singleton\u0026amp;) {} }; // 继承 Singleton // g++ Singleton.cpp -o singleton -std=c++11 /*该版本具备 版本5 所有优点： 1. 利⽤静态局部变量特性，延迟加载； 2. 利⽤静态局部变量特性，系统⾃动回收内存，⾃动调⽤析构函数； 3. 静态局部变量初始化时，没有 new 操作带来的cpu指令reorder操作； 4. c++11 静态局部变量初始化时，具备线程安全； */ 这一版本优点很多，但是当其他类需要用到这个单例的时候，我们想到继承，可是构造函数为private，显然无法继承，因为继承后不能使用构造函数。\n这里我们只需要在单例中添加友元类就可以。 可是扩展性极差，因为可能需要写特别多的friend，所以用版本6：\n单例模式版本6： template\u0026lt;typename T\u0026gt; class Singleton { public: static T\u0026amp; GetInstance() { static T instance; // 这⾥要初始化DesignPattern，需要调⽤DesignPattern 构造函数，同时会调⽤⽗类的构造函数。  return instance; } protected: virtual ~Singleton() {} Singleton() {} // protected修饰构造函数，才能让别⼈继承  Singleton(const Singleton\u0026amp;) {} Singleton\u0026amp; operator =(const Singleton\u0026amp;) {} }; class DesignPattern : public Singleton\u0026lt;DesignPattern\u0026gt; { friend class Singleton\u0026lt;DesignPattern\u0026gt;; // friend 能让 Singleton\u0026lt;T\u0026gt; 访问到 DesignPattern构造函数 private: DesignPattern(){} DesignPattern(const DesignPattern\u0026amp;) {} DesignPattern\u0026amp; operator=(const DesignPattern\u0026amp;) {} }; 显然，使用protected来标记构造函数就可以避免继承问题了。\n工厂方法模式 \r\n定义： 定义⼀个用于创建对象的接口，让子类决定实例化哪⼀个类。Factory Method使得⼀个类的实例化延迟到子类。\n未使用工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; // =====1 int main() { std::string choose/* = */; if (choose == \u0026#34;txt\u0026#34;) { IExport *e = new ExportTxt(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } else if (choose == \u0026#34;json\u0026#34;) { IExport *e = new ExportJson(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } else if (choose == \u0026#34;xml\u0026#34;) { IExport *e = new ExportXml(); e-\u0026gt;Export(\u0026#34;hello world\u0026#34;); } } 使用工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv  class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class IExportFactory { public: virtual IExport * NewExport(/* ... */) = 0; }; class ExportXmlFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportXml; // 可能之后有什么操作  return temp; } }; class ExportJsonFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportJson; // 可能之后有什么操作  return temp; } }; class ExportTxtFactory : public IExportFactory { public: IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportTxt; // 可能之后有什么操作  return temp; } }; class ExportData { public: ExportData(IExportFactory *factory) : _factory(factory) {} ~ExportData() { if (_factory) { delete _factory; _factory = nullptr; } } bool Export(const std::string \u0026amp;data) { IExport * e = _factory-\u0026gt;NewExport(); e-\u0026gt;Export(data); } private: IExportFactory *_factory; }; int main() { ExportData ed(new ExportTxtFactory); ed.Export(\u0026#34;hello world\u0026#34;); return 0; } 抽象工厂 \r\n定义： 提供一个接口，让该接口负责创建一系列“相关或者相互依赖的对象”，无需指定它们具体的类。\n使用抽象工厂方法模式： #include \u0026lt;string\u0026gt;// 实现导出数据的接口, 导出数据的格式包含 xml，json，文本格式txt 后面可能扩展excel格式csv class IExport { public: virtual bool Export(const std::string \u0026amp;data) = 0; virtual ~IExport(){} }; class ExportXml : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportJson : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportTxt : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class ExportCSV : public IExport { public: virtual bool Export(const std::string \u0026amp;data) { return true; } }; class IImport { public: virtual bool Import(const std::string \u0026amp;data) = 0; virtual ~IImport(){} }; class ImportXml : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportJson : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportTxt : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { return true; } }; class ImportCSV : public IImport { public: virtual bool Import(const std::string \u0026amp;data) { // ....  return true; } }; class IDataApiFactory { public: IDataApiFactory() { _export = nullptr; _import = nullptr; } virtual ~IDataApiFactory() { if (_export) { delete _export; _export = nullptr; } if (_import) { delete _import; _import = nullptr; } } bool Export(const std::string \u0026amp;data) { if (_export == nullptr) { _export = NewExport(); } return _export-\u0026gt;Export(data); } bool Import(const std::string \u0026amp;data) { if (_import == nullptr) { _import = NewImport(); } return _import-\u0026gt;Import(data); } protected: virtual IExport * NewExport(/* ... */) = 0; virtual IImport * NewImport(/* ... */) = 0; private: IExport *_export; IImport *_import; }; class XmlApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportXml; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportXml; // 可能之后有什么操作  return temp; } }; class JsonApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportJson; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportJson; // 可能之后有什么操作  return temp; } }; class TxtApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportTxt; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportTxt; // 可能之后有什么操作  return temp; } }; class CSVApiFactory : public IDataApiFactory { protected: virtual IExport * NewExport(/* ... */) { // 可能有其它操作，或者许多参数  IExport * temp = new ExportCSV; // 可能之后有什么操作  return temp; } virtual IImport * NewImport(/* ... */) { // 可能有其它操作，或者许多参数  IImport * temp = new ImportCSV; // 可能之后有什么操作  return temp; } }; int main () { IDataApiFactory *factory = new CSVApiFactory(); factory-\u0026gt;Import(\u0026#34;hello world\u0026#34;); factory-\u0026gt;Export(\u0026#34;hello world\u0026#34;); return 0; } 适配器模式 \r\n定义： 将⼀个类的接口转换成客户希望的另一个接口。Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以⼀起工作。\n使用适配器模式： #include \u0026lt;string\u0026gt;#include \u0026lt;vector\u0026gt;using namespace std; class LogSys { public: LogSys() {} void WriteLog(const vector\u0026lt;string\u0026gt; \u0026amp;) { // ... 日志id 时间戳 服务器id 具体日志内容 roleid  } vector\u0026lt;string\u0026gt;\u0026amp; ReadLog() { // ...  vector\u0026lt;string\u0026gt; data /* = ...*/; return data; } }; class DB; // 面向接口编程 而不是具体类 强依赖 耦合性高 mysql mongo  class LogSysEx : public LogSys { public: LogSysEx(DB *db) : _db(db) {} void AddLog(const vector\u0026lt;string\u0026gt; \u0026amp;data) { LogSys::WriteLog(data); /* 这里调用 _db 的方法将 data 数据存储到数据库 */ } void DelLog(const int logid) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); // 从 vector\u0026lt;string\u0026gt; 中删除 logid的日志  LogSys::WriteLog(data); // 调用 _db 的方法将 logid的日志删除  } void UpdateLog(const int logid, const string \u0026amp;udt) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); // 从 vector\u0026lt;string\u0026gt; 中更新 logid的日志 udt  LogSys::WriteLog(data); // 调用 _db 的方法将 logid的日志更改  } string\u0026amp; LocateLog(const int logid) { vector\u0026lt;string\u0026gt;\u0026amp; data = LogSys::ReadLog(); string log1 /* = from log file*/; string log2 /* = from db */; string temp = log1 + \u0026#34;;\u0026#34; + log2; return temp; } private: DB* _db; }; 代理模式 \r\n定义： 为其他对象提供一种代理以控制对这对象的访问。\n使用代理模式： class ISubject { public: virtual void Handle() = 0; virtual ~ISubject() {} }; // 该类在当前进程，也可能在其他进程当中 class RealSubject : public ISubject { public: virtual void Handle() { // 只完成功能相关的操作，不做其他模块的判断  } }; // 在当前进程当中 只会在某个模块中使用 class Proxy1 : public ISubject { public: Proxy1(ISubject *subject) : _subject(subject) {} virtual void Handle() { // 在访问 RealSubject 之前做一些处理  //if (不满足条件)  // return;  _subject-\u0026gt;Handle(); count++; // 在访问 RealSubject 之后做一些处理  } private: ISubject* _subject; static int count; }; int Proxy1::count = 0; // 在分布式系统当中 skynet actor class Proxy2 : public ISubject { public: virtual void Handle() { // 在访问 RealSubject 之前做一些处理  // 发送到数据到远端 网络处理 同步非阻塞 ntyco c协程  //IResult * val = rpc-\u0026gt;call(\u0026#34;RealSubject\u0026#34;, \u0026#34;Handle\u0026#34;);  // 在访问 RealSubject 之后做一些处理  } private: /*void callback(IResult * val) { // 在访问 RealSubject 之后做一些处理 }*/ }; 策略模式 \r\n定义： 定义一系列算法，把它们一个个封装起来，并且使它们可互相替换。该模式使得算法可独立于使用它的客户程序而变化。\n未使用策略模式： enum VacationEnum { VAC_Spring, VAC_QiXi, VAC_Wuyi, VAC_GuoQing, //VAC_ShengDan, }; // 稳定的 变化的 class Promotion { VacationEnum vac; public: double CalcPromotion(){ if (vac == VAC_Spring){ // 春节  } else if (vac == VAC_QiXi){ // 七夕  } else if (vac == VAC_Wuyi){ // 五一  } else if (vac == VAC_GuoQing){ // 国庆 \t} } }; 使用策略模式： class Context { }; class ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx) = 0; virtual ~ProStategy(); }; // cpp class VAC_Spring : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_QiXi : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; class VAC_QiXi1 : public VAC_QiXi { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_Wuyi : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // cpp class VAC_GuoQing : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; class VAC_Shengdan : public ProStategy { public: virtual double CalcPro(const Context \u0026amp;ctx){} }; // 稳定的 变化的 class Promotion { public: Promotion(ProStategy *sss) : s(sss){} ~Promotion(){} double CalcPromotion(const Context \u0026amp;ctx){ return s-\u0026gt;CalcPro(ctx); } private: ProStategy *s; }; int main () { Context ctx; ProStategy *s = new VAC_QiXi1(); Promotion *p = new Promotion(s); p-\u0026gt;CalcPromotion(ctx); return 0; } ","date":"2020-03-24T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/462_hu609bcc78771f538873d0b8560ebecb73_10199040_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":"连接池实现 mysql连接池 头文件： #ifndef DBPOOL_H_ #define DBPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;list\u0026gt;#include \u0026lt;mutex\u0026gt;#include \u0026lt;condition_variable\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;stdint.h\u0026gt; #include \u0026lt;mysql.h\u0026gt; #define MAX_ESCAPE_STRING_LEN\t10240  using namespace std; // 返回结果 select的时候用 class CResultSet { public: CResultSet(MYSQL_RES* res); virtual ~CResultSet(); bool Next(); int GetInt(const char* key); char* GetString(const char* key); private: int _GetIndex(const char* key); MYSQL_RES* m_res; MYSQL_ROW\tm_row; map\u0026lt;string, int\u0026gt;\tm_key_map; }; // 插入数据用 class CPrepareStatement { public: CPrepareStatement(); virtual ~CPrepareStatement(); bool Init(MYSQL* mysql, string\u0026amp; sql); void SetParam(uint32_t index, int\u0026amp; value); void SetParam(uint32_t index, uint32_t\u0026amp; value); void SetParam(uint32_t index, string\u0026amp; value); void SetParam(uint32_t index, const string\u0026amp; value); bool ExecuteUpdate(); uint32_t GetInsertId(); private: MYSQL_STMT*\tm_stmt; MYSQL_BIND*\tm_param_bind; uint32_t\tm_param_cnt; }; class CDBPool; class CDBConn { public: CDBConn(CDBPool* pDBPool); virtual ~CDBConn(); int Init(); // 创建表 \tbool ExecuteCreate(const char* sql_query); // 删除表 \tbool ExecuteDrop(const char* sql_query); // 查询 \tCResultSet* ExecuteQuery(const char* sql_query); /** * 执行DB更新，修改 * * @param sql_query sql * @param care_affected_rows 是否在意影响的行数，false:不在意；true:在意 * * @return 成功返回true 失败返回false */ bool ExecuteUpdate(const char* sql_query, bool care_affected_rows = true); uint32_t GetInsertId(); // 开启事务 \tbool StartTransaction(); // 提交事务 \tbool Commit(); // 回滚事务 \tbool Rollback(); // 获取连接池名 \tconst char* GetPoolName(); MYSQL* GetMysql() { return m_mysql; } private: CDBPool* m_pDBPool;\t// to get MySQL server information \tMYSQL* m_mysql;\t// 对应一个连接 \tchar\tm_escape_string[MAX_ESCAPE_STRING_LEN + 1]; }; class CDBPool {\t// 只是负责管理连接CDBConn，真正干活的是CDBConn public: CDBPool() {} CDBPool(const char* pool_name, const char* db_server_ip, uint16_t db_server_port, const char* username, const char* password, const char* db_name, int max_conn_cnt); virtual ~CDBPool(); int Init();\t// 连接数据库，创建连接 \tCDBConn* GetDBConn(const int timeout_ms = -1);\t// 获取连接资源 \tvoid RelDBConn(CDBConn* pConn);\t// 归还连接资源  const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetDBServerIP() { return m_db_server_ip.c_str(); } uint16_t GetDBServerPort() { return m_db_server_port; } const char* GetUsername() { return m_username.c_str(); } const char* GetPasswrod() { return m_password.c_str(); } const char* GetDBName() { return m_db_name.c_str(); } private: string m_pool_name;\t// 连接池名称 \tstring m_db_server_ip;\t// 数据库ip \tuint16_t\tm_db_server_port; // 数据库端口 \tstring m_username; // 用户名 \tstring m_password;\t// 用户密码 \tstring m_db_name;\t// db名称 \tint\tm_db_cur_conn_cnt;\t// 当前启用的连接数量 \tint m_db_max_conn_cnt;\t// 最大连接数量 \tlist\u0026lt;CDBConn*\u0026gt;\tm_free_list;\t// 空闲的连接  list\u0026lt;CDBConn*\u0026gt;\tm_used_list;\t// 记录已经被请求的连接 \tstd::mutex m_mutex; std::condition_variable m_cond_var; bool m_abort_request = false; // CThreadNotify\tm_free_notify;\t// 信号量 }; #endif /* DBPOOL_H_ */ }; 实现：  #include \u0026#34;DBPool.h\u0026#34;#include \u0026lt;string.h\u0026gt; #define log_error printf #define log_warn printf #define log_info printf #define MIN_DB_CONN_CNT 2 #define MAX_DB_CONN_FAIL_NUM 10  CResultSet::CResultSet(MYSQL_RES *res) { m_res = res; // map table field key to index in the result array \tint num_fields = mysql_num_fields(m_res); MYSQL_FIELD *fields = mysql_fetch_fields(m_res); for (int i = 0; i \u0026lt; num_fields; i++) { // 多行 \tm_key_map.insert(make_pair(fields[i].name, i)); } } CResultSet::~CResultSet() { if (m_res) { mysql_free_result(m_res); m_res = NULL; } } bool CResultSet::Next() { m_row = mysql_fetch_row(m_res); if (m_row) { return true; } else { return false; } } int CResultSet::_GetIndex(const char *key) { map\u0026lt;string, int\u0026gt;::iterator it = m_key_map.find(key); if (it == m_key_map.end()) { return -1; } else { return it-\u0026gt;second; } } int CResultSet::GetInt(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return 0; } else { return atoi(m_row[idx]); // 有索引 \t} } char *CResultSet::GetString(const char *key) { int idx = _GetIndex(key); if (idx == -1) { return NULL; } else { return m_row[idx];\t// 列 \t} } ///////////////////////////////////////// CPrepareStatement::CPrepareStatement() { m_stmt = NULL; m_param_bind = NULL; m_param_cnt = 0; } CPrepareStatement::~CPrepareStatement() { if (m_stmt) { mysql_stmt_close(m_stmt); m_stmt = NULL; } if (m_param_bind) { delete[] m_param_bind; m_param_bind = NULL; } } bool CPrepareStatement::Init(MYSQL *mysql, string \u0026amp;sql) { mysql_ping(mysql);\t// 当mysql连接丢失的时候，使用mysql_ping能够自动重连数据库  //g_master_conn_fail_num ++; \tm_stmt = mysql_stmt_init(mysql); if (!m_stmt) { log_error(\u0026#34;mysql_stmt_init failed\\n\u0026#34;); return false; } if (mysql_stmt_prepare(m_stmt, sql.c_str(), sql.size())) { log_error(\u0026#34;mysql_stmt_prepare failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } m_param_cnt = mysql_stmt_param_count(m_stmt); if (m_param_cnt \u0026gt; 0) { m_param_bind = new MYSQL_BIND[m_param_cnt]; if (!m_param_bind) { log_error(\u0026#34;new failed\\n\u0026#34;); return false; } memset(m_param_bind, 0, sizeof(MYSQL_BIND) * m_param_cnt); } return true; } void CPrepareStatement::SetParam(uint32_t index, int \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, uint32_t \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_LONG; m_param_bind[index].buffer = \u0026amp;value; } void CPrepareStatement::SetParam(uint32_t index, string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } void CPrepareStatement::SetParam(uint32_t index, const string \u0026amp;value) { if (index \u0026gt;= m_param_cnt) { log_error(\u0026#34;index too large: %d\\n\u0026#34;, index); return; } m_param_bind[index].buffer_type = MYSQL_TYPE_STRING; m_param_bind[index].buffer = (char *)value.c_str(); m_param_bind[index].buffer_length = value.size(); } bool CPrepareStatement::ExecuteUpdate() { if (!m_stmt) { log_error(\u0026#34;no m_stmt\\n\u0026#34;); return false; } if (mysql_stmt_bind_param(m_stmt, m_param_bind)) { log_error(\u0026#34;mysql_stmt_bind_param failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_execute(m_stmt)) { log_error(\u0026#34;mysql_stmt_execute failed: %s\\n\u0026#34;, mysql_stmt_error(m_stmt)); return false; } if (mysql_stmt_affected_rows(m_stmt) == 0) { log_error(\u0026#34;ExecuteUpdate have no effect\\n\u0026#34;); return false; } return true; } uint32_t CPrepareStatement::GetInsertId() { return mysql_stmt_insert_id(m_stmt); } ///////////////////// CDBConn::CDBConn(CDBPool *pPool) { m_pDBPool = pPool; m_mysql = NULL; } CDBConn::~CDBConn() { if (m_mysql) { mysql_close(m_mysql); } } int CDBConn::Init() { m_mysql = mysql_init(NULL);\t// mysql_标准的mysql c client对应的api \tif (!m_mysql) { log_error(\u0026#34;mysql_init failed\\n\u0026#34;); return 1; } my_bool reconnect = true; mysql_options(m_mysql, MYSQL_OPT_RECONNECT, \u0026amp;reconnect);\t// 配合mysql_ping实现自动重连 \tmysql_options(m_mysql, MYSQL_SET_CHARSET_NAME, \u0026#34;utf8mb4\u0026#34;);\t// utf8mb4和utf8区别  // ip 端口 用户名 密码 数据库名 \tif (!mysql_real_connect(m_mysql, m_pDBPool-\u0026gt;GetDBServerIP(), m_pDBPool-\u0026gt;GetUsername(), m_pDBPool-\u0026gt;GetPasswrod(), m_pDBPool-\u0026gt;GetDBName(), m_pDBPool-\u0026gt;GetDBServerPort(), NULL, 0)) { log_error(\u0026#34;mysql_real_connect failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return 2; } return 0; } const char *CDBConn::GetPoolName() { return m_pDBPool-\u0026gt;GetPoolName(); } bool CDBConn::ExecuteCreate(const char *sql_query) { mysql_ping(m_mysql); // mysql_real_query 实际就是执行了SQL \tif (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::ExecuteDrop(const char *sql_query) { mysql_ping(m_mysql);\t// 如果端开了，能够自动重连  if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } CResultSet *CDBConn::ExecuteQuery(const char *sql_query) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); return NULL; } // 返回结果 \tMYSQL_RES *res = mysql_store_result(m_mysql);\t// 返回结果 \tif (!res) { log_error(\u0026#34;mysql_store_result failed: %s\\n\u0026#34;, mysql_error(m_mysql)); return NULL; } CResultSet *result_set = new CResultSet(res);\t// 存储到CResultSet \treturn result_set; } /* 1.执行成功，则返回受影响的行的数目，如果最近一次查询失败的话，函数返回 -1 2.对于delete,将返回实际删除的行数. 3.对于update,如果更新的列值原值和新值一样,如update tables set col1=10 where id=1; id=1该条记录原值就是10的话,则返回0。 mysql_affected_rows返回的是实际更新的行数,而不是匹配到的行数。 */ bool CDBConn::ExecuteUpdate(const char *sql_query, bool care_affected_rows) { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, sql_query, strlen(sql_query))) { log_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\u0026#34;, mysql_error(m_mysql), sql_query); //g_master_conn_fail_num ++; \treturn false; } if (mysql_affected_rows(m_mysql) \u0026gt; 0) { return true; } else { // 影响的行数为0时 \tif (care_affected_rows) { // 如果在意影响的行数时, 返回false, 否则返回true \tlog_error(\u0026#34;mysql_real_query failed: %s, sql: %s\\n\\n\u0026#34;, mysql_error(m_mysql), sql_query); return false; } else { log_warn(\u0026#34;affected_rows=0, sql: %s\\n\\n\u0026#34;, sql_query); return true; } } } bool CDBConn::StartTransaction() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;start transaction\\n\u0026#34;, 17)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: start transaction\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Rollback() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;rollback\\n\u0026#34;, 8)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: rollback\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } bool CDBConn::Commit() { mysql_ping(m_mysql); if (mysql_real_query(m_mysql, \u0026#34;commit\\n\u0026#34;, 6)) { log_error(\u0026#34;mysql_real_query failed: %s, sql: commit\\n\u0026#34;, mysql_error(m_mysql)); return false; } return true; } uint32_t CDBConn::GetInsertId() { return (uint32_t)mysql_insert_id(m_mysql); } //////////////// CDBPool::CDBPool(const char *pool_name, const char *db_server_ip, uint16_t db_server_port, const char *username, const char *password, const char *db_name, int max_conn_cnt) { m_pool_name = pool_name; m_db_server_ip = db_server_ip; m_db_server_port = db_server_port; m_username = username; m_password = password; m_db_name = db_name; m_db_max_conn_cnt = max_conn_cnt;\t// \tm_db_cur_conn_cnt = MIN_DB_CONN_CNT; // 最小连接数量 } // 释放连接池 CDBPool::~CDBPool() { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); m_abort_request = true; m_cond_var.notify_all();\t// 通知所有在等待的  for (list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CDBConn *pConn = *it; delete pConn; } m_free_list.clear(); } int CDBPool::Init() { // 创建固定最小的连接数量 \tfor (int i = 0; i \u0026lt; m_db_cur_conn_cnt; i++) { CDBConn *pDBConn = new CDBConn(this); int ret = pDBConn-\u0026gt;Init(); if (ret) { delete pDBConn; return ret; } m_free_list.push_back(pDBConn); } // log_error(\u0026#34;db pool: %s, size: %d\\n\u0026#34;, m_pool_name.c_str(), (int)m_free_list.size()); \treturn 0; } /* *TODO: 增加保护机制，把分配的连接加入另一个队列，这样获取连接时，如果没有空闲连接， *TODO: 检查已经分配的连接多久没有返回，如果超过一定时间，则自动收回连接，放在用户忘了调用释放连接的接口 * timeout_ms默认为-1死等 * timeout_ms \u0026gt;=0 则为等待的时间 */ int wait_cout = 0; CDBConn *CDBPool::GetDBConn(const int timeout_ms) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(m_mutex); if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } if (m_free_list.empty())\t// 当没有连接可以用时 \t{ // 第一步先检测 当前连接数量是否达到最大的连接数量 \tif (m_db_cur_conn_cnt \u0026gt;= m_db_max_conn_cnt) { // 如果已经到达了，看看是否需要超时等待 \tif(timeout_ms \u0026lt; 0)\t// 死等，直到有连接可以用 或者 连接池要退出 \t{ log_info(\u0026#34;wait ms:%d\\n\u0026#34;, timeout_ms); m_cond_var.wait(lock, [this] { // log_info(\u0026#34;wait:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \t// 当前连接数量小于最大连接数量 或者请求释放连接池时退出 \treturn (!m_free_list.empty()) | m_abort_request; }); } else { // return如果返回 false，继续wait(或者超时), 如果返回true退出wait \t// 1.m_free_list不为空 \t// 2.超时退出 \t// 3. m_abort_request被置为true，要释放整个连接池 \tm_cond_var.wait_for(lock, std::chrono::milliseconds(timeout_ms), [this] { // log_info(\u0026#34;wait_for:%d, size:%d\\n\u0026#34;, wait_cout++, m_free_list.size()); \treturn (!m_free_list.empty()) | m_abort_request; }); // 带超时功能时还要判断是否为空 \tif(m_free_list.empty()) // 如果连接池还是没有空闲则退出 \t{ return NULL; } } if(m_abort_request) { log_warn(\u0026#34;have aboort\\n\u0026#34;); return NULL; } } else // 还没有到最大连接则创建连接 \t{ CDBConn *pDBConn = new CDBConn(this);\t//新建连接 \tint ret = pDBConn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init DBConnecton failed\\n\\n\u0026#34;); delete pDBConn; return NULL; } else { m_free_list.push_back(pDBConn); m_db_cur_conn_cnt++; log_info(\u0026#34;new db connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_db_cur_conn_cnt); } } } CDBConn *pConn = m_free_list.front();\t// 获取连接 \tm_free_list.pop_front();\t// STL 吐出连接，从空闲队列删除 \t// pConn-\u0026gt;setCurrentTime(); // 伪代码 \tm_used_list.push_back(pConn);\t//  return pConn; } void CDBPool::RelDBConn(CDBConn *pConn) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(m_mutex); list\u0026lt;CDBConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++)\t// 避免重复归还 \t{ if (*it == pConn)\t{ break; } } if (it == m_free_list.end()) { m_used_list.remove(pConn); m_free_list.push_back(pConn); m_cond_var.notify_one();\t// 通知取队列 \t} else { log_error(\u0026#34;RelDBConn failed\\n\u0026#34;); } } // 遍历检测是否超时未归还 // pConn-\u0026gt;isTimeout(); // 当前时间 - 被请求的时间 // 强制回收 从m_used_list 放回 m_free_list redis连接池 =============\n头文件  /* * @Author: your name * @Date: 2019-12-07 10:54:57 * @LastEditTime : 2020-01-10 16:35:13 * @LastEditors : Please set LastEditors * @Description: In User Settings Edit * @FilePath: \\src\\cache_pool\\CachePool.h */ #ifndef CACHEPOOL_H_ #define CACHEPOOL_H_  #include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;list\u0026gt; #include \u0026#34;Thread.h\u0026#34; #include \u0026#34;hiredis.h\u0026#34; using std::string; using std::list; using std::map; using std::vector; class CachePool; class CacheConn { public: CacheConn(const char* server_ip, int server_port, int db_index, const char* password, const char *pool_name =\u0026#34;\u0026#34;); CacheConn(CachePool* pCachePool);\tvirtual ~CacheConn(); int Init(); void DeInit(); const char* GetPoolName(); // 通用操作  // 判断一个key是否存在  bool isExists(string \u0026amp;key); // 删除某个key  long del(string \u0026amp;key); // ------------------- 字符串相关 ------------------- \tstring get(string key); string set(string key, string\u0026amp; value); string setex(string key, int timeout, string value); // string mset(string key, map);  //批量获取  bool mget(const vector\u0026lt;string\u0026gt;\u0026amp; keys, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); //原子加减1  long incr(string key); long decr(string key); // ---------------- 哈希相关 ------------------------ \tlong hdel(string key, string field); string hget(string key, string field); bool hgetAll(string key, map\u0026lt;string, string\u0026gt;\u0026amp; ret_value); long hset(string key, string field, string value); long hincrBy(string key, string field, long value); long incrBy(string key, long value); string hmset(string key, map\u0026lt;string, string\u0026gt;\u0026amp; hash); bool hmget(string key, list\u0026lt;string\u0026gt;\u0026amp; fields, list\u0026lt;string\u0026gt;\u0026amp; ret_value); // ------------ 链表相关 ------------ \tlong lpush(string key, string value); long rpush(string key, string value); long llen(string key); bool lrange(string key, long start, long end, list\u0026lt;string\u0026gt;\u0026amp; ret_value); bool flushdb(); private: CachePool* m_pCachePool; redisContext* m_pContext; uint64_t\tm_last_connect_time; uint16_t m_server_port; string m_server_ip; string m_password; uint16_t m_db_index; string m_pool_name; }; class CachePool { public: // db_index和mysql不同的地方 \tCachePool(const char* pool_name, const char* server_ip, int server_port, int db_index, const char *password, int max_conn_cnt); virtual ~CachePool(); int Init(); // 获取空闲的连接资源 \tCacheConn* GetCacheConn(); // Pool回收连接资源 \tvoid RelCacheConn(CacheConn* pCacheConn); const char* GetPoolName() { return m_pool_name.c_str(); } const char* GetServerIP() { return m_server_ip.c_str(); } const char* GetPassword() { return m_password.c_str(); } int GetServerPort() { return m_server_port; } int GetDBIndex() { return m_db_index; } private: string m_pool_name; string\tm_server_ip; string m_password; int\tm_server_port; int\tm_db_index;\t// mysql 数据库名字， redis db index  int\tm_cur_conn_cnt; int m_max_conn_cnt; list\u0026lt;CacheConn*\u0026gt;\tm_free_list; CThreadNotify\tm_free_notify; }; #endif /* CACHEPOOL_H_ */ 实现  #include \u0026#34;CachePool.h\u0026#34; #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;Thread.h\u0026#34; #define log_error printf #define log_info printf  #define MIN_CACHE_CONN_CNT 2 #define MAX_CACHE_CONN_FAIL_NUM 10  CacheConn::CacheConn(const char *server_ip, int server_port, int db_index, const char *password, const char *pool_name) { m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_pool_name = pool_name; m_pContext = NULL; m_last_connect_time = 0; } CacheConn::CacheConn(CachePool *pCachePool) { m_pCachePool = pCachePool; if (pCachePool) { m_server_ip = pCachePool-\u0026gt;GetServerIP(); m_server_port = pCachePool-\u0026gt;GetServerPort(); m_db_index = pCachePool-\u0026gt;GetDBIndex(); m_password = pCachePool-\u0026gt;GetPassword(); m_pool_name = pCachePool-\u0026gt;GetPoolName(); } else { log_error(\u0026#34;pCachePool is NULL\\n\u0026#34;); } m_pContext = NULL; m_last_connect_time = 0; } CacheConn::~CacheConn() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } /* * redis初始化连接和重连操作，类似mysql_ping() */ int CacheConn::Init() { if (m_pContext)\t// 非空，连接是正常的 \t{ return 0; } // 1s 尝试重连一次 \tuint64_t cur_time = (uint64_t)time(NULL); if (cur_time \u0026lt; m_last_connect_time + 1) // 重连尝试 间隔1秒 \t{ printf(\u0026#34;cur_time:%lu, m_last_connect_time:%lu\\n\u0026#34;, cur_time, m_last_connect_time); return 1; } // printf(\u0026#34;m_last_connect_time = cur_time\\n\u0026#34;); \tm_last_connect_time = cur_time; // 1000ms超时 \tstruct timeval timeout = {0, 1000000}; // 建立连接后使用 redisContext 来保存连接状态。 \t// redisContext 在每次操作后会修改其中的 err 和 errstr 字段来表示发生的错误码（大于0）和对应的描述。 \tm_pContext = redisConnectWithTimeout(m_server_ip.c_str(), m_server_port, timeout); if (!m_pContext || m_pContext-\u0026gt;err) { if (m_pContext) { log_error(\u0026#34;redisConnect failed: %s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; } else { log_error(\u0026#34;redisConnect failed\\n\u0026#34;); } return 1; } redisReply *reply; // 验证 \tif (!m_password.empty()) { reply = (redisReply *)redisCommand(m_pContext, \u0026#34;AUTH %s\u0026#34;, m_password.c_str()); if (!reply || reply-\u0026gt;type == REDIS_REPLY_ERROR) { log_error(\u0026#34;Authentication failure:%p\\n\u0026#34;, reply); if (reply) freeReplyObject(reply); return -1; } else { // log_info(\u0026#34;Authentication success\\n\u0026#34;); \t} freeReplyObject(reply); } reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SELECT %d\u0026#34;, 0); if (reply \u0026amp;\u0026amp; (reply-\u0026gt;type == REDIS_REPLY_STATUS) \u0026amp;\u0026amp; (strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0)) { freeReplyObject(reply); return 0; } else { if (reply) log_error(\u0026#34;select cache db failed:%s\\n\u0026#34;, reply-\u0026gt;str); return 2; } } void CacheConn::DeInit() { if (m_pContext) { redisFree(m_pContext); m_pContext = NULL; } } const char *CacheConn::GetPoolName() { return m_pool_name.c_str(); } string CacheConn::get(string key) { string value; if (Init()) { return value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;GET %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return value; } string CacheConn::set(string key, string \u0026amp;value) { string ret_value; if (Init()) { return ret_value; } // 返回的结果存放在redisReply \tredisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SET %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); // 释放资源 \treturn ret_value; } string CacheConn::setex(string key, int timeout, string value) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;SETEX %s %d %s\u0026#34;, key.c_str(), timeout, value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); freeReplyObject(reply); return ret_value; } bool CacheConn::mget(const vector\u0026lt;string\u0026gt; \u0026amp;keys, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } if (keys.empty()) { return false; } string strKey; bool bFirst = true; for (vector\u0026lt;string\u0026gt;::const_iterator it = keys.begin(); it != keys.end(); ++it) { if (bFirst) { bFirst = false; strKey = *it; } else { strKey += \u0026#34; \u0026#34; + *it; } } if (strKey.empty()) { return false; } strKey = \u0026#34;MGET \u0026#34; + strKey; redisReply *reply = (redisReply *)redisCommand(m_pContext, strKey.c_str()); if (!reply) { log_info(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; ++i) { redisReply *child_reply = reply-\u0026gt;element[i]; if (child_reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value[keys[i]] = child_reply-\u0026gt;str; } } } freeReplyObject(reply); return true; } bool CacheConn::isExists(string \u0026amp;key) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;EXISTS %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); if (0 == ret_value) { return false; } else { return true; } } long CacheConn::del(string \u0026amp;key) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DEL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hdel(string key, string field) { if (Init()) { return 0; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HDEL %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return 0; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hget(string key, string field) { string ret_value; if (Init()) { return ret_value; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGET %s %s\u0026#34;, key.c_str(), field.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return ret_value; } if (reply-\u0026gt;type == REDIS_REPLY_STRING) { ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); } freeReplyObject(reply); return ret_value; } bool CacheConn::hgetAll(string key, map\u0026lt;string, string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HGETALL %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if ((reply-\u0026gt;type == REDIS_REPLY_ARRAY) \u0026amp;\u0026amp; (reply-\u0026gt;elements % 2 == 0)) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i += 2) { redisReply *field_reply = reply-\u0026gt;element[i]; redisReply *value_reply = reply-\u0026gt;element[i + 1]; string field(field_reply-\u0026gt;str, field_reply-\u0026gt;len); string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.insert(make_pair(field, value)); } } freeReplyObject(reply); return true; } long CacheConn::hset(string key, string field, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HSET %s %s %s\u0026#34;, key.c_str(), field.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::hincrBy(string key, string field, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;HINCRBY %s %s %ld\u0026#34;, key.c_str(), field.c_str(), value); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::incrBy(string key, long value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCRBY %s %ld\u0026#34;, key.c_str(), value); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } string CacheConn::hmset(string key, map\u0026lt;string, string\u0026gt; \u0026amp;hash) { string ret_value; if (Init()) { return ret_value; } int argc = hash.size() * 2 + 2; const char **argv = new const char *[argc]; if (!argv) { return ret_value; } argv[0] = \u0026#34;HMSET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (map\u0026lt;string, string\u0026gt;::iterator it = hash.begin(); it != hash.end(); it++) { argv[i++] = it-\u0026gt;first.c_str(); argv[i++] = it-\u0026gt;second.c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return ret_value; } ret_value.append(reply-\u0026gt;str, reply-\u0026gt;len); delete[] argv; freeReplyObject(reply); return ret_value; } bool CacheConn::hmget(string key, list\u0026lt;string\u0026gt; \u0026amp;fields, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } int argc = fields.size() + 2; const char **argv = new const char *[argc]; if (!argv) { return false; } argv[0] = \u0026#34;HMGET\u0026#34;; argv[1] = key.c_str(); int i = 2; for (list\u0026lt;string\u0026gt;::iterator it = fields.begin(); it != fields.end(); it++) { argv[i++] = it-\u0026gt;c_str(); } redisReply *reply = (redisReply *)redisCommandArgv(m_pContext, argc, (const char **)argv, NULL); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); delete[] argv; redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } delete[] argv; freeReplyObject(reply); return true; } long CacheConn::incr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;INCR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::decr(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;DECR %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redis Command failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::lpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::rpush(string key, string value) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;RPUSH %s %s\u0026#34;, key.c_str(), value.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } long CacheConn::llen(string key) { if (Init()) { return -1; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LLEN %s\u0026#34;, key.c_str()); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return -1; } long ret_value = reply-\u0026gt;integer; freeReplyObject(reply); return ret_value; } bool CacheConn::lrange(string key, long start, long end, list\u0026lt;string\u0026gt; \u0026amp;ret_value) { if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;LRANGE %s %d %d\u0026#34;, key.c_str(), start, end); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_ARRAY) { for (size_t i = 0; i \u0026lt; reply-\u0026gt;elements; i++) { redisReply *value_reply = reply-\u0026gt;element[i]; string value(value_reply-\u0026gt;str, value_reply-\u0026gt;len); ret_value.push_back(value); } } freeReplyObject(reply); return true; } bool CacheConn::flushdb() { bool ret = false; if (Init()) { return false; } redisReply *reply = (redisReply *)redisCommand(m_pContext, \u0026#34;FLUSHDB\u0026#34;); if (!reply) { log_error(\u0026#34;redisCommand failed:%s\\n\u0026#34;, m_pContext-\u0026gt;errstr); redisFree(m_pContext); m_pContext = NULL; return false; } if (reply-\u0026gt;type == REDIS_REPLY_STRING \u0026amp;\u0026amp; strncmp(reply-\u0026gt;str, \u0026#34;OK\u0026#34;, 2) == 0) { ret = true; } freeReplyObject(reply); return ret; } /////////////// CachePool::CachePool(const char *pool_name, const char *server_ip, int server_port, int db_index, const char *password, int max_conn_cnt) { m_pool_name = pool_name; m_server_ip = server_ip; m_server_port = server_port; m_db_index = db_index; m_password = password; m_max_conn_cnt = max_conn_cnt; m_cur_conn_cnt = MIN_CACHE_CONN_CNT; } CachePool::~CachePool() { m_free_notify.Lock(); for (list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); it != m_free_list.end(); it++) { CacheConn *pConn = *it; delete pConn; } m_free_list.clear(); m_cur_conn_cnt = 0; m_free_notify.Unlock(); } int CachePool::Init() { for (int i = 0; i \u0026lt; m_cur_conn_cnt; i++) { CacheConn *pConn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); if (pConn-\u0026gt;Init()) { delete pConn; return 1; } m_free_list.push_back(pConn); } log_info(\u0026#34;cache pool: %s, list size: %lu\\n\u0026#34;, m_pool_name.c_str(), m_free_list.size()); return 0; } CacheConn *CachePool::GetCacheConn() { m_free_notify.Lock(); while (m_free_list.empty()) { if (m_cur_conn_cnt \u0026gt;= m_max_conn_cnt) { m_free_notify.Wait(); } else { CacheConn *p_cache_conn = new CacheConn(m_server_ip.c_str(), m_server_port, m_db_index, m_password.c_str(), m_pool_name.c_str()); int ret = p_cache_conn-\u0026gt;Init(); if (ret) { log_error(\u0026#34;Init CacheConn failed\\n\u0026#34;); delete p_cache_conn; m_free_notify.Unlock(); return NULL; } else { m_free_list.push_back(p_cache_conn); m_cur_conn_cnt++; log_info(\u0026#34;new cache connection: %s, conn_cnt: %d\\n\u0026#34;, m_pool_name.c_str(), m_cur_conn_cnt); } } } CacheConn *pConn = m_free_list.front(); m_free_list.pop_front(); m_free_notify.Unlock(); return pConn; } void CachePool::RelCacheConn(CacheConn *p_cache_conn) { m_free_notify.Lock(); list\u0026lt;CacheConn *\u0026gt;::iterator it = m_free_list.begin(); for (; it != m_free_list.end(); it++) { if (*it == p_cache_conn) { break; } } if (it == m_free_list.end()) { m_free_list.push_back(p_cache_conn); } m_free_notify.Signal(); m_free_notify.Unlock(); } ","date":"2020-03-21T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/245_hubfe96db144675e6281ae6d025e60e504_7615599_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"连接池"},{"content":"内存池实现（注释详细） #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define MP_ALIGNMENT 32 //对齐信息 #define MP_PAGE_SIZE\t4096 //单次分配大块大小 #define MP_MAX_ALLOC_FROM_POOL\t(MP_PAGE_SIZE-1)  #define mp_align(n, alignment) (((n)+(alignment-1)) \u0026amp; ~(alignment-1)) #define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) \u0026amp; ~(alignment-1))  struct mp_large_s { struct mp_large_s *next; void *alloc; }; // 当单次分配超过pagesize时就需要一次分配然后归入large的一个链表中保存  struct mp_node_s { unsigned char *last; unsigned char *end; struct mp_node_s *next; size_t failed; };// 页，用于小块的分配,last指向页内使用到的位置  struct mp_pool_s { size_t max; struct mp_node_s *current; struct mp_large_s *large; struct mp_node_s head[0]; }; //内存池  struct mp_pool_s *mp_create_pool(size_t size); void mp_destory_pool(struct mp_pool_s *pool); void *mp_alloc(struct mp_pool_s *pool, size_t size); void *mp_nalloc(struct mp_pool_s *pool, size_t size); void *mp_calloc(struct mp_pool_s *pool, size_t size); void mp_free(struct mp_pool_s *pool, void *p); //首先需要明确，在分配的时候需要将所有的数据结构都存在我们管理的内存池中 //比如struct mp_pool_s *pool这个内存池本身也需要受我们管理 struct mp_pool_s *mp_create_pool(size_t size) { struct mp_pool_s *p; int ret = posix_memalign((void **)\u0026amp;p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s)); //posix_memalign 分配足够的内存，size（page_size：4096） 加上内存池本身和小块结构本身 \tif (ret) { return NULL; } p-\u0026gt;max = (size \u0026lt; MP_MAX_ALLOC_FROM_POOL) ? size : MP_MAX_ALLOC_FROM_POOL; //内存池单块大小受我们定义的pagesize限制 \tp-\u0026gt;current = p-\u0026gt;head;// 初始化时还有没分配数内存，所以head就是current \tp-\u0026gt;large = NULL;// 还没有分配large块  p-\u0026gt;head-\u0026gt;last = (unsigned char *)p + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s); //最后一个分配的小块 \tp-\u0026gt;head-\u0026gt;end = p-\u0026gt;head-\u0026gt;last + size;//这次分配的页面的末尾  p-\u0026gt;head-\u0026gt;failed = 0; return p; } void mp_destory_pool(struct mp_pool_s *pool) { //释放内存池  struct mp_node_s *h, *n; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { //循环释放分配的large块 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } h = pool-\u0026gt;head-\u0026gt;next; while (h) { //循环释放分配的页面 \tn = h-\u0026gt;next; free(h); h = n; } free(pool);//释放内存池本身  } void mp_reset_pool(struct mp_pool_s *pool) { //重置内存池  struct mp_node_s *h; struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) {//重置的时候需要释放大块内存 \tif (l-\u0026gt;alloc) { free(l-\u0026gt;alloc); } } pool-\u0026gt;large = NULL; for (h = pool-\u0026gt;head; h; h = h-\u0026gt;next) {// 但针对页面只需要将页面内的小块内存指针退回起始位置就可以，不需要将已经分配的页面还给操作系统 \th-\u0026gt;last = (unsigned char *)h + sizeof(struct mp_node_s); } } static void *mp_alloc_block(struct mp_pool_s *pool, size_t size) { // 开辟新的页面并分配内存  unsigned char *m; struct mp_node_s *h = pool-\u0026gt;head; //先拿到当前页面的head指针 \tsize_t psize = (size_t)(h-\u0026gt;end - (unsigned char *)h); //拿到页面总大小 \tint ret = posix_memalign((void **)\u0026amp;m, MP_ALIGNMENT, psize);//分配新的页面 \tif (ret) return NULL; struct mp_node_s *p, *new_node, *current; new_node = (struct mp_node_s*)m; //初始化新的页面  new_node-\u0026gt;end = m + psize; new_node-\u0026gt;next = NULL; new_node-\u0026gt;failed = 0; m += sizeof(struct mp_node_s); m = mp_align_ptr(m, MP_ALIGNMENT); new_node-\u0026gt;last = m + size; //把需要的内存大小从新分配的页面上取走  current = pool-\u0026gt;current; //关键点：旨在减少页面末尾的内存碎片，nginx使用的方式  for (p = current; p-\u0026gt;next; p = p-\u0026gt;next) { //每一个页面都有一个自己的failed关键字，用于表明其页面末尾提供给新需求时失败的次数，失败次数大于4就将内存池的current指针换到下一个页面 \t// 失败次数少于4那么就不变内存池的current指针，这样在下一个需求到来时还是从当前分配失败的(页面末尾内存不够用)这一页面末尾开始查找，有利于减少末尾内存碎片，4这个值的得出应该是nginx的实验 \tif (p-\u0026gt;failed++ \u0026gt; 4) { current = p-\u0026gt;next; } } p-\u0026gt;next = new_node; pool-\u0026gt;current = current ? current : new_node; //这里可以看出，如果刚好所有之前创建的页面都失败大于4次，那么将当前内存池首选页面变为刚新建的页面即可  return m; } static void *mp_alloc_large(struct mp_pool_s *pool, size_t size) { //分配大块空间  void *p = malloc(size); if (p == NULL) return NULL; size_t n = 0; struct mp_large_s *large; for (large = pool-\u0026gt;large; large; large = large-\u0026gt;next) { if (large-\u0026gt;alloc == NULL) { large-\u0026gt;alloc = p; return p; } if (n ++ \u0026gt; 3) break; } large = mp_alloc(pool, sizeof(struct mp_large_s));// large这个数据结构本身也需要交由内存池来管理，分析一下在mp_alloc中因为这个数据结构很小会存储在页面上，故不会产生无限循环 \tif (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; //large链表的头插法，很简单  return p; } void *mp_memalign(struct mp_pool_s *pool, size_t size, size_t alignment) { void *p; int ret = posix_memalign(\u0026amp;p, alignment, size); if (ret) { return NULL; } struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s)); if (large == NULL) { free(p); return NULL; } large-\u0026gt;alloc = p; large-\u0026gt;next = pool-\u0026gt;large; pool-\u0026gt;large = large; return p; } void *mp_alloc(struct mp_pool_s *pool, size_t size) { //内存分配的入口函数，分别处理大块内存和小块内存需求，小块内存需求在页面末尾空间不足时进入新建页面并分配函数中，否则直接分配在当前页面就可以  unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = mp_align_ptr(p-\u0026gt;last, MP_ALIGNMENT); if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m + size; return m; } p = p-\u0026gt;next; } while (p); //循环在current及其后的一个或多个页面上查找符合要求的末尾空间，存在的话就return  return mp_alloc_block(pool, size); //进到这里说明不存在符合要求空间，那就新建页面然后分配并对页面failed值计数和调整current页面 \t} return mp_alloc_large(pool, size); //大块内存情况 \t} void *mp_nalloc(struct mp_pool_s *pool, size_t size) { unsigned char *m; struct mp_node_s *p; if (size \u0026lt;= pool-\u0026gt;max) { p = pool-\u0026gt;current; do { m = p-\u0026gt;last; if ((size_t)(p-\u0026gt;end - m) \u0026gt;= size) { p-\u0026gt;last = m+size; return m; } p = p-\u0026gt;next; } while (p); return mp_alloc_block(pool, size); } return mp_alloc_large(pool, size); } void *mp_calloc(struct mp_pool_s *pool, size_t size) { void *p = mp_alloc(pool, size); if (p) { memset(p, 0, size); } return p; } void mp_free(struct mp_pool_s *pool, void *p) { struct mp_large_s *l; for (l = pool-\u0026gt;large; l; l = l-\u0026gt;next) { if (p == l-\u0026gt;alloc) { free(l-\u0026gt;alloc); l-\u0026gt;alloc = NULL; return ; } } } int main(int argc, char *argv[]) { int size = 1 \u0026lt;\u0026lt; 12; struct mp_pool_s *p = mp_create_pool(size); int i = 0; for (i = 0;i \u0026lt; 10;i ++) { void *mp = mp_alloc(p, 512); //\tmp_free(mp); \t} //printf(\u0026#34;mp_create_pool: %ld\\n\u0026#34;, p-\u0026gt;max); \tprintf(\u0026#34;mp_align(123, 32): %d, mp_align(17, 32): %d\\n\u0026#34;, mp_align(24, 32), mp_align(17, 32)); //printf(\u0026#34;mp_align_ptr(p-\u0026gt;current, 32): %lx, p-\u0026gt;current: %lx, mp_align(p-\u0026gt;large, 32): %lx, p-\u0026gt;large: %lx\\n\u0026#34;, mp_align_ptr(p-\u0026gt;current, 32), p-\u0026gt;current, mp_align_ptr(p-\u0026gt;large, 32), p-\u0026gt;large);  int j = 0; for (i = 0;i \u0026lt; 5;i ++) { char *pp = mp_calloc(p, 32); for (j = 0;j \u0026lt; 32;j ++) { if (pp[j]) { printf(\u0026#34;calloc wrong\\n\u0026#34;); } printf(\u0026#34;calloc success\\n\u0026#34;); } } //printf(\u0026#34;mp_reset_pool\\n\u0026#34;);  for (i = 0;i \u0026lt; 5;i ++) { void *l = mp_alloc(p, 8192); mp_free(p, l); } mp_reset_pool(p); //printf(\u0026#34;mp_destory_pool\\n\u0026#34;); \tfor (i = 0;i \u0026lt; 58;i ++) { mp_alloc(p, 256); } mp_destory_pool(p); return 0; } ","date":"2020-03-19T00:00:00Z","image":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/244_hud068d333fa339d76c3f1e66a3f8bf604_8126422_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E5%86%85%E5%AD%98%E6%B1%A0/","title":"内存池"},{"content":"请求池实现 同步阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  struct dns_header { unsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item { char *domain; char *ip; }; int dns_create_header(struct dns_header *header) { if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) { int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) { int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); return 0; } char *domain[] = { //\t\u0026#34;www.ntytcp.com\u0026#34;, \t\u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; typedef void (*async_result_cb)(struct dns_item *arg, int count); struct async_context { int epfd; pthread_t threadid; }; struct ep_arg { int sockfd; async_result_cb cb; }; #define ASYNC_EVENTS\t128  void *dns_async_callback(void *arg) { struct async_context* ctx = (struct async_context*)arg; while (1) { struct epoll_event events[ASYNC_EVENTS] = {0}; int nready = epoll_wait(ctx-\u0026gt;epfd, events, ASYNC_EVENTS, -1); if (nready \u0026lt; 0) { continue; } int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *ptr = events[i].data.ptr; int sockfd = ptr-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; int count = dns_parse_response(buffer, \u0026amp;domains); ptr-\u0026gt;cb(domains, count); // sockfd \tclose (sockfd); free(ptr); // epollout --\u0026gt; \t//epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_MOD, sockfd, NULL); \t} } } // 1 . context // 2 . return context; // struct async_context* dns_async_client_init(void) { int epfd = epoll_create(1); if (epfd \u0026lt; 0) return NULL; struct async_context* ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) return NULL; ctx-\u0026gt;epfd = epfd; int ret = pthread_create(\u0026amp;ctx-\u0026gt;threadid, NULL, dns_async_callback, ctx); if (ret) { close(epfd); free(ctx); return NULL; } return ctx; } int dns_async_client_destroy(struct async_context* ctx) { close(ctx-\u0026gt;epfd); pthread_cancel(ctx-\u0026gt;threadid); } // int dns_async_client_commit(struct async_context *ctx, async_result_cb cb) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); printf(\u0026#34;connect :%d\\n\u0026#34;, ret); struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *ptr = calloc(1, sizeof(struct ep_arg)); if (ptr == NULL) return -1; ptr-\u0026gt;sockfd = sockfd; ptr-\u0026gt;cb = cb; // \tstruct epoll_event ev; ev.data.ptr = ptr; ev.events = EPOLLIN; epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); return 0; } int main(int argc, char *argv[]) { int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_client_commit(domain[i]); } getchar(); } 异步非阻塞请求池 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;#include \u0026lt;netdb.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define DNS_SVR\t\u0026#34;114.114.114.114\u0026#34;  #define DNS_HOST\t0x01 #define DNS_CNAME\t0x05  #define ASYNC_CLIENT_NUM\t1024  struct dns_header { //dns 头 \tunsigned short id; unsigned short flags; unsigned short qdcount; unsigned short ancount; unsigned short nscount; unsigned short arcount; }; struct dns_question {//dns 请求 \tint length; unsigned short qtype; unsigned short qclass; char *qname; }; struct dns_item {//dns 基本信息 \tchar *domain; char *ip; }; typedef void (*async_result_cb)(struct dns_item *list, int count); //请求端针对返回数据的处理函数（以回调函数的方式存在，在recv后调用，通过epoll中的event内的data中ptr传递，这一点与reactor中很像）  struct async_context { //异步请求池中 epoll_wait单独占用一个线程 且epoll也需要在其他线程中出现，这一部分为不同线程共用部分，所以命名为async context也叫上下文 //上下文中一般来讲还需要保存进行epoll_wait的线程id，但这里保存也没啥用 \tint epfd; }; struct ep_arg { //每一个请求（fd）和其处理函数(cb)存在一个sturct中，与reactor 类似 \tint sockfd; async_result_cb cb; }; int dns_create_header(struct dns_header *header) { //dns 创建头  if (header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags |= htons(0x0100); header-\u0026gt;qdcount = htons(1); return 0; } int dns_create_question(struct dns_question *question, const char *hostname) { //dns 创建请求  if (question == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;qname = (char*)malloc(strlen(hostname) + 2); if (question-\u0026gt;qname == NULL) return -2; question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname_p = question-\u0026gt;qname; while (token != NULL) { size_t len = strlen(token); *qname_p = len; qname_p ++; strncpy(qname_p, token, len+1); qname_p += len; token = strtok(NULL, delim); } free(hostname_dup); return 0; } int dns_build_request(struct dns_header *header, struct dns_question *question, char *request) {//dns 创建请求  int header_s = sizeof(struct dns_header); int question_s = question-\u0026gt;length + sizeof(question-\u0026gt;qtype) + sizeof(question-\u0026gt;qclass); int length = question_s + header_s; int offset = 0; memcpy(request+offset, header, sizeof(struct dns_header)); offset += sizeof(struct dns_header); memcpy(request+offset, question-\u0026gt;qname, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); return length; } static int is_pointer(int in) { return ((in \u0026amp; 0xC0) == 0xC0); } static int set_block(int fd, int block) { //设置fd的阻塞类型 \tint flags = fcntl(fd, F_GETFL, 0); if (flags \u0026lt; 0) return flags; if (block) { flags \u0026amp;= ~O_NONBLOCK; } else { flags |= O_NONBLOCK; } if (fcntl(fd, F_SETFL, flags) \u0026lt; 0) return -1; return 0; } static void dns_parse_name(unsigned char *chunk, unsigned char *ptr, char *out, int *len) { //dns 解析域名  int flag = 0, n = 0, alen = 0; char *pos = out + (*len); while (1) { flag = (int)ptr[0]; if (flag == 0) break; if (is_pointer(flag)) { n = (int)ptr[1]; ptr = chunk + n; dns_parse_name(chunk, ptr, out, len); break; } else { ptr ++; memcpy(pos, ptr, flag); pos += flag; ptr += flag; *len += flag; if ((int)ptr[0] != 0) { memcpy(pos, \u0026#34;.\u0026#34;, 1); pos += 1; (*len) += 1; } } } } static int dns_parse_response(char *buffer, struct dns_item **domains) {//dns 解析应答  int i = 0; unsigned char *ptr = buffer; ptr += 4; int querys = ntohs(*(unsigned short*)ptr); ptr += 2; int answers = ntohs(*(unsigned short*)ptr); ptr += 6; for (i = 0;i \u0026lt; querys;i ++) { while (1) { int flag = (int)ptr[0]; ptr += (flag + 1); if (flag == 0) break; } ptr += 4; } char cname[128], aname[128], ip[20], netip[4]; int len, type, ttl, datalen; int cnt = 0; struct dns_item *list = (struct dns_item*)calloc(answers, sizeof(struct dns_item)); if (list == NULL) { return -1; } for (i = 0;i \u0026lt; answers;i ++) { bzero(aname, sizeof(aname)); len = 0; dns_parse_name(buffer, ptr, aname, \u0026amp;len); ptr += 2; type = htons(*(unsigned short*)ptr); ptr += 4; ttl = htons(*(unsigned short*)ptr); ptr += 4; datalen = ntohs(*(unsigned short*)ptr); ptr += 2; if (type == DNS_CNAME) { bzero(cname, sizeof(cname)); len = 0; dns_parse_name(buffer, ptr, cname, \u0026amp;len); ptr += datalen; } else if (type == DNS_HOST) { bzero(ip, sizeof(ip)); if (datalen == 4) { memcpy(netip, ptr, datalen); inet_ntop(AF_INET , netip , ip , sizeof(struct sockaddr)); printf(\u0026#34;%s has address %s\\n\u0026#34; , aname, ip); printf(\u0026#34;\\tTime to live: %d minutes , %d seconds\\n\u0026#34;, ttl / 60, ttl % 60); list[cnt].domain = (char *)calloc(strlen(aname) + 1, 1); memcpy(list[cnt].domain, aname, strlen(aname)); list[cnt].ip = (char *)calloc(strlen(ip) + 1, 1); memcpy(list[cnt].ip, ip, strlen(ip)); cnt ++; } ptr += datalen; } } *domains = list; ptr += 2; return cnt; } int dns_client_commit(const char *domain) { //这个是同步请求中的提交，大概看一下就可以  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); while (1) { char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); if (n \u0026lt;= 0) continue; printf(\u0026#34;recvfrom n : %d\\n\u0026#34;, n); struct dns_item *domains = NULL; dns_parse_response(buffer, \u0026amp;domains); break; } return 0; } void dns_async_client_free_domains(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { free(list[i].domain); free(list[i].ip); } free(list); } //dns_async_client_proc() //epoll_wait //result callback static void* dns_async_client_proc(void *arg) { //异步请求框架第三环节。 callback函数，这个callback函数不是用于处理接受数据的callback，而是initial中用于接受和处理数据的线程运行的函数，在线程创建时传入 \tstruct async_context *ctx = (struct async_context*)arg; int epfd = ctx-\u0026gt;epfd; while (1) { struct epoll_event events[ASYNC_CLIENT_NUM] = {0}; int nready = epoll_wait(epfd, events, ASYNC_CLIENT_NUM, -1); if (nready \u0026lt; 0) { if (errno == EINTR || errno == EAGAIN) { continue; } else { break; } } else if (nready == 0) { continue; } printf(\u0026#34;nready:%d\\n\u0026#34;, nready); int i = 0; for (i = 0;i \u0026lt; nready;i ++) { struct ep_arg *data = (struct ep_arg*)events[i].data.ptr; int sockfd = data-\u0026gt;sockfd; char buffer[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); struct dns_item *domain_list = NULL; int count = dns_parse_response(buffer, \u0026amp;domain_list); data-\u0026gt;cb(domain_list, count); //call cb \tint ret = epoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL); //printf(\u0026#34;epoll_ctl DEL --\u0026gt; sockfd:%d\\n\u0026#34;, sockfd);  close(sockfd); ///// //异步请求框架第四环节。 关闭  dns_async_client_free_domains(domain_list, count); free(data); } } } //dns_async_client_init() //epoll init //thread init struct async_context *dns_async_client_init(void) { //异步请求框架第一环节。 initial 初始化上下文信息，主要是创建epoll、创建线程（接受数据的线程）  int epfd = epoll_create(1); // \tif (epfd \u0026lt; 0) return NULL; struct async_context *ctx = calloc(1, sizeof(struct async_context)); if (ctx == NULL) { close(epfd); return NULL; } ctx-\u0026gt;epfd = epfd; pthread_t thread_id; int ret = pthread_create(\u0026amp;thread_id, NULL, dns_async_client_proc, ctx); if (ret) { perror(\u0026#34;pthread_create\u0026#34;); return NULL; } usleep(1); //child go first  return ctx; } //dns_async_client_commit(ctx, domain) //socket init //dns_request //sendto dns send int dns_async_client_commit(struct async_context* ctx, const char *domain, async_result_cb cb) { //异步请求框架第二环节 commit，提交请求。 可以看到传参上下文，获取epoll，将新来的请求提交并加入epoll中在另一个线程中等待返回数据并使用传递的cb解析  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if (sockfd \u0026lt; 0) { perror(\u0026#34;create socket failed\\n\u0026#34;); exit(-1); } printf(\u0026#34;url:%s\\n\u0026#34;, domain); set_block(sockfd, 0); //nonblock  struct sockaddr_in dest; bzero(\u0026amp;dest, sizeof(dest)); dest.sin_family = AF_INET; dest.sin_port = htons(53); dest.sin_addr.s_addr = inet_addr(DNS_SVR); int ret = connect(sockfd, (struct sockaddr*)\u0026amp;dest, sizeof(dest)); //printf(\u0026#34;connect :%d\\n\u0026#34;, ret);  struct dns_header header = {0}; dns_create_header(\u0026amp;header); struct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); char request[1024] = {0}; int req_len = dns_build_request(\u0026amp;header, \u0026amp;question, request); int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)\u0026amp;dest, sizeof(struct sockaddr)); struct ep_arg *eparg = (struct ep_arg*)calloc(1, sizeof(struct ep_arg)); //初始化这个新请求后续返回结果对应的处理函数，并通过下面ev.data.ptr = eparg保存并通过另一个线程检测到返回数据后调用 \tif (eparg == NULL) return -1; eparg-\u0026gt;sockfd = sockfd; eparg-\u0026gt;cb = cb; struct epoll_event ev; ev.data.ptr = eparg; ev.events = EPOLLIN; ret = epoll_ctl(ctx-\u0026gt;epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); //printf(\u0026#34; epoll_ctl ADD: sockfd-\u0026gt;%d, ret:%d\\n\u0026#34;, sockfd, ret);  return ret; } char *domain[] = { \u0026#34;www.ntytcp.com\u0026#34;, \u0026#34;bojing.wang\u0026#34;, \u0026#34;www.baidu.com\u0026#34;, \u0026#34;tieba.baidu.com\u0026#34;, \u0026#34;news.baidu.com\u0026#34;, \u0026#34;zhidao.baidu.com\u0026#34;, \u0026#34;music.baidu.com\u0026#34;, \u0026#34;image.baidu.com\u0026#34;, \u0026#34;v.baidu.com\u0026#34;, \u0026#34;map.baidu.com\u0026#34;, \u0026#34;baijiahao.baidu.com\u0026#34;, \u0026#34;xueshu.baidu.com\u0026#34;, \u0026#34;cloud.baidu.com\u0026#34;, \u0026#34;www.163.com\u0026#34;, \u0026#34;open.163.com\u0026#34;, \u0026#34;auto.163.com\u0026#34;, \u0026#34;gov.163.com\u0026#34;, \u0026#34;money.163.com\u0026#34;, \u0026#34;sports.163.com\u0026#34;, \u0026#34;tech.163.com\u0026#34;, \u0026#34;edu.163.com\u0026#34;, \u0026#34;www.taobao.com\u0026#34;, \u0026#34;q.taobao.com\u0026#34;, \u0026#34;sf.taobao.com\u0026#34;, \u0026#34;yun.taobao.com\u0026#34;, \u0026#34;baoxian.taobao.com\u0026#34;, \u0026#34;www.tmall.com\u0026#34;, \u0026#34;suning.tmall.com\u0026#34;, \u0026#34;www.tencent.com\u0026#34;, \u0026#34;www.qq.com\u0026#34;, \u0026#34;www.aliyun.com\u0026#34;, \u0026#34;www.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;hotels.ctrip.com\u0026#34;, \u0026#34;vacations.ctrip.com\u0026#34;, \u0026#34;flights.ctrip.com\u0026#34;, \u0026#34;trains.ctrip.com\u0026#34;, \u0026#34;bus.ctrip.com\u0026#34;, \u0026#34;car.ctrip.com\u0026#34;, \u0026#34;piao.ctrip.com\u0026#34;, \u0026#34;tuan.ctrip.com\u0026#34;, \u0026#34;you.ctrip.com\u0026#34;, \u0026#34;g.ctrip.com\u0026#34;, \u0026#34;lipin.ctrip.com\u0026#34;, \u0026#34;ct.ctrip.com\u0026#34; }; static void dns_async_client_result_callback(struct dns_item *list, int count) { int i = 0; for (i = 0;i \u0026lt; count;i ++) { printf(\u0026#34;name:%s, ip:%s\\n\u0026#34;, list[i].domain, list[i].ip); } } int main(int argc, char *argv[]) { #if 0dns_client_commit(argv[1]); #else  struct async_context *ctx = dns_async_client_init(); if (ctx == NULL) return -2; int count = sizeof(domain) / sizeof(domain[0]); int i = 0; for (i = 0;i \u0026lt; count;i ++) { dns_async_client_commit(ctx, domain[i], dns_async_client_result_callback); //sleep(2); \t} getchar(); #endif \t} ","date":"2020-03-17T00:00:00Z","image":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/247_hu00377075f8d3c09f9b9bff67495c3ec1_5277773_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E8%AF%B7%E6%B1%82%E6%B1%A0/","title":"请求池"},{"content":"锁 自旋锁 当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -\u0026gt; 等待的机制被称为自旋锁(spinlock)。\n自旋锁的原理比较简单，如果持有锁的线程能在短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。\n因为自旋锁避免了操作系统进程调度和线程切换，所以自旋锁通常适用在时间比较短的情况下。由于这个原因，操作系统的内核经常使用自旋锁。但是，如果长时间上锁的话，自旋锁会非常耗费性能，它阻止了其他线程的运行和调度。线程持有锁的时间越长，则持有该锁的线程将被 OS(Operating System) 调度程序中断的风险越大。如果发生中断情况，那么其他线程将保持旋转状态(反复尝试获取锁)，而持有该锁的线程并不打算释放锁，这样导致的是结果是无限期推迟，直到持有锁的线程可以完成并释放它为止。\n解决上面这种情况一个很好的方式是给自旋锁设定一个自旋时间，等时间一到立即释放自旋锁。自旋锁的目的是占着CPU资源不进行释放，等到获取锁立即进行处理。但是如何去选择自旋时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用 CPU 资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！\n在计算任务轻的情况下使用自旋锁可以显著提升速度，这是因为线程切换的开销大于等锁的开销，但是计算任务重的话自旋锁的等待时间就成为主要的开销了。\n互斥锁 互斥锁实际是一个互斥量，为获得互斥锁的线程会挂起，这就涉及到线程切换的开销，计算任务重的情况下会比较适合使用。\n读写锁 读写锁即只能由一人写但可以由多人读的锁，适用于读操作很多但写操作很少的情况下。\n原子操作 多线程下使用原子操作确保我们的操作不会被其他线程参与，一般内联汇编 memory 内存屏障，只允许这一缓存写回内存，确保多线程安全。\n多进程下对共享内存的操作使用内联汇编lock，锁住总线，同一时刻只允许一个进程通过总线操作内存。\n粒度大小排序\n互斥锁 \u0026gt; 自旋锁 \u0026gt; 读写锁 \u0026gt; 原子操作\n代码实现 #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #define THREAD_SIZE 10  int count = 0; pthread_mutex_t mutex; pthread_spinlock_t spinlock; pthread_rwlock_t rwlock; // MOV dest, src; at\u0026amp;t // MOV src, dest; x86  int inc(int *value, int add) { int old; __asm__ volatile ( //lock 锁住总线 只允许一个cpu操作内存（通过总线）确保多进程安全  \u0026#34;lock; xaddl %2, %1;\u0026#34; // \u0026#34;lock; xchg %2, %1, %3;\u0026#34;  : \u0026#34;=a\u0026#34; (old) : \u0026#34;m\u0026#34; (*value), \u0026#34;a\u0026#34; (add) : \u0026#34;cc\u0026#34;, \u0026#34;memory\u0026#34; //memory 内存屏障，只允许这一缓存写回内存，确保多线程安全  ); return old; } // void *func(void *arg) { int *pcount = (int *)arg; int i = 0; while (i++ \u0026lt; 100000) { #if 0//无锁 (*pcount) ++; #elif 0// 互斥锁版本  pthread_mutex_lock(\u0026amp;mutex); (*pcount) ++; pthread_mutex_unlock(\u0026amp;mutex); #elif 0// 互斥锁非阻塞版本  if (0 != pthread_mutex_trylock(\u0026amp;mutex)) { i --; continue; } (*pcount) ++; pthread_mutex_unlock(\u0026amp;mutex); #elif 0 //自旋锁  pthread_spin_lock(\u0026amp;spinlock); (*pcount) ++; pthread_spin_unlock(\u0026amp;spinlock); #elif 0//读写锁  pthread_rwlock_wrlock(\u0026amp;rwlock); (*pcount) ++; pthread_rwlock_unlock(\u0026amp;rwlock); #else //原子操作  inc(pcount, 1); #endif  usleep(1); } } int main() { #if 0//多线程 pthread_t threadid[THREAD_SIZE] = {0}; pthread_mutex_init(\u0026amp;mutex, NULL); pthread_spin_init(\u0026amp;spinlock, PTHREAD_PROCESS_SHARED); pthread_rwlock_init(\u0026amp;rwlock, NULL); int i = 0; int count = 0; for (i = 0;i \u0026lt; THREAD_SIZE;i ++) { int ret = pthread_create(\u0026amp;threadid[i], NULL, func, \u0026amp;count); if (ret) { break; } } for (i = 0;i \u0026lt; 100;i ++) { pthread_rwlock_rdlock(\u0026amp;rwlock); printf(\u0026#34;count --\u0026gt; %d\\n\u0026#34;, count); pthread_rwlock_unlock(\u0026amp;rwlock); sleep(1); } #else //多进程  int *pcount = mmap(NULL, sizeof(int), PROT_READ | PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0); //将pcount设置为共享内存区域，这样多进程都可以访问  int i = 0; pid_t pid = 0; for (i = 0;i \u0026lt; THREAD_SIZE;i ++) { pid = fork(); if (pid \u0026lt;= 0) { //创建固定数量进程的巧妙操作，子进程被创建后会休眠1微秒后退出这个创建循环开始下面的加法运算，确保只有主进程在创建子进程。  usleep(1); break; } } if (pid \u0026gt; 0) { // 主进程不做加法，只打印信息  for (i = 0;i \u0026lt; 100;i ++) { printf(\u0026#34;count --\u0026gt; %d\\n\u0026#34;, (*pcount)); sleep(1); } } else {//子进程在这里做加法运算  int i = 0; while (i++ \u0026lt; 100000) { #if 0//总线不锁，进程读取到的共享内存更新存在延迟，最终结果小于一百万 (*pcount) ++; #else//锁总线，一次只有一个cpu能操作共享内存，也告诉我们临界资源就是很难发挥出多核性能  inc(pcount, 1); #endif  usleep(1); } } #endif } ","date":"2020-03-10T00:00:00Z","image":"https://gao377020481.github.io/p/%E9%94%81/389_hu19a302d4968ef7ca02b91dd8f18c3f5f_3179286_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E9%94%81/","title":"锁"},{"content":"HTTP 实现http客户端程序\n基础 HTTP使用TCP连接\nHTTP报文：\n\r \r\n实现 域名到ip地址转换(dns) 直接调用api进行转换比较简单：\nchar * host_to_ip(const char* hostname) { struct hostent *host_entry = gethostbyname(hostname); if(host_entry) { return inet_ntoa(*(struct in_addr*)*host_entry -\u0026gt; h_addr_list); } return NULL; } host_entry存储了dns请求的接收，从中取出第一个ip地址并将点分十进制转换为字符串返回\n创建TCP套接字（建立连接） posix api创建\nint http_create_socket(char *ip) { int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in sin = {0}; sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr.s_addr = inet_addr(ip); if(0 != connect(sockfd, (struct sockaddr*)\u0026amp;sin, sizeof(struct sockaddr_in))) { return -1; } fcntl(sockfd, F_SETFL, O_NONBLOCK); return sockfd; } fcntl(sockfd, F_SETFL, O_NONBLOCK);这个函数用于设置该套接字io为非阻塞\n通过套接字向目标网站请求资源（select）\nchar * http_send_request(const char *hostname, const char *resource) { char *ip = host_to_ip(hostname); // \tint sockfd = http_create_socket(ip); char buffer[BUFFER_SIZE] = {0}; sprintf(buffer, \u0026#34;GET %s %s\\r\\n\\ Host: %s\\r\\n\\ %s\\r\\n\\ \\r\\n\u0026#34;, resource, HTTP_VERSION, hostname, CONNECTION_TYPE ); send(sockfd, buffer, strlen(buffer), 0); //select  fd_set fdread; FD_ZERO(\u0026amp;fdread); FD_SET(sockfd, \u0026amp;fdread); struct timeval tv; tv.tv_sec = 5; tv.tv_usec = 0; char *result = malloc(sizeof(int)); memset(result, 0, sizeof(int)); while (1) { int selection = select(sockfd+1, \u0026amp;fdread, NULL, NULL, \u0026amp;tv); if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread)) { break; } else { memset(buffer, 0, BUFFER_SIZE); int len = recv(sockfd, buffer, BUFFER_SIZE, 0); if (len == 0) { // disconnect \tbreak; } result = realloc(result, (strlen(result) + len + 1) * sizeof(char)); strncat(result, buffer, len); } } return result; } select部分： 首先根据套接字初始化fread来监听io，如果有消息到来就置为1，调用select函数： select(sockfd, \u0026amp;rset, \u0026amp;wset, *eset, *tv); \u0026amp;rset位置表示读监听io \u0026amp;wset位置表示写监听io \u0026amp;eset位置表示错误监听io（断开或者其他） tv为轮询间隔时间 select函数内部轮询监听这几个io，有置1就说明有信息需要处理，就返回然后处理信息 断开连接的话返回0，所以if (!selection || !FD_ISSET(sockfd, \u0026amp;fdread))可以有效控制连接断开的break 正常时返回收到的结果result\n附main函数\nint main(char argc, char*argv[]) { if(argc \u0026lt;3) { return -1; } char *response = http_send_request(argv[1], argv[2]); printf(\u0026#34;response: %s\\n\u0026#34;, response); free(response); return 1; } ","date":"2020-03-01T00:00:00Z","image":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/florian-klauer-nptLmg6jqDo-unsplash_hu595aaf3b3dbbb41af5aed8d3958cc9f9_13854_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/http%E5%AE%A2%E6%88%B7%E7%AB%AF/","title":"简易http客户端(C posix API)"},{"content":"DNS Domain Name System 域名系统，是一个分布式数据库，用于映射IP与域名\n每级域名长度限制为63，总长度限制为253，使用TCP UDP端口53\nDNS分层 顶级域：com org等 第二级域：baidu google等 第三级域：www edu等\n域名解析 静态映射：在本机上配置域名和ip映射并直接使用\n动态映射：使用DNS域名解析系统，在DNS服务器上配置ip到域名的映射\n域名服务器 根域名服务器： 共a-m十三台（十三个ip）但拥有很多镜像服务器，镜像与本体使用同一个ip，存有顶级域名服务器的ip 顶级域名服务器：管理在该顶级域名服务器下注册的二级域名 权限域名服务器：一个区域的域名解析 本地域名服务器：处理本地的请求，保存本地的映射\n域名解析方式 迭代查询：本机请求本地域名服务器，本地域名服务器开始迭代的查询各个层级服务器，先查询根获得顶级的ip然后根据获得的ip查询顶级在获得区域的ip依次迭代查到请求的映射\n递归查询：递归查询时只发出一次请求然后等待接收到最终结果，在上面的步骤中本机使用的就是递归查询\n协议报文格式 \rdns_dp\r \rdns_dp\r \rdns_dp\r \rdns_dp\r\n具体查看文档\nDNS client UDP编程 首先需要自己定义数据结构用于存储dns报文\nstruct dns_header{ unsigned short id; unsigned short flags; unsigned short questions; unsigned short answer; unsigned short authority; unsigned short additional; }; struct dns_question { int length; unsigned short qtype; unsigned short qclass; unsigned char *name; }; 这里只需要question和header是因为我们作为client只实现发送A请求也就是获取域名的ipv4地址，在实现中header的授权码和附加码都不需要使用只需要使用questions id和flags即可\n先建立UDP套接字用于发送UDP报文：\n#define DNS_SERVER_PORT 53 #define DNS_SERVER_IP \u0026#34;114.114.114.114\u0026#34;  int sockfd = socket(AF_INET, SOCK_DGRAM, 0); if(sockfd \u0026lt; 0) { return -1; } struct sockaddr_in servaddr = {0}; servaddr.sin_family = AF_INET; servaddr.sin_port = htons(DNS_SERVER_PORT); servaddr.sin_addr.s_addr = inet_addr(DNS_SERVER_IP); 然后使用connect检测开辟本机到DNS服务器的通路：\nint ret = connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); ret返回值可以用于检测通路是否存在\n然后开始初始化要发送的DNS报文：\n1.初始化dns header：\nstruct dns_header header = {0}; dns_create_header(\u0026amp;header); 使用的dns_create_header：\nint dns_create_header(struct dns_header *header) { if(header == NULL) return -1; memset(header, 0, sizeof(struct dns_header)); //random  srandom(time(NULL)); header-\u0026gt;id = random(); header-\u0026gt;flags = htons(0x0100); header-\u0026gt;questions = htons(1); return 0; } 这里的id为随机生成 questions为1代表一个问题 flags的0x0100查表后代表：期望递归 传地址来初始化header\n然后就是初始化正文部分的question：\nstruct dns_question question = {0}; dns_create_question(\u0026amp;question, domain); dns_create_question的实现：\nint dns_create_question(struct dns_question* question, const char *hostname) { if(question == NULL || hostname == NULL) return -1; memset(question, 0, sizeof(struct dns_question)); question-\u0026gt;name = (char*)malloc(strlen(hostname) + 2); if(question-\u0026gt;name == NULL) { return -1; } question-\u0026gt;length = strlen(hostname) + 2; question-\u0026gt;qtype = htons(1); question-\u0026gt;qclass = htons(1); //name  const char delim[2] = \u0026#34;.\u0026#34;; char *hostname_dup = strdup(hostname); char *token = strtok(hostname_dup, delim); char *qname = question-\u0026gt;name; while(token != NULL) { size_t len = strlen(token); *qname = len; qname++; strncpy(qname, token, len+1); // +1 copy \\0  qname += len; token = strtok(NULL, delim); } free(hostname_dup); } 这里正文的初始化使用循环和strtok分别取出每一级域名然后形成正文部分，www.baidu.com \u0026ndash;\u0026gt; 3www5baidu3com0\n接下来就可以将已经初始化好的header和question组装成dns报文request：\nchar request[1024] = {0}; int length = dns_build_request(\u0026amp;header, \u0026amp;question, request, 1024); dns_build_request的实现：\nint dns_build_request(struct dns_header* header, struct dns_question* question, char *request, int rlen ) { if(header == NULL || question == NULL || request == NULL) return -1; memset(request, 0 , rlen); //header --\u0026gt; request  int offset = 0; memcpy(request, header, sizeof(struct dns_header)); offset = sizeof(struct dns_header); //question --\u0026gt; request  memcpy(request+offset, question-\u0026gt;name, question-\u0026gt;length); offset += question-\u0026gt;length; memcpy(request+offset, \u0026amp;question-\u0026gt;qtype, sizeof(question-\u0026gt;qtype)); offset += sizeof(question-\u0026gt;qtype); memcpy(request+offset, \u0026amp;question-\u0026gt;qclass, sizeof(question-\u0026gt;qclass)); offset += sizeof(question-\u0026gt;qclass); return offset; } 可以看到就是按照协议进行拼接\n接下来就可以调用unix 网络编程capi来通过udp套接字发送正文：\n//request int slen = sendto(sockfd, request, length, 0, (struct sockaddr*)\u0026amp;servaddr, sizeof(struct sockaddr)); 至此发送就结束了\n我们发送了dns查询报文dns服务器会返回结果，所以可以通过套接字来接收这个结果，并打印出来验证一下：\n//recvfrom() char response[1024] = {0}; struct sockaddr_in addr; size_t addr_len = sizeof(struct sockaddr_in); int n =recvfrom(sockfd, response, sizeof(response), 0, (struct sockaddr*)\u0026amp;addr, (socklen_t*)\u0026amp;addr_len); printf(\u0026#34;recvfrom : %d, %s\\n\u0026#34;, n, response); 也可以写代码解析返回的结果，具体查看代码就可以\n至此就实现了一个dns客户端用于给dns服务器发送查询报文来查询一个域名的ipv4地址\n","date":"2020-02-23T00:00:00Z","image":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/081_hu7f5bc23efa6e31f2e831fa0dcdad6471_5540510_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/dns%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90/","title":"DNS协议解析"},{"content":"Tcp服务器 一请求一线程 首先说明问题： 已请求一线程能承载的请求数量极少，posix标准线程8M，请求数量多时极其占用内存\n简单实现 实现一请求一线程很简单：\n#define BUFFER_SIZE 1024 void *client_routine(void *arg) { int clientfd = *(int *) arg; while(1) { char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); break; } else if(len ==0 ) { close(clientfd); break; } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } while(1) { struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); pthread_t thread_id; pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd); } return 0; } main函数中首先在指定端口处打开一个迎宾套接字sockfd用于对到来的请求分配线程（新建客户端套接字）来处理\nint clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); 就是迎宾套接字不断检测是否有新请求到来，如果到来就返回clientfd 后面新建立的线程将clientfd作为参数传递给线程执行函数即可： pthread_create(\u0026amp;thread_id, NULL, client_routine, \u0026amp;clientfd);\nEpoll实现Tcp服务端 使用epoll来管理多个io到来的请求然后依次处理这些请求\n首先理清逻辑\n还是通过迎宾套接字来捕获请求，只是迎宾套接字应该首先加入epoll中 外层循环需要对epoll中有输入的套接字（io）遍历，如果有输入（有请求），就处理，这里需要判断套接字的类型：如果是迎宾套接字说明有新的请求，需要通过accpet来创建clientfd并交给epoll管理，如果不是就处理请求，处理完毕后从epoll中删除套接字（io），所以这个逻辑下可以知道，到来的请求是在下一轮中才处理的，并不是一到来就立即处理\nC实现\n#define EPOLL_SIZE 1024  #define BUFFER_SIZE 1024  int main(int argc, char *argv[]) { if(argc \u0026lt;2) return -1; int port = atoi(argv[1]); int sockfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in addr; memset(\u0026amp;addr, 0, sizeof(struct sockaddr_in)); addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = INADDR_ANY; if(bind(sockfd, (struct sockaddr*)\u0026amp;addr, sizeof(struct sockaddr_in)) \u0026lt;0) { perror(\u0026#34;bind\\n\u0026#34;); return 2; } if(listen(sockfd, 5) \u0026lt;0 ) { perror(\u0026#34;listen\\n\u0026#34;); return 3; } int epfd = epoll_create(1); struct epoll_event events[EPOLL_SIZE] = {0}; struct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); while(1) { int nready = epoll_wait(epfd, events, EPOLL_SIZE, 5); //-1 events里没东西就不去  if(nready == -1) continue; int i=0; for(i = 0;i \u0026lt;nready;++i) { if (events[i].data.fd == sockfd) { //迎宾的sock那就新产生一个clientfd然后交给epoll  struct sockaddr_in client_addr; memset(\u0026amp;client_addr, 0, sizeof(struct sockaddr_in)); socklen_t client_len = sizeof(client_addr); int clientfd = accept(sockfd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len); ev.events = EPOLLIN | EPOLLET; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, \u0026amp;ev); } else { int clientfd = events[i].data.fd; char buffer[BUFFER_SIZE] = {0}; int len = recv(clientfd, buffer, BUFFER_SIZE, 0); if(len \u0026lt;0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else if(len ==0 ) { close(clientfd); ev.events = EPOLLIN; ev.data.fd = clientfd; epoll_ctl(epfd, EPOLL_CTL_DEL, clientfd, \u0026amp;ev); } else{ printf(\u0026#34;Recv: %s, %d btye(s) from %d\\n\u0026#34;, buffer, len, clientfd); } } } } return 0; } 关键的函数是epoll_ctl，epoll_create，epoll_wait\nepoll_create的参数是一个int size这个参数在新版本linux内核中无意义，原来也只是告诉内核epoll大致的大小 epoll_ctl配合EPOLL_CTL_xxx的宏来对epoll进行操作（添加删除修改） epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout) 用来等待事件发生，其他参数比较简单，只说timeout参数：在没有检测到事件发生时最多等待的时间（单位为毫秒）\nET、LT工作模式 水平触发模式：\nev.events = EPOLLIN; 边缘触发模式：\nev.events = EPOLLIN | EPOLLET; 设置好触发模式后可以把event加入epoll中：\nstruct epoll_event ev; ev.events = EPOLLIN; ev.data.fd = sockfd; epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, \u0026amp;ev); ","date":"2020-02-20T00:00:00Z","image":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/029_hu5f7dae7e78ac97b0b045f4b1159a4d54_14023957_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/%E7%AE%80%E6%98%93tcp%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"简易Tcp服务器"},{"content":"MYSQL mysql安装与配置 在虚拟机上安装mysql,使用apt-get install就可以 这里我只检索到了mysql-server-5.7就安装了5.7\n在本地win10上安装mysqlbench用于连接虚拟机的mysql服务器 这里使用网络连接，可能是因为mysql版本的原因，本来应该在/etc/mysql中的my.cnf文件中显式的配置有基本信息，我只需要修改部分，但5.7在/etc/mysql/mysql.conf.d/mysqld.cnf,在它的基础上修改对应的bind-address为0.0.0.0保证回环地址可访问：\n# # The MySQL database server configuration file. # # You can copy this to one of: # - \u0026#34;/etc/mysql/my.cnf\u0026#34; to set global options, # - \u0026#34;~/.my.cnf\u0026#34; to set user-specific options. # # One can use all long options that the program supports. # Run program with --help to get a list of available options and with # --print-defaults to see which it would actually understand and use. # # For explanations see # http://dev.mysql.com/doc/mysql/en/server-system-variables.html # This will be passed to all mysql clients # It has been reported that passwords should be enclosed with ticks/quotes # escpecially if they contain \u0026#34;#\u0026#34; chars... # Remember to edit /etc/mysql/debian.cnf when changing the socket location. # Here is entries for some specific programs # The following values assume you have at least 32M ram [mysqld_safe] socket = /var/run/mysqld/mysqld.sock nice = 0 [mysqld] # # * Basic Settings # user = mysql pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock port = 3306 basedir = /usr datadir = /var/lib/mysql tmpdir = /tmp lc-messages-dir = /usr/share/mysql skip-external-locking # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address = 0.0.0.0 # # * Fine Tuning # key_buffer_size = 16M max_allowed_packet = 16M thread_stack = 192K thread_cache_size = 8 # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched myisam-recover-options = BACKUP #max_connections = 100 #table_cache = 64 #thread_concurrency = 10 # # * Query Cache Configuration # query_cache_limit = 1M query_cache_size = 16M # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error log - should be very few entries. # log_error = /var/log/mysql/error.log # # Here you can see queries with especially long duration #log_slow_queries = /var/log/mysql/mysql-slow.log #long_query_time = 2 #log-queries-not-using-indexes # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id = 1 #log_bin = /var/log/mysql/mysql-bin.log expire_logs_days = 10 max_binlog_size = 100M #binlog_do_db = include_database_name #binlog_ignore_db = include_database_name # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI \u0026#34;tinyca\u0026#34;. # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem 这样保证win10的mysqlbench可以连接到虚拟机的mysql服务器\n但是还需要在mysql中设置对应用户并使之具有外部访问的权限和操作数据库的权限，我这里直接新建gao用户并赋予外部访问权限和操作权限：\nCREATE USER 'gao'@'%' IDENTIFIED BY 'password';\r%号表示可以被任意位置访问也就允许了远程ip访问 然后给gao用户授权，使其能随意操作数据库：\nGRANT all privileges ON * TO 'gao'@'%';\r可以在mysql这个数据库内的user表内找到自己添加的用户信息\nmysql 建表添加数据等操作 建立使用的数据库USR_DB\ncreatedatabasesUSR_DB;使用USR_DB\nuseUSR_DB;建表TBL_USR\ncreatetableTBL_USR(U_IDintprimarykeyauto_increment,U_NAMEchar(10),U_GENGDERchar(10));插入表项\ninsertTBL_USR(U_NAME,U_GENGDER)values(\u0026#39;gao\u0026#39;,\u0026#39;man\u0026#39;);选取表中所有数据显示\nselect*fromTBL_USR;删除与修改表项就涉及到安全模式，mysql默认运行在安全模式所以不能进行修改和删除表项的操作，所以需要取消安全模式然后操作，操作结束后需要再设置回安全模式\nsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;但这样操作是有问题的，如果这三部操作是原子的，是没有问题的，但不是原子的就引入不安全的因素，其他进程可能趁这个时候错误的篡改数据，所以将这三条合成一个过程来确保操作的安全性\nDELIMITER##createprocedureproc_delete_usr(inUNAMEchar(10))beginsetSQL_SAFE_UPDATES=0;deletefromTBL_USRwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##上面这个DELIMITER ##的意思就是这一段使用##作为限制符号，也就是##框住的区域视为一个整体区域，用于指示过程的区域\n定义了过程之后就可以使用call调用过程达到安全的操作：\ncallproc_delete_usr(\u0026#39;gao\u0026#39;);同样的，修改也可以这样：\nDELIMITER##createprocedureset_img(inUNAMEchar(10),UIMGBLOB)beginsetSQL_SAFE_UPDATES=0;updateTBL_USRsetU_IMG=UIMGwhereU_NAME=UNAME;setSQL_SAFE_UPDATES=1;end##callset_img(\u0026#39;gao\u0026#39;,IMG);这里的call set_img里的IMG其实在后面用于c的API调用,绑定statement之后传入的是一个char*的buffer接收的图像数据,然后设置到数据库里\n上面用到了U_IMG的column,这个列在建表时没有建立，需要使用添加column操作：\nALTERTABLETBL_USRcreatecolumnU_IMG;当然也可以使用以下操作删除：\nALTERTABLETBL_USRdropcolumnU_IMG;C api远程调用mysql 编写C程序来做到控制mysql数据库\n安装库： 先安装相关依赖和库才可以调用c api： 直接在虚拟机上\nsudo apt-get install libmysqlclient-dev;\r就安装成功了相关的c开发套件\n使用时需要在程序中包含头文件：\n#include\u0026lt;mysql.h\u0026gt;在编译相关程序时:\ngcc -o xxx xxx.c -I /usr/include/mysql -lmysqlclient 这里基本准备就完成\n基本操作: 首先需要连接mysql数据库，可以想到的就是建立一个mysql的handler，所以很自然的这里就需要一个特殊的struct，库为我们提供了MYSQL的数据类型：\nMYSQL mysql; 这样就建立了mysql这样一个handle，之后的所有操作都基于这个handle进行\n连接操作：\nif(NULL == mysql_init(\u0026amp;mysql)) { printf(\u0026#34;mysql init %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); return -1; } if(!mysql_real_connect(\u0026amp;mysql, king_db_server_ip, king_db_username, king_db_password, king_db_default_db, king_db_server_port, NULL, 0)) { printf(\u0026#34;mysql_real_connect: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } 可读性很高，这里不解释\n然后发送自己预定义好的sql语句：\n#define sql_insert \u0026#34;insert TBL_USR(U_NAME, U_GENGDER) values(\u0026#39;qiuxiang\u0026#39;, \u0026#39;woman\u0026#39;);\u0026#34;  #if 1 if(mysql_real_query(\u0026amp;mysql, sql_insert, strlen(sql_insert))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(\u0026amp;mysql)); } #endif 一般情况下不进行其他操作了的话需要关闭mysql连接：\nmysql_close(\u0026amp;mysql); 以上就是简单的基于c api的mysql操作了\n其他操作 select基础\n如果需要从mysql服务器接收数据，比如select一些数据， 那么就需要一个容器来接受数据，这里使用MYSQL_RES来保存mysql的返回的结果:\n同样需要先query：\nif(mysql_real_query(mysql, sql_select, strlen(sql_select))) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -1; } 然后接收数据\nMYSQL_RES *res = mysql_store_result(mysql); if(res == NULL) { printf(\u0026#34;mysql_real_query: %s\\n\u0026#34;, mysql_error(mysql)); return -2; } 然后处理数据（打印出来） 想打印的话首先需要知道行列数然后再选取需要的数据来打印：\nint rows = mysql_num_rows(res); printf(\u0026#34;rows: %d\\n\u0026#34;, rows); int fields = mysql_num_fields(res); printf(\u0026#34;fields: %d\\n\u0026#34;, fields); 再根据获取的行列数循环fetch数据行然后打印特定行列的数据\nMYSQL_ROW row; while(row = mysql_fetch_row(res)) { int i=0; for(i=0; i\u0026lt;fields;++i) { printf(\u0026#34;%s\\t\u0026#34;, row[i]); } printf(\u0026#34;\\n\u0026#34;); } 这里就可以看到，数据转存到了MYSQL_ROW这个结构中\n最后释放接收的结果\nmysql_free_result(res); statement\n使用statement来存储或发送数据到mysql服务器\n整个流程： 1、初始化stmt，使用MYSQL* handle 2、 准备statement类似于query但是不执行 3、初始化绑定参数MYSQL_BIND param，因为要insert所以要初始化buffer用于指示insert数据 4、将参数绑定到stmt上 5、将buffer中的数据通过statement发送到mysql服务器？（不太清楚是否真的发送了） 6、执行statement 7、执行完毕关闭statement\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_insert_img, strlen(sql_insert_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND param = {0}; param.buffer_type = MYSQL_TYPE_LONG_BLOB; param.buffer = NULL; param.is_null = 0; param.length = NULL; ret = mysql_stmt_bind_param(stmt, \u0026amp;param); if(ret) { printf(\u0026#34;mysql_stmt_bind_param error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_send_long_data(stmt, 0, buffer, length); if(ret) { printf(\u0026#34;mysql_stmt_send_long_data error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } ret = mysql_stmt_close(stmt); if(ret) { printf(\u0026#34;mysql_stmt_close error: %s\\n\u0026#34;, mysql_error(handle)); return -6; } 以下是一个read的statement：\nMYSQL_STMT *stmt = mysql_stmt_init(handle); int ret = mysql_stmt_prepare(stmt, sql_select_img, strlen(sql_select_img)); if(ret) { printf(\u0026#34;mysql_stmt_prepare error: %s\\n\u0026#34;, mysql_error(handle)); return -2; } MYSQL_BIND result = {0}; result.buffer_type = MYSQL_TYPE_LONG_BLOB; unsigned long total_length = 0; result.length = \u0026amp;total_length; ret = mysql_stmt_bind_result(stmt, \u0026amp;result); if(ret) { printf(\u0026#34;mysql_stmt_bind_result error: %s\\n\u0026#34;, mysql_error(handle)); return -3; } ret = mysql_stmt_execute(stmt); if(ret) { printf(\u0026#34;mysql_stmt_execute error: %s\\n\u0026#34;, mysql_error(handle)); return -4; } ret = mysql_stmt_store_result(stmt); if(ret) { printf(\u0026#34;mysql_stmt_store_result error: %s\\n\u0026#34;, mysql_error(handle)); return -5; } while(1) { ret = mysql_stmt_fetch(stmt); if(ret !=0 \u0026amp;\u0026amp; ret != MYSQL_DATA_TRUNCATED) { break; } int start = 0; while(start \u0026lt; (int)total_length) { result.buffer = buffer + start; result.buffer_length = 1; mysql_stmt_fetch_column(stmt, \u0026amp;result, 0, start); start += result.buffer_length; } } mysql_stmt_close(stmt); 可以看到多了fetch操作将result的buffer成员指向外部接受用的buffer的最新的接受位置，mysql_stmt_fetch_column进行了接收工作，MYSQL_BIND result也有了新的定义方式\n","date":"2020-02-12T00:00:00Z","image":"https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/mysql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/","title":"Mysql基本知识"},{"content":"ubuntu-server虚拟机配置 Sh配置 安装完ubuntu后先配置sh这样可以通过xshell连接 只需要：\nsudo apt-get install openssh-server ssh即可 Samba配置 然后安装samba\nsudo apt-get install samba 创建share文件夹\ncd /home sudo mkdir share sudo chmod 777 share 然后在/etc/samba里配置smb.conf 文件 在文件尾部添加：\n[share] comment = My Samba path = /home/gao/share browseable = yes writeable = yes 然后设置密码\nsudo smbpasswd -a gao 然后去主机上映射盘符就可以方便的访问 在文件框内输入\\192.168.134.xxx 也就是虚拟机ip就可以 把share映射为盘符\ngcc配置 换apt阿里源：\ncd /etc/apt sudo mv source.list source.list.back sudo vim source.list 改为：\ndeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial universe deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties deb http://archive.canonical.com/ubuntu xenial partner deb-src http://archive.canonical.com/ubuntu xenial partner deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 安装gcc：\nsudo apt-get install build-essential ","date":"2020-02-01T00:00:00Z","image":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/hudai-gayiran-3Od_VKcDEAA-unsplash_hub241c2a9c7a2caf7e16a2a5bbc7141ff_18711_120x120_fill_q75_box_smart1.jpg","permalink":"https://gao377020481.github.io/p/ubuntu-server%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE/","title":"ubuntu-server虚拟机配置"}]